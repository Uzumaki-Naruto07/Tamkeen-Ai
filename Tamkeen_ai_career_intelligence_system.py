# -*- coding: utf-8 -*-
"""Copy of Tamkeen_Ai_Career_Intelligence_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HeVn6SBPILSX44Nq62DN-CM4Xa9cnvFt

# Section 1: SETUP AND IMPORTS
"""

!nvidia-smi

!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg

!pip install transformers torch huggingface_hub gtts pandas numpy scikit-learn joblib SpeechRecognition pyaudio tensorflow matplotlib datasets gradio

# Install required packages
!pip install -q pdfplumber python-docx spacy scikit-learn
# Download spaCy model
!python -m spacy download en_core_web_sm

# ============== SECTION 1: SETUP AND IMPORTS ==============
# Install required packages with pip:
# pip install transformers torch huggingface_hub gtts pandas numpy scikit-learn joblib SpeechRecognition pyaudio tensorflow matplotlib datasets torch

# Initialize Colab detection variable at the very beginning
USING_COLAB = False


import os
import json
import time
import re
import requests
import numpy as np
import pandas as pd
from datetime import datetime
from getpass import getpass
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from collections import Counter


# Initialize feature flags
TTS_AVAILABLE = False
SPEECH_RECOGNITION_AVAILABLE = False
ML_AVAILABLE = False
TF_AVAILABLE = False
XGBOOST_AVAILABLE = False
TRANSFORMERS_AVAILABLE = False
SENTIMENT_ANALYSIS_AVAILABLE = False
DATASETS_AVAILABLE = False

# Try importing optional packages
try:
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import mean_squared_error, accuracy_score, r2_score
    from sklearn.preprocessing import StandardScaler
    import joblib
    import matplotlib.pyplot as plt
    ML_AVAILABLE = True
    print("✅ Machine learning support available")
except ImportError:
    print("❌ Machine learning libraries not available")

try:
    # TensorFlow is optional for neural network models
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    TF_AVAILABLE = True
    print("✅ TensorFlow/Keras support available for neural networks")
except ImportError:
    print("❌ TensorFlow not available - advanced ML models will be limited")

try:
    from gtts import gTTS
    import os
    TTS_AVAILABLE = True
    print("✅ Text-to-speech support available")
except ImportError:
    print("❌ Text-to-speech not available")

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
    print("✅ Speech recognition support available")
except ImportError:
    print("❌ Speech recognition not available")

try:
    from huggingface_hub import login
    from transformers import pipeline

    # Initialize the transformers-specific flags
    TRANSFORMERS_AVAILABLE = True

    # Try to set up sentiment analysis
    try:
        sentiment_analyzer = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
        SENTIMENT_ANALYSIS_AVAILABLE = True
        print("✅ Sentiment analysis model loaded and ready")
    except Exception as e:
        print(f"❌ Sentiment analysis model couldn't be loaded: {str(e)}")

    print("✅ Hugging Face support available")
except ImportError:
    print("❌ Hugging Face libraries not available, LLM functionality will be limited")

# Try loading the datasets library for sentiment dataset
try:
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
    print("✅ Hugging Face datasets library available")
except ImportError:
    print("❌ Hugging Face datasets library not available")

print("\nSection 1 complete: Setup and imports finished.\n")

"""# Section 2 : User Information Collection"""

# ============== SECTION 2: USER INFORMATION COLLECTION & CAREER GUIDANCE ==============
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output, Javascript, IFrame
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import time
import os
import requests
import json
import re
from datetime import datetime
import base64
from huggingface_hub import HfFolder
import io
from PIL import Image

# Initialize global variables
language = 'en'  # Default language
user_data = {}   # Dictionary to store all user information
career_matches = {}  # Store career matching results
current_view = 'form'  # Track current view (form, results, visualization, timeline)

# DeepSeek API Integration - Load from HuggingFace secret
try:
    # Try to get API key from HuggingFace secrets
    from huggingface_hub.constants import HF_HUB_CACHE
    with open(os.path.join(HF_HUB_CACHE, 'secrets.json'), 'r') as f:
        secrets = json.load(f)
        DEEPSEEK_API_KEY = secrets.get('DEEPSEEK_API_KEY', '')
except:
    # Fallback to environment variable or hardcoded value (only for demo purposes)
    DEEPSEEK_API_KEY = os.environ.get('DEEPSEEK_API_KEY', 'YOUR_API_KEY_HERE')  # Replace in production

def query_deepseek(query, lang='en'):
    """Query the DeepSeek API for advanced career guidance"""
    # System prompts tailored to specific career guidance
    if lang == 'ar':
        system_msg = """أنت مستشار مهني خبير باللغة العربية يدعى حصة المازمي. قدم إجابات دقيقة ومفيدة عن المسارات المهنية والمهارات المطلوبة والتعليم.
        ركز على المعلومات العملية والمحددة. قدم نصائح ملموسة حول المهارات والشهادات والخبرات المطلوبة لمختلف المسارات المهنية.
        اجعل إجاباتك موجزة ومباشرة وفي نقاط إذا أمكن. أعط أمثلة محددة عن مسارات التقدم الوظيفي.
        قم بتخصيص نصائحك للسياق المحلي في الإمارات العربية المتحدة والشرق الأوسط عندما يكون ذلك مناسبًا."""
        user_prompt = f"السؤال عن المسار المهني: {prompt}. قدم إجابة مفصلة ومفيدة باللغة العربية."
    else:
        system_msg = """You are Hessa Almaazmi, an expert career advisor specializing in emerging technologies and traditional professions.
        Provide accurate, helpful answers about career paths, required skills, and education. Focus on practical, specific information.
        Give concrete advice about skills, certifications, and experiences needed for various career paths.
        Keep your answers concise, direct, and in bullet points when possible. Provide specific examples of career progression.
        Tailor your advice to the UAE and Middle Eastern context when appropriate.
        Include salary ranges, job outlook, and work-life balance information when relevant."""
        user_prompt = f"Career question: {prompt}. Provide a detailed and helpful answer focused on practical steps."

    # API endpoint
    url = "https://api.deepseek.com/v1/chat/completions"

    # Prepare the payload with the carefully crafted system message
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }

    # Set up headers with API key
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }

    try:
        # Make the API request
        response = requests.post(url, headers=headers, data=json.dumps(payload))

        # Check if the request was successful
        if response.status_code == 200:
            # Parse the response
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            # Handle API errors but don't expose technical details to user
            if lang == 'en':
                return "I'm sorry, I couldn't retrieve detailed information about that career path at the moment. Let me provide some general guidance instead: The most important skills in any career are adaptability, continuous learning, and effective communication. Consider researching industry certifications, relevant online courses, and networking opportunities in your desired field."
            else:
                return "عذرًا، لم أتمكن من استرجاع معلومات مفصلة حول هذا المسار المهني في الوقت الحالي. دعني أقدم بعض الإرشادات العامة بدلاً من ذلك: أهم المهارات في أي مهنة هي القدرة على التكيف، والتعلم المستمر، والتواصل الفعال. ابحث عن الشهادات المهنية، والدورات الإلكترونية ذات الصلة، وفرص التواصل المهني في المجال الذي ترغب فيه."

    except Exception:
        # Handle exceptions without exposing error details
        if lang == 'en':
            return "I'm experiencing connectivity issues. Here's some general advice: 1) Research the specific skills and qualifications needed in your field, 2) Connect with professionals through LinkedIn and industry events, 3) Look for internships or volunteer opportunities to gain experience, 4) Consider relevant certifications or advanced degrees if applicable."
        else:
            return "أواجه مشاكل في الاتصال. إليك بعض النصائح العامة: 1) ابحث عن المهارات والمؤهلات المطلوبة في مجالك، 2) تواصل مع المحترفين من خلال LinkedIn والفعاليات المهنية، 3) ابحث عن فرص التدريب أو التطوع لاكتساب الخبرة، 4) فكر في الحصول على شهادات مهنية أو درجات علمية متقدمة إذا كان ذلك مناسبًا."

# Comprehensive translations dictionary with cultural nuances
translations = {
    'en': {
        'title': 'Career Profile Builder',
        'language_selector': 'Select Language',
        'name': 'Name:',
        'email': 'Enter your email:',
        'education': 'Current education status:',
        'linkedin': 'LinkedIn profile URL:',
        'skills': 'Your key skills (comma separated):',
        'goals': 'What are your career goals?',
        'personality_title': 'Personality Assessment',
        'q1': 'I prefer working in teams rather than individually:',
        'q2': 'I enjoy solving complex problems:',
        'q3': 'I consider myself detail-oriented:',
        'q4': 'I adapt easily to changing environments:',
        'q5': 'I prefer structured work with clear guidelines:',
        'options': ['Strongly Disagree', 'Disagree', 'Neutral', 'Agree', 'Strongly Agree'],
        'submit': 'Submit',
        'chatbot_welcome': "Hello! My name is Hessa Almaazmi, and I'll be your career assistant today. I can provide guidance on any career field, including emerging ones like AI, blockchain, and UX design. Ask me anything about career paths, skills, or education requirements!",
        'chatbot_placeholder': 'Ask a question about any career field...',
        'send': 'Send',
        'name_placeholder': 'Enter your full name',
        'email_placeholder': 'john.doe@example.com',
        'education_placeholder': 'Bachelor in Computer Science',
        'skills_placeholder': 'Python, data analysis, communication, leadership',
        'goals_placeholder': 'To become a senior professional in my field...',
        'success_message': '✅ Profile information submitted successfully!',
        'summary': 'Summary of your information:',
        'personality_assessment': 'Personality Assessment:',
        'completion_message': '✅ Profile completed successfully.',
        'career_match_title': 'Your Top Career Matches',
        'experience_level': 'Years of experience:',
        'experience_options': ['Entry Level (0-2 years)', 'Mid Level (3-5 years)', 'Senior Level (6-10 years)', 'Expert (10+ years)'],
        'industry_preference': 'Preferred industry:',
        'industry_options': ['Technology', 'Finance', 'Healthcare', 'Education', 'Manufacturing', 'Creative', 'Public Sector', 'Other'],
        'work_environment': 'Preferred work environment:',
        'work_env_options': ['Remote', 'Office-based', 'Hybrid', 'Field work', 'No preference'],
        'certifications': 'Professional Certifications:',
        'add_certification': 'Add Certification',
        'certification_name': 'Certification Name:',
        'certification_issuer': 'Issuing Organization:',
        'certification_date': 'Date Obtained:',
        'dark_mode': 'Dark Mode',
        'accessibility_mode': 'Accessibility Mode',
        'upload_resume': 'Upload Resume',
        'export_data': 'Export Data',
        'back_to_form': 'Back to Form',
        'view_matches': 'View Career Matches',
        'view_timeline': 'View Career Timeline',
        'view_resume': 'View Resume Preview',
        'timeline_title': 'Your Career Timeline',
        'resume_preview': 'Resume Preview',
        'resume_tip': 'This is a simplified preview based on your profile. You can use this as a starting point for your actual resume.',
        'career_timeline_explanation': 'This timeline shows a potential career progression based on your profile and top career match.',
        'assistant_title': 'Assistant'
    },
    'ar': {
        'title': 'أداة بناء الملف المهني',
        'language_selector': 'اختر اللغة',
        'name': 'الاسم:',
        'email': 'أدخل بريدك الإلكتروني:',
        'education': 'الوضع التعليمي الحالي:',
        'linkedin': 'رابط الملف الشخصي LinkedIn:',
        'skills': 'مهاراتك الرئيسية (مفصولة بفواصل):',
        'goals': 'ما هي أهدافك المهنية؟',
        'personality_title': 'تقييم الشخصية',
        'q1': 'أفضل العمل في فرق بدلاً من العمل بشكل فردي:',
        'q2': 'أستمتع بحل المشكلات المعقدة:',
        'q3': 'أعتبر نفسي مهتماً بالتفاصيل:',
        'q4': 'أتكيف بسهولة مع البيئات المتغيرة:',
        'q5': 'أفضل العمل المنظم بإرشادات واضحة:',
        'options': ['لا أوافق بشدة', 'لا أوافق', 'محايد', 'أوافق', 'أوافق بشدة'],
        'submit': 'إرسال',
        'chatbot_welcome': "مرحبًا! اسمي حصة المازمي، وسأكون مساعدك المهني اليوم. يمكنني تقديم إرشادات حول أي مجال مهني، بما في ذلك المجالات الناشئة مثل الذكاء الاصطناعي وسلسلة الكتل وتصميم تجربة المستخدم. اسألني أي شيء عن المسارات المهنية أو المهارات أو متطلبات التعليم!",
        'chatbot_placeholder': 'اطرح سؤالاً حول أي مجال مهني...',
        'send': 'إرسال',
        'name_placeholder': 'أدخل اسمك الكامل',
        'email_placeholder': 'mohammed.ahmed@example.com',
        'education_placeholder': 'بكالوريوس في علوم الكمبيوتر',
        'skills_placeholder': 'البرمجة بلغة بايثون، تحليل البيانات، التواصل، القيادة',
        'goals_placeholder': 'أن أصبح محترفًا متقدمًا في مجالي...',
        'success_message': '✅ تم إرسال معلومات الملف الشخصي بنجاح!',
        'summary': 'ملخص معلوماتك:',
        'personality_assessment': 'تقييم الشخصية:',
        'completion_message': '✅ تم إكمال الملف الشخصي بنجاح.',
        'career_match_title': 'أفضل المهن المناسبة لك',
        'experience_level': 'سنوات الخبرة:',
        'experience_options': ['مستوى مبتدئ (0-2 سنوات)', 'مستوى متوسط (3-5 سنوات)', 'مستوى متقدم (6-10 سنوات)', 'خبير (أكثر من 10 سنوات)'],
        'industry_preference': 'الصناعة المفضلة:',
        'industry_options': ['التكنولوجيا', 'التمويل', 'الرعاية الصحية', 'التعليم', 'التصنيع', 'المجال الإبداعي', 'القطاع العام', 'أخرى'],
        'work_environment': 'بيئة العمل المفضلة:',
        'work_env_options': ['عن بعد', 'في المكتب', 'مختلط', 'عمل ميداني', 'لا تفضيل'],
        'certifications': 'الشهادات المهنية:',
        'add_certification': 'إضافة شهادة',
        'certification_name': 'اسم الشهادة:',
        'certification_issuer': 'الجهة المصدرة:',
        'certification_date': 'تاريخ الحصول عليها:',
        'dark_mode': 'الوضع الداكن',
        'accessibility_mode': 'وضع إمكانية الوصول',
        'upload_resume': 'تحميل السيرة الذاتية',
        'export_data': 'تصدير البيانات',
        'back_to_form': 'العودة إلى النموذج',
        'view_matches': 'عرض المهن المتطابقة',
        'view_timeline': 'عرض الجدول الزمني للمهنة',
        'view_resume': 'عرض معاينة السيرة الذاتية',
        'timeline_title': 'الجدول الزمني لمهنتك',
        'resume_preview': 'معاينة السيرة الذاتية',
        'resume_tip': 'هذه معاينة مبسطة بناءً على ملفك الشخصي. يمكنك استخدام هذا كنقطة بداية لسيرتك الذاتية الفعلية.',
        'career_timeline_explanation': 'يوضح هذا الجدول الزمني تقدمًا مهنيًا محتملاً بناءً على ملفك الشخصي وأفضل مهنة متطابقة.',
        'assistant_title': 'المساعد'
    }
}

# Career knowledge base with detailed information about different career paths
career_knowledge_base = {
    'software_developer': {
        'title': 'Software Developer',
        'title_ar': 'مطور برمجيات',
        'skills': ['Python', 'Java', 'JavaScript', 'React', 'Node.js', 'Git', 'problem-solving', 'collaboration'],
        'education': ['Computer Science', 'Software Engineering', 'Information Technology', 'Bootcamp'],
        'traits': [3, 5, 4, 4, 3],  # Team, Problem-solving, Detail, Adaptability, Structure (1-5 scale)
        'growth_areas': ['Cloud technologies (AWS/Azure/GCP)', 'DevOps and CI/CD pipelines', 'Mobile app development', 'Cybersecurity practices', 'AI/ML integration']
    },
    'data_scientist': {
        'title': 'Data Scientist',
        'title_ar': 'عالم بيانات',
        'skills': ['Python', 'R', 'SQL', 'Machine Learning', 'Statistics', 'Data Visualization', 'pandas', 'TensorFlow'],
        'education': ['Data Science', 'Statistics', 'Mathematics', 'Computer Science', 'Physics'],
        'traits': [2, 5, 5, 3, 4],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Deep learning architectures', 'Natural Language Processing', 'Big Data technologies', 'Cloud-based ML platforms', 'Business intelligence tools']
    },
    'ux_designer': {
        'title': 'UX Designer',
        'title_ar': 'مصمم تجربة المستخدم',
        'skills': ['User Research', 'Wireframing', 'Prototyping', 'Figma', 'Adobe XD', 'User Testing', 'empathy', 'UI design'],
        'education': ['Design', 'Human-Computer Interaction', 'Psychology', 'Fine Arts'],
        'traits': [4, 4, 5, 3, 3],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Design systems', 'Accessibility standards', '3D interface design', 'VR/AR experiences', 'Motion design']
    },
    'product_manager': {
        'title': 'Product Manager',
        'title_ar': 'مدير منتج',
        'skills': ['Product Strategy', 'Market Research', 'User Stories', 'Roadmapping', 'Stakeholder Management', 'Agile', 'communication'],
        'education': ['Business Administration', 'Product Management', 'Computer Science', 'Marketing'],
        'traits': [5, 4, 3, 5, 3],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Data-driven decision making', 'Technical product management', 'Growth hacking', 'Enterprise product management', 'Product marketing alignment']
    },
    'ai_engineer': {
        'title': 'AI Engineer',
        'title_ar': 'مهندس ذكاء اصطناعي',
        'skills': ['Python', 'TensorFlow', 'PyTorch', 'Computer Vision', 'NLP', 'Deep Learning', 'MLOps', 'Mathematics'],
        'education': ['AI', 'Machine Learning', 'Computer Science', 'Mathematics', 'Data Science'],
        'traits': [3, 5, 4, 4, 3],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Reinforcement learning', 'Generative AI', 'AI ethics & governance', 'Edge AI deployment', 'Large language models']
    },
    'cybersecurity_analyst': {
        'title': 'Cybersecurity Analyst',
        'title_ar': 'محلل أمن سيبراني',
        'skills': ['Network Security', 'Penetration Testing', 'SIEM tools', 'Risk Assessment', 'Security Compliance', 'Threat Intelligence'],
        'education': ['Cybersecurity', 'Information Security', 'Computer Science', 'IT'],
        'traits': [3, 5, 5, 4, 4],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Cloud security', 'Zero trust architecture', 'OT/IoT security', 'Threat hunting', 'Security automation']
    },
    'cloud_architect': {
        'title': 'Cloud Architect',
        'title_ar': 'مهندس سحابة',
        'skills': ['AWS', 'Azure', 'GCP', 'Docker', 'Kubernetes', 'Terraform', 'Networking', 'Security'],
        'education': ['Cloud Computing', 'Computer Science', 'IT Infrastructure', 'Systems Engineering'],
        'traits': [4, 5, 4, 4, 3],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Multi-cloud strategies', 'FinOps', 'Serverless architectures', 'Edge computing', 'AI-integrated cloud solutions']
    },
    'digital_marketer': {
        'title': 'Digital Marketer',
        'title_ar': 'مسوق رقمي',
        'skills': ['SEO', 'SEM', 'Content Marketing', 'Social Media', 'Analytics', 'Email Marketing', 'copywriting'],
        'education': ['Marketing', 'Digital Marketing', 'Business', 'Communications'],
        'traits': [4, 3, 4, 5, 2],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Marketing automation', 'Conversion rate optimization', 'Video marketing', 'Influencer marketing strategy', 'AI in marketing']
    },
    'data_engineer': {
        'title': 'Data Engineer',
        'title_ar': 'مهندس بيانات',
        'skills': ['SQL', 'Python', 'Spark', 'Hadoop', 'ETL', 'Data Warehousing', 'Airflow', 'Kafka'],
        'education': ['Computer Science', 'Data Engineering', 'Information Systems', 'Database Management'],
        'traits': [3, 4, 5, 3, 4],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Real-time data processing', 'Data mesh architecture', 'DataOps', 'Data governance', 'Streaming analytics']
    },
    'blockchain_developer': {
        'title': 'Blockchain Developer',
        'title_ar': 'مطور بلوكتشين',
        'skills': ['Solidity', 'Smart Contracts', 'Web3.js', 'Ethereum', 'Cryptography', 'JavaScript', 'Distributed Systems'],
        'education': ['Computer Science', 'Blockchain Technology', 'Cryptography', 'Security'],
        'traits': [2, 5, 5, 3, 3],  # Team, Problem-solving, Detail, Adaptability, Structure
        'growth_areas': ['Layer 2 scaling solutions', 'Cross-chain interoperability', 'Zero-knowledge proofs', 'DeFi protocols', 'Sustainable blockchain']
    }
}

# Function to get text based on selected language
def get_text(key):
    """Retrieve text in the selected language"""
    global language
    return translations[language].get(key, key)

# Enhanced career-aware chatbot function with DeepSeek fallback for ANY career field
def get_chatbot_response(query, lang):
    query = query.lower().strip()

    # List of standard career fields we have predefined responses for
    standard_fields = list(career_knowledge_base.keys())
    standard_titles = [info['title'].lower() for info in career_knowledge_base.values()]

    # Define the basic predefined responses
    if lang == 'en':
        # Check for form-related questions first
        if 'how to fill' in query or 'fill the info' in query or 'fill my info' in query:
            return "To complete your profile, enter your details in each field. Start with your name, email, and education status. Then list your skills relevant to your career field. For career goals, consider both short-term and long-term ambitions. Finally, the personality assessment helps us match you with suitable career paths."

        # Check if query contains specific career titles
        for idx, title in enumerate(standard_titles):
            if title in query:
                career_id = list(career_knowledge_base.keys())[idx]
                career = career_knowledge_base[career_id]
                return f"For a career as a {career['title']}, focus on developing skills such as {', '.join(career['skills'][:5])}. Education typically includes {', '.join(career['education'][:3])}. To stay competitive, consider developing expertise in {', '.join(career['growth_areas'][:3])}."

        # Important: Check if this is a career-related question but not about our standard fields
        # This is where we route to DeepSeek for AI and other specialized careers
        career_keywords = ['career', 'job', 'profession', 'field', 'industry', 'work',
                          'occupation', 'employment', 'ai', 'artificial intelligence',
                          'blockchain', 'cybersecurity', 'robotics', 'architecture',
                          'become', 'how to', 'skills', 'education', 'degree', 'certification',
                          'salary', 'market', 'future', 'trends', 'path']

        # If we detect any career keyword, use DeepSeek
        if any(keyword in query for keyword in career_keywords):
            try:
                deepseek_response = query_deepseek(query, lang)
                return deepseek_response
            except Exception as e:
                # If DeepSeek fails, give a helpful response
                return f"I'd be happy to provide information about careers in {query}. To succeed in most emerging fields, focus on developing both technical skills specific to the domain and soft skills like communication, adaptability, and continuous learning. Consider relevant certifications and joining professional communities to build your network."

        # Default response if no matches
        available_fields = ", ".join([career_knowledge_base[k]['title'] for k in standard_fields])
        return f"I'm here to help with your career profile! I can provide guidance on skills, education, and career paths for various fields including: {available_fields}, and many others. What specific career field are you interested in?"

    else:
        # Arabic version with similar logic
        # Check for form-related questions
        if 'كيف' in query and ('ملء' in query or 'تعبئة' in query):
            return "لإكمال ملفك الشخصي، أدخل تفاصيلك في كل حقل. ابدأ باسمك وبريدك الإلكتروني والحالة التعليمية. ثم اذكر مهاراتك ذات الصلة بمجال عملك. بالنسبة للأهداف المهنية، فكر في الطموحات قصيرة وطويلة المدى. أخيرًا، يساعدنا تقييم الشخصية على مطابقتك مع المسارات المهنية المناسبة."

        # Map between Arabic and English career names
        profession_ar_map = {}
        for career_id, career in career_knowledge_base.items():
            profession_ar_map[career.get('title_ar', '').lower()] = career_id

        # Check if query contains Arabic career names
        for ar_title, career_id in profession_ar_map.items():
            if ar_title and ar_title in query:
                career = career_knowledge_base[career_id]
                return f"للعمل كـ {career['title_ar']}، ركز على تطوير مهارات مثل {', '.join(career['skills'][:5])}. يتضمن التعليم عادةً {', '.join(career['education'][:3])}. للبقاء متنافسًا، فكر في تطوير خبرة في {', '.join(career['growth_areas'][:3])}."

        # Use DeepSeek for career-related Arabic queries
        career_keywords_ar = ['مهنة', 'وظيفة', 'مهارات', 'تعليم', 'مجال', 'عمل',
                             'مسار', 'احتراف', 'ذكاء اصطناعي', 'بلوكتشين', 'أمن سيبراني',
                             'تطوير', 'تصميم', 'هندسة', 'إدارة', 'خبرة']

        if any(keyword in query for keyword in career_keywords_ar):
            try:
                deepseek_response = query_deepseek(query, lang)
                return deepseek_response
            except:
                return f"يسعدني تقديم معلومات حول المسارات المهنية في {query}. للنجاح في معظم المجالات الناشئة، ركز على تطوير المهارات التقنية الخاصة بالمجال والمهارات الشخصية مثل التواصل والقدرة على التكيف والتعلم المستمر. فكر في الحصول على الشهادات ذات الصلة والانضمام إلى المجتمعات المهنية لبناء شبكة علاقاتك."

        # Default Arabic response
        available_fields = ", ".join([career_knowledge_base[k].get('title_ar', k) for k in standard_fields])
        return f"أنا هنا لمساعدتك في ملفك المهني! يمكنني تقديم إرشادات حول المهارات والتعليم والمسارات المهنية لمختلف المجالات بما في ذلك: {available_fields}. ما هو المجال الذي تهتم به؟"

# Match user profile to potential careers
def match_careers_to_profile(user_data):
    """Calculate match percentages between user profile and career paths"""
    matches = {}

    # Extract user traits from form data
    personality_scores = user_data.get('Personality Scores', [3, 3, 3, 3, 3])  # Default to neutral if not available
    user_skills_text = user_data.get('Skills', '').lower()
    user_skills = [skill.strip() for skill in user_skills_text.split(',')]
    education = user_data.get('Education Status', '').lower()

    # Calculate match for each career
    for career_id, career in career_knowledge_base.items():
        # Skills match
        skill_score = 0
        for skill in career['skills']:
            if any(user_skill in skill.lower() or skill.lower() in user_skill for user_skill in user_skills):
                skill_score += 1
        skill_match = min(skill_score / len(career['skills']) * 100, 100) if career['skills'] else 0

        # Education match
        education_match = 0
        for edu in career['education']:
            if edu.lower() in education:
                education_match = 100
                break

        # Personality trait match
        trait_match = 0
        if len(personality_scores) == len(career['traits']):
            trait_differences = [abs(user_trait - career_trait) for user_trait, career_trait in zip(personality_scores, career['traits'])]
            max_possible_diff = 4 * len(trait_differences)  # Maximum difference (4) for each trait
            total_diff = sum(trait_differences)
            trait_match = (1 - (total_diff / max_possible_diff)) * 100

        # Calculate overall match percentage
        overall_match = (skill_match * 0.5) + (education_match * 0.3) + (trait_match * 0.2)

        # Round to nearest whole number
        overall_match = round(overall_match)

        # Store in matches dictionary
        matches[career_id] = {
            'title': career['title'],
            'title_ar': career.get('title_ar', career['title']),
            'match_percentage': overall_match,
            'skills_match': round(skill_match),
            'education_match': round(education_match),
            'trait_match': round(trait_match),
            'growth_areas': career['growth_areas']
        }

    # Sort by match percentage
    matches = dict(sorted(matches.items(), key=lambda item: item[1]['match_percentage'], reverse=True))

    return matches

# Create a visual representation of career matches
def create_career_visualization(career_matches):
    """Create a bar chart visualizing career matches"""
    plt.figure(figsize=(10, 6))

    # Get top 5 matches
    top_careers = list(career_matches.keys())[:5]

    # Get titles and percentages
    if language == 'ar':
        titles = [career_matches[career_id]['title_ar'] for career_id in top_careers]
    else:
        titles = [career_matches[career_id]['title'] for career_id in top_careers]
    percentages = [career_matches[career_id]['match_percentage'] for career_id in top_careers]

    # Create bars with attractive colors
    bars = plt.barh(titles, percentages, color=['#3498db', '#2ecc71', '#9b59b6', '#f1c40f', '#e74c3c'])

    # Add percentage labels
    for bar, percentage in zip(bars, percentages):
        plt.text(percentage + 1, bar.get_y() + bar.get_height()/2, f'{percentage}%',
                 va='center', ha='left', fontweight='bold')

    # Add title and labels
    if language == 'ar':
        plt.title('تطابق المسار المهني', fontsize=16, fontweight='bold')
        plt.xlabel('نسبة التطابق (%)', fontsize=12)
        plt.ylabel('المسار المهني', fontsize=12)
        plt.gca().invert_yaxis()  # Invert y-axis for Arabic
    else:
        plt.title('Career Path Match', fontsize=16, fontweight='bold')
        plt.xlabel('Match Percentage (%)', fontsize=12)
        plt.ylabel('Career Path', fontsize=12)

    # Set axis limits
    plt.xlim(0, 105)

    # Add grid
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    # Tight layout
    plt.tight_layout()

    # Save to a buffer
    buf = io.BytesIO()
    plt.savefig(buf, format='png')
    buf.seek(0)

    # Encode as base64 for HTML display
    img_str = base64.b64encode(buf.read()).decode('utf-8')

    # Close the figure to free memory
    plt.close()

    # Create HTML to display image
    html = f'<img src="data:image/png;base64,{img_str}" alt="Career Matches" style="max-width:100%;">'

    # Display in output widget
    with visualization_output:
        clear_output()
        display(HTML(html))

# Create a timeline visualization for career progression
def create_timeline_visualization(timeline_data):
    """Create a visual timeline of career progression"""
    # Prepare data for visualization
    years = [entry['year'] for entry in timeline_data]
    roles = [entry['role'] for entry in timeline_data]

    # Create an attractive timeline visual
    with timeline_output:
        clear_output()

        # Header
        display(HTML(f"<h2>{get_text('timeline_title')}</h2>"))
        display(HTML(f"<p>{get_text('career_timeline_explanation')}</p>"))

        # Create timeline HTML
        timeline_html = """
        <div style="font-family: 'Roboto', sans-serif; max-width: 800px; margin: 0 auto;">
            <div style="display: flex; flex-direction: column; gap: 30px;">
        """

        for i, entry in enumerate(timeline_data):
            # Alternate colors
            colors = ['#3498db', '#2ecc71', '#9b59b6', '#e74c3c', '#f1c40f']
            color = colors[i % len(colors)]

            timeline_html += f"""
            <div style="display: flex; align-items: flex-start; gap: 20px;">
                <div style="min-width: 80px; text-align: center;">
                    <div style="background-color: {color}; color: white; font-weight: bold; padding: 8px; border-radius: 5px;">{entry['year']}</div>
                    <div style="height: 30px; width: 2px; background-color: {color}; margin: 5px auto;"></div>
                </div>
                <div style="flex-grow: 1; background-color: #f8f9fa; border-radius: 8px; padding: 15px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                    <h3 style="margin-top: 0; color: #2c3e50;">{entry['role']}</h3>
                    <p><strong>Key Skills:</strong> {entry['skills']}</p>
                    <p><strong>Development Focus:</strong> {entry['growth_areas']}</p>
                </div>
            </div>
            """

        timeline_html += """
            </div>
        </div>
        """

        display(HTML(timeline_html))

# Generate career timeline
def generate_career_timeline(top_career, experience_level):
    """Generate a career development timeline based on top matched career"""
    timeline = []

    # Determine starting year based on experience level
    current_year = datetime.now().year

    if experience_level == get_text('experience_options')[0]:  # Entry Level
        start_year = current_year
        years_back = 0
    elif experience_level == get_text('experience_options')[1]:  # Mid Level
        start_year = current_year - 3
        years_back = 3
    elif experience_level == get_text('experience_options')[2]:  # Senior Level
        start_year = current_year - 7
        years_back = 7
    else:  # Expert
        start_year = current_year - 12
        years_back = 12

    # Define career progression stages
    career_stages = []
    if years_back == 0:
        # Entry level - future projection only
        career_stages = [
            {"years_offset": 0, "prefix": "Junior ", "suffix": ""},
            {"years_offset": 2, "prefix": "", "suffix": ""},
            {"years_offset": 4, "prefix": "Senior ", "suffix": ""},
            {"years_offset": 7, "prefix": "Lead ", "suffix": ""},
            {"years_offset": 10, "prefix": "", "suffix": " Manager"}
        ]
    elif years_back == 3:
        # Mid level - some history + future projection
        career_stages = [
            {"years_offset": -3, "prefix": "Junior ", "suffix": ""},
            {"years_offset": 0, "prefix": "", "suffix": ""},
            {"years_offset": 2, "prefix": "Senior ", "suffix": ""},
            {"years_offset": 5, "prefix": "Lead ", "suffix": ""},
            {"years_offset": 8, "prefix": "", "suffix": " Manager"}
        ]
    elif years_back == 7:
        # Senior level - more history + future projection
        career_stages = [
            {"years_offset": -7, "prefix": "Junior ", "suffix": ""},
            {"years_offset": -4, "prefix": "", "suffix": ""},
            {"years_offset": 0, "prefix": "Senior ", "suffix": ""},
            {"years_offset": 3, "prefix": "Lead ", "suffix": ""},
            {"years_offset": 6, "prefix": "", "suffix": " Director"}
        ]
    else:
        # Expert level - extensive history + future projection
        career_stages = [
            {"years_offset": -12, "prefix": "Junior ", "suffix": ""},
            {"years_offset": -8, "prefix": "", "suffix": ""},
            {"years_offset": -4, "prefix": "Senior ", "suffix": ""},
            {"years_offset": 0, "prefix": "Lead ", "suffix": " / Principal"},
            {"years_offset": 3, "prefix": "", "suffix": " Director"}
        ]

    # Generate the timeline entries
    base_title = top_career['title']

    for i, stage in enumerate(career_stages):
        year = start_year + stage["years_offset"]
        role = f"{stage['prefix']}{base_title}{stage['suffix']}"

        # Generate progressive skills for each stage
        if i == 0:
            skills = "Foundational technical skills, teamwork, time management"
        elif i == 1:
            skills = "Advanced technical skills, project participation, problem-solving"
        elif i == 2:
            skills = "Technical expertise, mentoring, project leadership"
        elif i == 3:
            skills = "Strategic planning, team leadership, cross-functional collaboration"
        else:
            skills = "Organizational leadership, executive communication, strategic vision"

        timeline.append({
            'year': year,
            'role': role,
            'skills': skills,
            'growth_areas': top_career['growth_areas'][i % len(top_career['growth_areas'])]
        })

    return timeline

# Generate a simplified resume preview
def generate_resume_preview(user_data, top_career):
    """Generate a simplified resume preview based on user data and top career match"""
    if language == 'en':
        resume = f"""
        <div style="font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; border: 1px solid #ddd;">
            <h1 style="color: #2c3e50;">{user_data.get('Name', 'Your Name')}</h1>
            <p style="color: #7f8c8d;">{user_data.get('Email', 'your.email@example.com')} | {user_data.get('LinkedIn URL', 'LinkedIn Profile')}</p>

            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">Summary</h2>
            <p>Aspiring {top_career['title']} with a background in {user_data.get('Education Status', 'relevant education')}.
            Focused on {user_data.get('Career Goals', 'career development and professional growth')}.</p>

            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">Skills</h2>
            <ul>
        """

        # Add skills
        skills = user_data.get('Skills', '').split(',')
        for skill in skills:
            if skill.strip():
                resume += f"<li>{skill.strip()}</li>"

        # Add recommended skills based on top career
        resume += """
            </ul>
            <h3 style="color: #2c3e50;">Recommended Skills to Develop</h3>
            <ul>
        """

        # Get career from database
        career_id = next((k for k, v in career_knowledge_base.items() if v['title'] == top_career['title']), None)
        if career_id and career_id in career_knowledge_base:
            growth_areas = career_knowledge_base[career_id]['growth_areas']
            for area in growth_areas:
                resume += f"<li>{area}</li>"

        resume += """
            </ul>
            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">Education</h2>
            <p><strong>Degree/Certification:</strong> {0}</p>

            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">Career Objective</h2>
            <p>{1}</p>
        </div>
        """.format(user_data.get('Education Status', 'Your Education'), user_data.get('Career Goals', 'Your Career Goals'))
    else:
        # Arabic resume template
        resume = f"""
        <div style="font-family: 'Arial', sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; border: 1px solid #ddd; direction: rtl; text-align: right;">
            <h1 style="color: #2c3e50;">{user_data.get('Name', 'اسمك')}</h1>
            <p style="color: #7f8c8d;">{user_data.get('Email', 'your.email@example.com')} | {user_data.get('LinkedIn URL', 'الملف الشخصي LinkedIn')}</p>

            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">ملخص</h2>
            <p>طموح للعمل كـ {top_career['title_ar']} مع خلفية في {user_data.get('Education Status', 'التعليم ذو الصلة')}.
            التركيز على {user_data.get('Career Goals', 'تطوير المهنة والنمو المهني')}.</p>

            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">المهارات</h2>
            <ul>
        """

        # Add skills
        skills = user_data.get('Skills', '').split(',')
        for skill in skills:
            if skill.strip():
                resume += f"<li>{skill.strip()}</li>"

        # Add recommended skills based on top career
        resume += """
            </ul>
            <h3 style="color: #2c3e50;">المهارات الموصى بتطويرها</h3>
            <ul>
        """

        # Get career from database
        career_id = next((k for k, v in career_knowledge_base.items() if v['title'] == top_career['title']), None)
        if career_id and career_id in career_knowledge_base:
            growth_areas = career_knowledge_base[career_id]['growth_areas']
            for area in growth_areas:
                resume += f"<li>{area}</li>"

        resume += """
            </ul>
            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">التعليم</h2>
            <p><strong>الشهادة/الدرجة العلمية:</strong> {0}</p>

            <h2 style="color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 5px;">الهدف المهني</h2>
            <p>{1}</p>
        </div>
        """.format(user_data.get('Education Status', 'تعليمك'), user_data.get('Career Goals', 'أهدافك المهنية'))

    return resume

# Function to create the user profile collection UI
def create_user_profile_ui():
    global language, user_data, career_matches, current_view

    # Create output widgets for different sections
    chatbot_output = widgets.Output()
    results_output = widgets.Output()
    visualization_output = widgets.Output()
    timeline_output = widgets.Output()
    resume_output = widgets.Output()

    # Create language selector
    language_selector = widgets.Dropdown(
        options=[('English', 'en'), ('العربية', 'ar')],
        value=language,
        description=get_text('language_selector'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='30%')
    )

    # Dark mode toggle
    dark_mode_toggle = widgets.Checkbox(
        value=False,
        description=get_text('dark_mode'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='30%')
    )

    # Accessibility mode toggle
    accessibility_toggle = widgets.Checkbox(
        value=False,
        description=get_text('accessibility_mode'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='30%')
    )

    # Resume upload button
    upload_resume_button = widgets.FileUpload(
        accept='.pdf,.docx,.txt',
        multiple=False,
        description=get_text('upload_resume'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='30%')
    )

    # Export data button
    export_button = widgets.Button(
        description=get_text('export_data'),
        style={'button_color': '#4CAF50'},
        layout=widgets.Layout(width='20%')
    )

    # Navigation buttons
    back_button = widgets.Button(
        description=get_text('back_to_form'),
        style={'button_color': '#e74c3c'},
        layout=widgets.Layout(width='30%')
    )

    view_matches_button = widgets.Button(
        description=get_text('view_matches'),
        style={'button_color': '#3498db'},
        layout=widgets.Layout(width='30%')
    )

    view_timeline_button = widgets.Button(
        description=get_text('view_timeline'),
        style={'button_color': '#9b59b6'},
        layout=widgets.Layout(width='30%')
    )

    view_resume_button = widgets.Button(
        description=get_text('view_resume'),
        style={'button_color': '#f1c40f'},
        layout=widgets.Layout(width='30%')
    )

    # ---------- FORM ELEMENTS ----------

    # Basic information inputs
    name_input = widgets.Text(
        description=get_text('name'),
        placeholder=get_text('name_placeholder'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    email_input = widgets.Text(
        description=get_text('email'),
        placeholder=get_text('email_placeholder'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    education_input = widgets.Text(
        description=get_text('education'),
        placeholder=get_text('education_placeholder'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    linkedin_input = widgets.Text(
        description=get_text('linkedin'),
        placeholder='https://linkedin.com/in/username',
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    skills_input = widgets.Textarea(
        description=get_text('skills'),
        placeholder=get_text('skills_placeholder'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%', height='80px')
    )

    goals_input = widgets.Textarea(
        description=get_text('goals'),
        placeholder=get_text('goals_placeholder'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%', height='80px')
    )

    # Experience level dropdown
    experience_dropdown = widgets.Dropdown(
        options=get_text('experience_options'),
        description=get_text('experience_level'),
        value=get_text('experience_options')[0],
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    # Industry preference dropdown
    industry_dropdown = widgets.Dropdown(
        options=get_text('industry_options'),
        description=get_text('industry_preference'),
        value=get_text('industry_options')[0],
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

        # Work environment dropdown
    work_env_dropdown = widgets.Dropdown(
        options=get_text('work_env_options'),
        description=get_text('work_environment'),
        value=get_text('work_env_options')[0],
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    # Certifications section
    certifications_label = widgets.HTML(f"<h4>{get_text('certifications')}</h4>")
    certifications_list = widgets.Output()

    # Certification input fields
    cert_name = widgets.Text(
        placeholder='AWS Certified Solutions Architect',
        description=get_text('certification_name'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    cert_issuer = widgets.Text(
        placeholder='Amazon Web Services',
        description=get_text('certification_issuer'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    cert_date = widgets.Text(
        placeholder='2023-01',
        description=get_text('certification_date'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    add_cert_button = widgets.Button(
        description=get_text('add_certification'),
        style={'button_color': '#2ecc71'},
        layout=widgets.Layout(width='30%')
    )

    # Personality assessment dropdowns
    q1_dropdown = widgets.Dropdown(
        options=get_text('options'),
        description=get_text('q1'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    q2_dropdown = widgets.Dropdown(
        options=get_text('options'),
        description=get_text('q2'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    q3_dropdown = widgets.Dropdown(
        options=get_text('options'),
        description=get_text('q3'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    q4_dropdown = widgets.Dropdown(
        options=get_text('options'),
        description=get_text('q4'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    q5_dropdown = widgets.Dropdown(
        options=get_text('options'),
        description=get_text('q5'),
        style={'description_width': 'initial'},
        layout=widgets.Layout(width='80%')
    )

    # Create a submit button
    submit_button = widgets.Button(
        description=get_text('submit'),
        style={'button_color': '#3498db'},
        layout=widgets.Layout(width='30%')
    )

    # Create a navigation bar
    navigation_bar = widgets.HBox([
        submit_button,
        export_button,
        upload_resume_button
    ])

    # Create a chatbot interface
    chatbot_input = widgets.Text(
        placeholder=get_text('chatbot_placeholder'),
        layout=widgets.Layout(width='70%')
    )

    send_button = widgets.Button(
        description=get_text('send'),
        style={'button_color': '#9b59b6'},
        layout=widgets.Layout(width='20%')
    )

    # Group form elements
    basic_info_box = widgets.VBox([
        name_input,
        email_input,
        education_input,
        linkedin_input,
        skills_input,
        goals_input,
        experience_dropdown,
        industry_dropdown,
        work_env_dropdown
    ])

    certification_box = widgets.VBox([
        certifications_label,
        certifications_list,
        widgets.HBox([cert_name, cert_issuer, cert_date]),
        add_cert_button
    ])

    personality_box = widgets.VBox([
        q1_dropdown,
        q2_dropdown,
        q3_dropdown,
        q4_dropdown,
        q5_dropdown
    ])

    # Function to update UI language
    def update_language(change):
        global language
        language = change.new

        # Force redraw of entire UI based on current view
        redraw_ui()

    # Function to redraw the entire UI based on current view
    def redraw_ui():
        """Completely redraw the UI based on current view and language"""
        global current_view, language

        # Clear the output first
        clear_output()

        # Display title in current language
        display(HTML(f"<h2>{get_text('title')}</h2>"))

        # Update all UI elements based on current language
        name_input.description = get_text('name')
        email_input.description = get_text('email')
        education_input.description = get_text('education')
        linkedin_input.description = get_text('linkedin')
        skills_input.description = get_text('skills')
        goals_input.description = get_text('goals')

        q1_dropdown.description = get_text('q1')
        q1_dropdown.options = get_text('options')
        q2_dropdown.description = get_text('q2')
        q2_dropdown.options = get_text('options')
        q3_dropdown.description = get_text('q3')
        q3_dropdown.options = get_text('options')
        q4_dropdown.description = get_text('q4')
        q4_dropdown.options = get_text('options')
        q5_dropdown.description = get_text('q5')
        q5_dropdown.options = get_text('options')

        experience_dropdown.description = get_text('experience_level')
        experience_dropdown.options = get_text('experience_options')
        experience_dropdown.value = get_text('experience_options')[0]

        industry_dropdown.description = get_text('industry_preference')
        industry_dropdown.options = get_text('industry_options')
        industry_dropdown.value = get_text('industry_options')[0]

        work_env_dropdown.description = get_text('work_environment')
        work_env_dropdown.options = get_text('work_env_options')
        work_env_dropdown.value = get_text('work_env_options')[0]

        submit_button.description = get_text('submit')

        name_input.placeholder = get_text('name_placeholder')
        email_input.placeholder = get_text('email_placeholder')
        education_input.placeholder = get_text('education_placeholder')
        skills_input.placeholder = get_text('skills_placeholder')
        goals_input.placeholder = get_text('goals_placeholder')

        chatbot_input.placeholder = get_text('chatbot_placeholder')
        send_button.description = get_text('send')

        back_button.description = get_text('back_to_form')
        view_matches_button.description = get_text('view_matches')
        view_timeline_button.description = get_text('view_timeline')
        view_resume_button.description = get_text('view_resume')
        export_button.description = get_text('export_data')

        certifications_label.value = f"<h4>{get_text('certifications')}</h4>"
        cert_name.description = get_text('certification_name')
        cert_issuer.description = get_text('certification_issuer')
        cert_date.description = get_text('certification_date')
        add_cert_button.description = get_text('add_certification')

        dark_mode_toggle.description = get_text('dark_mode')
        accessibility_toggle.description = get_text('accessibility_mode')
        upload_resume_button.description = get_text('upload_resume')

        # Set RTL vs LTR layout based on language
        if language == 'ar':
            # Apply RTL styling
            display(HTML("""
            <style>
            .widget-label {
                text-align: right;
                direction: rtl;
            }

            .jupyter-widgets {
                direction: rtl;
            }
            </style>
            """))
        else:
            # Apply LTR styling
            display(HTML("""
            <style>
            .widget-label {
                text-align: left;
                direction: ltr;
            }

            .jupyter-widgets {
                direction: ltr;
            }
            </style>
            """))

        # Display appropriate view based on current_view
        if current_view == 'form':
            display(widgets.HBox([language_selector, widgets.VBox([dark_mode_toggle, accessibility_toggle])]))
            display(basic_info_box)
            display(certification_box)
            display(HTML(f"<h3>{get_text('personality_title')}</h3>"))
            display(personality_box)
            display(navigation_bar)
            display(HTML(f"<h3>{get_text('assistant_title')}</h3>"))
            display(chatbot_output)
            display(widgets.HBox([chatbot_input, send_button]))
        elif current_view == 'results':
            display(language_selector)
            display(results_output)
            display(visualization_output)
            display(widgets.HBox([back_button, view_timeline_button, view_resume_button]))
        elif current_view == 'timeline':
            display(language_selector)
            display(timeline_output)
            display(widgets.HBox([back_button, view_matches_button, view_resume_button]))
        elif current_view == 'resume':
            display(language_selector)
            display(resume_output)
            display(widgets.HBox([back_button, view_matches_button, view_timeline_button]))

        # Update chatbot output with welcome message
        with chatbot_output:
            clear_output()
            print("🤖 " + get_text('chatbot_welcome'))

    # Function to handle dark mode toggle
    def on_dark_mode_change(change):
        if change.new:
            # Apply dark mode styling
            display(HTML("""
            <style>
            .dark-mode-applied .jupyter-widgets {
                background-color: #333;
                color: #f0f0f0;
            }

            .dark-mode-applied input, .dark-mode-applied textarea, .dark-mode-applied select {
                background-color: #444;
                color: #f0f0f0;
                border: 1px solid #555;
            }

            .dark-mode-applied button {
                background-color: #555;
                color: #f0f0f0;
            }
            </style>
            """))

            # Add dark mode class to outputs
            display(Javascript("""
            document.body.classList.add('dark-mode-applied');
            """))
        else:
            # Remove dark mode class
            display(Javascript("""
            document.body.classList.remove('dark-mode-applied');
            """))

    # Function to handle accessibility mode toggle
    def on_accessibility_change(change):
        if change.new:
            # Apply accessibility styling
            display(HTML("""
            <style>
            .accessibility-applied .jupyter-widgets {
                font-size: 1.2em !important;
                line-height: 1.5 !important;
            }

            .accessibility-applied input, .accessibility-applied textarea, .accessibility-applied select {
                font-size: 1.1em !important;
                padding: 8px !important;
            }

            .accessibility-applied button {
                font-size: 1.1em !important;
                padding: 10px 15px !important;
            }

            .accessibility-applied .widget-label {
                font-weight: bold !important;
            }
            </style>
            """))

            # Add accessibility class to outputs
            display(Javascript("""
            document.body.classList.add('accessibility-applied');
            """))
        else:
            # Remove accessibility class
            display(Javascript("""
            document.body.classList.remove('accessibility-applied');
            """))

    # Function to handle resume upload
    def on_resume_upload(change):
        if not change.new:
            return

        # Get the uploaded file
        uploaded_file = next(iter(change.new.values()))
        content = uploaded_file['content']

        # Determine file type
        filename = uploaded_file['metadata']['name']
        file_ext = filename.split('.')[-1].lower()

        # Extract text from file (simplified example)
        extracted_text = ""
        try:
            if file_ext == 'txt':
                extracted_text = content.decode('utf-8')
            elif file_ext == 'pdf':
                # In a real implementation, use a library like PyPDF2 or pdfminer
                extracted_text = "PDF extraction placeholder - would use PyPDF2 in production"
            elif file_ext in ['docx', 'doc']:
                # In a real implementation, use a library like python-docx
                extracted_text = "DOCX extraction placeholder - would use python-docx in production"

            # In a real implementation, use AI to parse the resume and extract key information
            # Here we'll use a simplified approach

            # Extract name (look for name at beginning of resume)
            name_match = re.search(r'^([A-Za-z\s]{2,50})$', extracted_text, re.MULTILINE)
            if name_match:
                name_input.value = name_match.group(1).strip()

            # Extract email (look for email pattern)
            email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', extracted_text)
            if email_match:
                email_input.value = email_match.group(1)

            # Extract LinkedIn URL
            linkedin_match = re.search(r'(linkedin\.com/in/[a-zA-Z0-9_-]+)', extracted_text)
            if linkedin_match:
                linkedin_input.value = "https://" + linkedin_match.group(1)

            # Extract skills (look for 'Skills' section)
            skills_match = re.search(r'skills:?(.*?)(?:\n\n|\Z)', extracted_text, re.IGNORECASE | re.DOTALL)
            if skills_match:
                skills = skills_match.group(1).strip()
                # Clean up and format as comma-separated list
                skills = re.sub(r'[\n•\-]', ',', skills)
                skills = re.sub(r',+', ',', skills)
                skills_input.value = skills.strip(',')

            # Show confirmation message
            with chatbot_output:
                print(f"📄 {get_text('resume_upload_success')}")

        except Exception as e:
            # Handle extraction errors
            with chatbot_output:
                print(f"❌ {get_text('resume_upload_error')} {str(e)}")

    # Function to export user data
    def on_export_data(b):
        # Create a JSON representation of user data
        user_data_json = json.dumps(user_data, indent=2)

        # Create a CSV version of skills
        skills = user_data.get('Skills', '').split(',')
        skills_csv = ','.join([skill.strip() for skill in skills if skill.strip()])

        # Create HTML download links
        html_output = f"""
        <h3>{get_text('export_options')}</h3>
        <p><a href="data:application/json;charset=utf-8,{user_data_json}"
              download="career_profile.json"
              class="button">{get_text('download_json')}</a></p>

        <p><a href="data:text/csv;charset=utf-8,{skills_csv}"
              download="skills.csv"
              class="button">{get_text('download_csv')}</a></p>

        <style>
        .button {{
            display: inline-block;
            padding: 10px 15px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            margin: 5px 0;
        }}
        </style>
        """

        # Display download links
        with results_output:
            display(HTML(html_output))

    # Function to handle certification addition
    def on_add_certification(b):
        # Validate certification inputs
        if not cert_name.value or not cert_issuer.value:
            with certifications_list:
                print(f"❌ {get_text('cert_validation_error')}")
            return

        # Create a new certification entry
        certification = {
            'name': cert_name.value,
            'issuer': cert_issuer.value,
            'date': cert_date.value
        }

        # Add to user data
        if 'Certifications' not in user_data:
            user_data['Certifications'] = []

        user_data['Certifications'].append(certification)

        # Update UI
        with certifications_list:
            clear_output()
            print(f"📜 {get_text('certifications')}:")
            for cert in user_data['Certifications']:
                print(f"- {cert['name']} ({cert['issuer']}, {cert['date']})")

        # Clear input fields
        cert_name.value = ''
        cert_issuer.value = ''
        cert_date.value = ''

    # Function to send chatbot messages
    def on_send_button_clicked(b):
        query = chatbot_input.value

        if not query:
            return

        # Display user message
        with chatbot_output:
            print(f"🧑 {query}")

        # Clear input field
        chatbot_input.value = ''

        # Get and display chatbot response
        response = get_chatbot_response(query, language)

        with chatbot_output:
            print(f"🤖 {response}")

    # Function to handle form submission
    def on_submit_button_clicked(b):
        global career_matches, current_view

        # Collect user data
        user_data.update({
            'Name': name_input.value,
            'Email': email_input.value,
            'Education Status': education_input.value,
            'LinkedIn URL': linkedin_input.value,
            'Skills': skills_input.value,
            'Career Goals': goals_input.value,
            'Experience Level': experience_dropdown.value,
            'Industry Preference': industry_dropdown.value,
            'Work Environment': work_env_dropdown.value,
            'Personality Assessment': {
                'Team Orientation': q1_dropdown.index(q1_dropdown.value) + 1,
                'Problem Solving': q2_dropdown.index(q2_dropdown.value) + 1,
                'Detail Orientation': q3_dropdown.index(q3_dropdown.value) + 1,
                'Adaptability': q4_dropdown.index(q4_dropdown.value) + 1,
                'Structure Preference': q5_dropdown.index(q5_dropdown.value) + 1
            }
        })

        # Process certifications
        if 'Certifications' not in user_data:
            user_data['Certifications'] = []

        # Perform career matching
        career_matches = perform_career_matching(user_data)

        # Display results
        with results_output:
            clear_output()

            # Display success message
            display(HTML(f"<h3>{get_text('success_message')}</h3>"))

            # Display basic info summary
            display(HTML(f"<h4>{get_text('summary')}</h4>"))
            for key, value in user_data.items():
                if key != 'Personality Assessment' and key != 'Certifications':
                    display(HTML(f"<p><strong>{key}:</strong> {value}</p>"))

            # Display personality assessment
            display(HTML(f"<h4>{get_text('personality_assessment')}</h4>"))
            for key, value in user_data['Personality Assessment'].items():
                display(HTML(f"<p><strong>{key}:</strong> {value}/5</p>"))

            # Display certifications
            if user_data['Certifications']:
                display(HTML(f"<h4>{get_text('certifications')}</h4>"))
                for cert in user_data['Certifications']:
                    display(HTML(f"<p>{cert['name']} ({cert['issuer']}, {cert['date']})</p>"))

            # Display career matches
            display(HTML(f"<h4>{get_text('career_matches')}</h4>"))

            # Display top 5 matches
            top_matches = list(career_matches.items())[:5]
            for career_id, career_info in top_matches:
                if language == 'ar':
                    title = career_info['title_ar']
                else:
                    title = career_info['title']

                # Create a progress bar for the match percentage
                percentage = career_info['match_percentage']
                progress_html = f"""
                <div style="display: flex; align-items: center; margin-bottom: 10px;">
                    <div style="width: 150px;"><strong>{title}</strong></div>
                    <div style="flex-grow: 1; background-color: #f0f0f0; border-radius: 5px; margin: 0 10px;">
                        <div style="width: {percentage}%; height: 20px; background-color: #3498db; border-radius: 5px;"></div>
                    </div>
                    <div style="width: 40px;">{percentage}%</div>
                </div>
                """
                display(HTML(progress_html))

            # Display completion message
            display(HTML(f"<p>{get_text('completion_message')}</p>"))

        # Create visualization of career matches
        create_career_visualization(career_matches)

        # Update view state
        current_view = 'results'

        # Use the redraw function instead of manual redraw
        redraw_ui()

    # Function to view career timeline
    def on_view_timeline(b):
        global current_view

        if not career_matches:
            # If no matches yet, run the career matching first
            on_submit_button_clicked(None)

        # Get top career match
        top_career_id = list(career_matches.keys())[0]
        top_career = career_matches[top_career_id]

        # Get experience level
        experience_level = user_data.get('Experience Level', get_text('experience_options')[0])

        # Generate timeline
        timeline_data = generate_career_timeline(top_career, experience_level)

        # Create timeline visualization
        create_timeline_visualization(timeline_data)

        # Update view state
        current_view = 'timeline'

        # Use the redraw function
        redraw_ui()

    # Function to view resume preview
    def on_view_resume(b):
        global current_view

        if not career_matches:
            # If no matches yet, run the career matching first
            on_submit_button_clicked(None)

        # Get top career match
        top_career_id = list(career_matches.keys())[0]
        top_career = career_matches[top_career_id]

        # Generate resume preview
        resume_html = generate_resume_preview(user_data, top_career)

        # Display resume
        with resume_output:
            clear_output()
            display(HTML(f"<h2>{get_text('resume_preview')}</h2>"))
            display(HTML(resume_html))
            display(HTML(f"<p><em>{get_text('resume_tip')}</em></p>"))

        # Update view state
        current_view = 'resume'

        # Use the redraw function
        redraw_ui()

    # Function to go back to form
    def on_back_to_form(b):
        global current_view
        current_view = 'form'

        # Use the redraw function
        redraw_ui()

    # Connect button events
    send_button.on_click(on_send_button_clicked)
    submit_button.on_click(on_submit_button_clicked)
    view_matches_button.on_click(on_submit_button_clicked)  # Same as submit to ensure data is processed
    view_timeline_button.on_click(on_view_timeline)
    view_resume_button.on_click(on_view_resume)
    back_button.on_click(on_back_to_form)
    add_cert_button.on_click(on_add_certification)

    # Enable pressing Enter in the chatbot input
    def on_enter(widget):
        on_send_button_clicked(None)
    chatbot_input.on_submit(on_enter)

    # Connect language selector
    language_selector.observe(update_language, names='value')

    # Connect dark mode toggle
    dark_mode_toggle.observe(on_dark_mode_change, names='value')

    # Connect accessibility mode toggle
    accessibility_toggle.observe(on_accessibility_change, names='value')

    # Connect resume upload button
    upload_resume_button.observe(on_resume_upload, names='value')

    # Connect export data button
    export_button.on_click(on_export_data)

    # Display everything based on current view
    if current_view == 'form':
        display(HTML(f"<h2>{get_text('title')}</h2>"))
        display(widgets.HBox([language_selector, widgets.VBox([dark_mode_toggle, accessibility_toggle])]))
        display(basic_info_box)
        display(certification_box)
        display(HTML(f"<h3>{get_text('personality_title')}</h3>"))
        display(personality_box)
        display(navigation_bar)
        display(HTML(f"<h3>{get_text('assistant_title')}</h3>"))
        display(chatbot_output)
        display(widgets.HBox([chatbot_input, send_button]))
    elif current_view == 'results':
        display(HTML(f"<h2>{get_text('title')}</h2>"))
        display(language_selector)
        display(results_output)
        display(visualization_output)
        display(widgets.HBox([back_button, view_timeline_button, view_resume_button]))
    elif current_view == 'timeline':
        display(HTML(f"<h2>{get_text('title')}</h2>"))
        display(language_selector)
        display(timeline_output)
        display(widgets.HBox([back_button, view_matches_button, view_resume_button]))
    elif current_view == 'resume':
        display(HTML(f"<h2>{get_text('title')}</h2>"))
        display(language_selector)
        display(resume_output)
        display(widgets.HBox([back_button, view_matches_button, view_timeline_button]))

    # Return the widgets for potential reference elsewhere
    return {
        'user_data': user_data,
        'career_matches': career_matches,
        'form_elements': {
            'name_input': name_input,
            'email_input': email_input,
            'education_input': education_input,
            'linkedin_input': linkedin_input,
            'skills_input': skills_input,
            'goals_input': goals_input,
            'experience_dropdown': experience_dropdown,
            'industry_dropdown': industry_dropdown,
            'work_env_dropdown': work_env_dropdown,
            'certifications_list': certifications_list,
            'personality': {
                'q1': q1_dropdown,
                'q2': q2_dropdown,
                'q3': q3_dropdown,
                'q4': q4_dropdown,
                'q5': q5_dropdown
            }
        },
        'outputs': {
            'chatbot_output': chatbot_output,
            'results_output': results_output,
            'visualization_output': visualization_output,
            'timeline_output': timeline_output,
            'resume_output': resume_output
        },
        'buttons': {
            'send_button': send_button,
            'submit_button': submit_button,
            'back_button': back_button,
            'view_matches_button': view_matches_button,
            'view_timeline_button': view_timeline_button,
            'view_resume_button': view_resume_button,
            'export_button': export_button
        }
    }

# Run the enhanced user profile collection interface with AI career guidance
print("⏳ Initializing career profile builder...")
profile_interface = create_user_profile_ui()

# Apply any custom styling for the notebook
display(HTML("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500&display=swap');

.widget-label {
    font-family: 'Roboto', sans-serif;
    font-weight: 500;
}

.jupyter-widgets {
    font-family: 'Roboto', sans-serif;
}

.dark-mode {
    background-color: #2d2d2d;
    color: #f0f0f0;
}

.dark-mode .jupyter-widgets {
    color: #f0f0f0;
}

.dark-mode input, .dark-mode textarea, .dark-mode select {
    background-color: #3d3d3d;
    color: #f0f0f0;
    border: 1px solid #555;
}

.accessibility-mode {
    font-size: 1.2em;
    line-height: 1.5;
}

.accessibility-mode input, .accessibility-mode textarea, .accessibility-mode select {
    font-size: 1.1em;
    padding: 8px;
}

.accessibility-mode button {
    font-size: 1.1em;
    padding: 10px 15px;
}
</style>
"""))

print("✅ Career Profile Builder initialized. Please fill out the form and interact with Assistant Hessa for guidance on any career field.")

# Add an initial prompt encouraging users to ask the AI about careers
with profile_interface['outputs']['chatbot_output']:
    print("🤖 " + get_text('chatbot_welcome'))

# Add analytics tracking (optional - for production systems)
def track_user_interaction(event_type, event_data=None):
    """
    Track user interactions for analytics (dummy implementation)
    In a production environment, this would send data to an analytics service
    """
    if event_data is None:
        event_data = {}

    # Log interaction (in production, would send to analytics service)
    interaction_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"DEBUG: Tracked {event_type} at {interaction_time}")

    # Example implementation for a real analytics service:
    # requests.post(
    #     "https://analytics-api.example.com/events",
    #     json={
    #         "event_type": event_type,
    #         "timestamp": interaction_time,
    #         "user_id": "anonymous",  # Would use actual user ID in production
    #         "data": event_data
    #     },
    #     headers={"Authorization": "Bearer YOUR_API_KEY"}
    # )

# Setup event listeners for analytics (uncomment for production use)
# def setup_analytics():
#     # Track form submissions
#     profile_interface['buttons']['submit_button'].on_click(
#         lambda b: track_user_interaction("form_submit", {"fields_completed": len(user_data)})
#     )
#
#     # Track chatbot usage
#     profile_interface['buttons']['send_button'].on_click(
#         lambda b: track_user_interaction("chatbot_message", {"message_length": len(chatbot_input.value)})
#     )
#
#     # Track resume generation
#     profile_interface['buttons']['view_resume_button'].on_click(
#         lambda b: track_user_interaction("resume_view")
#     )
#
# setup_analytics()

# Cleanup function for resources (if needed)
def cleanup_resources():
    """Clean up any resources when the notebook is closed"""
    # Close any open file handles, connections, etc.
    pass

# Register cleanup to run when kernel is shut down
# This is a bit notebook-specific and implementation may vary
try:
    import atexit
    atexit.register(cleanup_resources)
except ImportError:
    pass

# Documentation on using the career profile builder
usage_instructions = """
# Career Profile Builder Usage Guide

This interactive career profile builder helps users explore career options with AI assistance.

## Main Features:
1. **Profile Form**: Fill out your basic information, skills, and career preferences
2. **AI Assistant**: Ask questions about any career field for personalized guidance
3. **Career Matching**: Get personalized career recommendations based on your profile
4. **Timeline View**: See a projected career progression timeline
5. **Resume Builder**: Generate a basic resume with recommended skills

## Getting Started:
1. Fill out the profile form with your information
2. Use the AI assistant to ask questions about careers
3. Click "Submit" to see your top career matches
4. Explore the timeline and resume views

## Tips:
- Be specific about your skills and interests for better matches
- Ask the AI about emerging fields like AI, blockchain, or cybersecurity
- Try different language settings if you prefer Arabic
- Use the export button to download your data
"""

# Uncomment to display usage instructions
# display(HTML(f"<details><summary>Usage Instructions</summary>{usage_instructions}</details>"))

# Additional resources and references
additional_resources = """
<h3>Additional Career Development Resources</h3>
<ul>
  <li><a href="https://www.linkedin.com/learning/" target="_blank">LinkedIn Learning</a> - Professional courses</li>
  <li><a href="https://www.coursera.org/" target="_blank">Coursera</a> - University-affiliated courses</li>
  <li><a href="https://www.udemy.com/" target="_blank">Udemy</a> - Practical skill development</li>
  <li><a href="https://www.edx.org/" target="_blank">edX</a> - Academic courses from top universities</li>
</ul>
"""

# Uncomment to display additional resources
# display(HTML(additional_resources))

# End of implementation
print("✨ Career Profile Builder ready for use!")

"""# Section 3: CV Upload + ATS Matching System

This section allows users to upload their CV/resume and analyze its compatibility with job descriptions, providing an ATS matching score and feedback for improvement.

Key features:
- CV upload and text extraction (PDF and DOCX formats)
- Matching against job descriptions
- Keyword analysis and matching
- Compatibility score calculation
- Customized feedback for improvement
"""

# Section C: CV Upload + ATS Matching System

# Install required packages if not already installed
!pip install -q pdfplumber python-docx spacy wordcloud openai keybert
!python -m spacy download en_core_web_sm

import os
import pdfplumber
import docx
import io
import re
import spacy
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import pandas as pd
from wordcloud import WordCloud
import json
from google.colab import userdata  # For accessing Colab secrets
from openai import OpenAI  # Ensure this import is present

# Ensure KeyBERT is installed
try:
    import keybert
except ImportError:
    print("KeyBERT not found. Installing...")
    !pip install keybert
    import keybert

from keybert import KeyBERT

# Load spaCy model
nlp = spacy.load("en_core_web_sm")

# Initialize keyword extraction models
keybert_model = KeyBERT()

# Retrieve DeepSeek API key from Colab secrets
def get_deepseek_api_key():
    try:
        # First, try to get the key from Colab secrets
        api_key = userdata.get('DEEPSEEK_API_KEY')

        # If not found in secrets, check environment variables
        if not api_key:
            api_key = os.environ.get('DEEPSEEK_API_KEY')

        # If still not found, prompt user
        if not api_key:
            print("DeepSeek API key not found. Please provide your API key:")
            api_key = input("Enter your DeepSeek API key: ")

        return api_key
    except Exception as e:
        print(f"Error accessing DeepSeek API key: {e}")
        return None

# Get the API key
DEEPSEEK_API_KEY = get_deepseek_api_key()

# DeepSeek R1 Client Setup
deepseek_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=DEEPSEEK_API_KEY
)

def extract_text_from_pdf(pdf_file):
    """Extract text from a PDF file."""
    text = ""
    try:
        with pdfplumber.open(pdf_file) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
    except Exception as e:
        print(f"Error extracting text from PDF: {e}")
    return text

def extract_text_from_docx(docx_file):
    """Extract text from a DOCX file."""
    text = ""
    try:
        doc = docx.Document(docx_file)
        for para in doc.paragraphs:
            text += para.text + "\n"
    except Exception as e:
        print(f"Error extracting text from DOCX: {e}")
    return text

def preprocess_text(text):
    """Clean and preprocess text."""
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\d+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_keywords(text, max_keywords=30):
    """Extract important keywords from text using spaCy."""
    doc = nlp(text)
    keywords = [token.text for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ'] and len(token.text) > 2]
    keyword_freq = {}
    for word in keywords:
        keyword_freq[word] = keyword_freq.get(word, 0) + 1
    sorted_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
    return [word for word, freq in sorted_keywords[:max_keywords]]

def calculate_ats_score(cv_text, job_description):
    """Calculate ATS compatibility score between CV and job description."""
    cv_text_processed = preprocess_text(cv_text)
    job_description_processed = preprocess_text(job_description)
    vectorizer = TfidfVectorizer(stop_words='english')
    vectors = vectorizer.fit_transform([cv_text_processed, job_description_processed])
    similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
    return similarity * 100

def create_word_cloud(text, title):
    """Create and display a word cloud."""
    wordcloud = WordCloud(width=800, height=400, background_color='white',
                          max_words=100, contour_width=3, contour_color='steelblue').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.tight_layout(pad=0)
    plt.show()

def highlight_matching_keywords(cv_text, job_description):
    """Identify matching keywords between CV and job description."""
    cv_keywords = set(extract_keywords(cv_text))
    job_keywords = set(extract_keywords(job_description))
    matching_keywords = cv_keywords.intersection(job_keywords)
    missing_keywords = job_keywords - cv_keywords
    return list(matching_keywords), list(missing_keywords)

def generate_ats_report(cv_text, job_description, job_title):
    """Generate a comprehensive ATS compatibility report."""
    score = calculate_ats_score(cv_text, job_description)
    matching_keywords, missing_keywords = highlight_matching_keywords(cv_text, job_description)

    if score >= 80:
        assessment = "Excellent! Your CV is highly compatible with this job."
        color = "green"
    elif score >= 60:
        assessment = "Good. Your CV matches the job requirements reasonably well."
        color = "blue"
    elif score >= 40:
        assessment = "Average. Consider optimizing your CV for better matching."
        color = "orange"
    else:
        assessment = "Low match. Your CV needs significant adjustments for this role."
        color = "red"

    print(f"\n🎯 ATS COMPATIBILITY SCORE: {score:.2f}%")
    print(f"\n📋 Job Title: {job_title}")
    print(f"\n📊 Assessment: {assessment}")

    print(f"\n✅ MATCHING KEYWORDS ({len(matching_keywords)}):")
    for keyword in sorted(matching_keywords):
        print(f"  • {keyword}")

    print(f"\n❌ MISSING KEYWORDS ({len(missing_keywords)}):")
    for keyword in sorted(missing_keywords)[:15]:
        print(f"  • {keyword}")

    print("\n📝 RECOMMENDATIONS:")
    if score < 60:
        print("  • Add more of the missing keywords to your CV where applicable")
        print("  • Reformat your CV to highlight relevant experience")
        print("  • Use industry-specific terminology that appears in the job description")
    print("  • Tailor your CV for each specific job application")
    print("  • Use action verbs to describe achievements rather than responsibilities")

    return {
        "score": score,
        "job_title": job_title,
        "matching_keywords": matching_keywords,
        "missing_keywords": missing_keywords,
        "assessment": assessment
    }

def extract_keywords_advanced(text, method="keybert", top_n=20):
    """Extract keywords using advanced methods (KeyBERT)."""
    if method == "keybert":
        # Use KeyBERT with more flexible extraction
        keywords = keybert_model.extract_keywords(
            text,
            keyphrase_ngram_range=(1, 2),  # Extract 1-2 word phrases
            stop_words='english',  # Remove common English stop words
            top_n=top_n,  # Number of top keywords to extract
            use_mmr=True,  # Use Maximal Marginal Relevance for diversity
            diversity=0.5  # Balance between relevance and diversity
        )
        return [keyword for keyword, score in keywords]
    else:
        # Fallback to existing spaCy method
        return extract_keywords(text, max_keywords=top_n)

def analyze_resume_with_deepseek(cv_text, job_description):
    """Analyze resume using DeepSeek R1 with a comprehensive prompt."""
    if not DEEPSEEK_API_KEY:
        return "DeepSeek API key is not configured. Please add it to Colab secrets or environment variables."

    # Truncate texts to manage token limits
    cv_text_truncated = cv_text[:4000]  # Adjust as needed
    job_description_truncated = job_description[:2000]

    prompt = f"""You are an expert ATS (Applicant Tracking System) evaluator.
Compare the following resume and job description with a critical and constructive approach.

RESUME:
{cv_text_truncated}

JOB DESCRIPTION:
{job_description_truncated}

Provide a comprehensive analysis with the following structure:

1. STRENGTHS 🟢
- Highlight the top 5 strengths of the resume
- Explain how these strengths align with the job description

2. WEAKNESSES 🔴
- Identify up to 5 key weaknesses or missing elements
- Suggest specific improvements for each weakness

3. KEYWORD ANALYSIS 🔍
- Top 10 matching keywords
- Top 5 missing keywords
- Recommendations for incorporating missing keywords

4. OVERALL MATCH 📊
- Provide an estimated percentage match
- Detailed reasoning behind the match percentage

5. IMPROVEMENT SUGGESTIONS 🚀
- Concrete, actionable recommendations to enhance resume
- Specific edits to better align with job description

6. CAREER DEVELOPMENT ROADMAP 🚀
- Personalized skill gap analysis
- Comprehensive learning strategy
- Technical and soft skill development
- Career positioning guidance
- Actionable recommendations

Maintain a professional, constructive tone. Focus on helping the candidate improve their resume."""

    try:
        response = deepseek_client.chat.completions.create(
            model="deepseek/deepseek-r1:free",
            messages=[
                {"role": "system", "content": "You are an expert ATS evaluator providing detailed, constructive resume analysis."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error using DeepSeek for analysis: {e}")
        return "DeepSeek analysis unavailable. Please check your API configuration."

def get_keywords_json(cv_text, job_description):
    """Get keywords from both resume and job description in JSON format."""
    cv_keywords = extract_keywords_advanced(cv_text, method="keybert")
    job_keywords = extract_keywords_advanced(job_description, method="keybert")
    matching_keywords = list(set(cv_keywords).intersection(set(job_keywords)))
    missing_keywords = list(set(job_keywords) - set(cv_keywords))

    keywords_json = {
        "resume_keywords": cv_keywords,
        "job_keywords": job_keywords,
        "matching_keywords": matching_keywords,
        "missing_keywords": missing_keywords
    }

    return json.dumps(keywords_json, indent=2)

def get_improvement_roadmap(cv_text, job_description):
    """
    Generate a comprehensive improvement roadmap using DeepSeek
    """
    prompt = f"""You are an expert career development advisor and AI system architect.
Analyze the following resume and job description to provide a comprehensive, strategic improvement roadmap.

RESUME:
{cv_text}

JOB DESCRIPTION:
{job_description}

Please provide a detailed analysis with the following structure:

1. SKILL GAP ANALYSIS 🔍
- Identify critical skills missing from the current resume
- Compare current skills with job requirements
- Rank skills by importance and difficulty to acquire

2. LEARNING ROADMAP 📚
- Recommended courses and certifications
- Suggested learning platforms
- Estimated time to skill proficiency
- Specific resources for each skill gap

3. CAREER DEVELOPMENT STRATEGY 🚀
- Short-term (3-6 months) skill development plan
- Medium-term (6-12 months) career positioning
- Long-term career trajectory alignment

4. TECHNICAL SKILL ENHANCEMENT 💻
- Specific programming languages/tools to learn
- Recommended projects to build practical experience
- Industry-recognized certifications

5. SOFT SKILLS DEVELOPMENT 🤝
- Communication and interpersonal skill improvements
- Leadership and collaboration skill enhancement
- Emotional intelligence and adaptability training

6. PORTFOLIO AND VISIBILITY STRATEGY 📈
- How to showcase newly acquired skills
- Networking recommendations
- Personal branding and professional online presence

7. MENTORSHIP AND GUIDANCE 🤝
- Suggested mentorship approaches
- Professional networks to join
- Industry conferences and events

Provide actionable, specific, and motivational guidance that transforms the current skill set into a competitive, future-ready professional profile."""

    try:
        response = deepseek_client.chat.completions.create(
            model="deepseek/deepseek-r1:free",
            messages=[
                {"role": "system", "content": "You are an expert career development strategist providing comprehensive, personalized improvement guidance."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error generating improvement roadmap: {e}")
        return "Analysis unavailable. Please check system configuration."

def enhanced_ats_report(cv_text, job_description, job_title):
    """Generate an enhanced ATS report with DeepSeek analysis."""
    # Get basic ATS score and keywords
    score = calculate_ats_score(cv_text, job_description)
    matching_keywords, missing_keywords = highlight_matching_keywords(cv_text, job_description)

    # Get DeepSeek analysis
    deepseek_analysis = analyze_resume_with_deepseek(cv_text, job_description)

    # Generate improvement roadmap
    improvement_roadmap = get_improvement_roadmap(cv_text, job_description)

    # Create assessment message
    if score >= 80:
        assessment = "Excellent! Your resume is highly compatible with this job."
        color = "green"
    elif score >= 60:
        assessment = "Good. Your resume matches the job requirements reasonably well."
        color = "blue"
    elif score >= 40:
        assessment = "Average. Consider optimizing your resume for better matching."
        color = "orange"
    else:
        assessment = "Low match. Your resume needs significant adjustments for this role."
        color = "red"

    print(f"\n🎯 ATS COMPATIBILITY SCORE: {score:.2f}%")
    print(f"\n📋 Job Title: {job_title}")
    print(f"\n📊 Assessment: {assessment}")

    print(f"\n✅ MATCHING KEYWORDS ({len(matching_keywords)}):")
    for keyword in sorted(matching_keywords):
        print(f"  • {keyword}")

    print(f"\n❌ MISSING KEYWORDS ({len(missing_keywords)}):")
    for keyword in sorted(missing_keywords)[:15]:
        print(f"  • {keyword}")

    print("\n🔍 DETAILED ANALYSIS (DeepSeek R1):")
    print(deepseek_analysis)

    print("\n🚀 CAREER DEVELOPMENT ROADMAP:")
    print(improvement_roadmap)

    print("\n📝 RECOMMENDATIONS:")
    if score < 60:
        print("  • Add more of the missing keywords to your resume where applicable")
        print("  • Reformat your resume to highlight relevant experience")
        print("  • Use industry-specific terminology that appears in the job description")
    print("  • Tailor your resume for each specific job application")
    print("  • Use action verbs to describe achievements rather than responsibilities")

    return {
        "score": score,
        "job_title": job_title,
        "matching_keywords": matching_keywords,
        "missing_keywords": missing_keywords,
        "assessment": assessment,
        "llm_analysis": deepseek_analysis,
        "improvement_roadmap": improvement_roadmap
    }

# Expanded sample job descriptions for various roles
sample_job_descriptions = {
    "Data Scientist": """
    We are looking for a Data Scientist to analyze large datasets and build predictive models.
    Requirements:
    - Advanced degree in Statistics, Mathematics, Computer Science or related field.
    - 3+ years experience with Python, R, SQL.
    - Experience with machine learning algorithms.
    - Knowledge of TensorFlow, PyTorch, scikit-learn.
    - Strong background in statistical analysis and data visualization.
    """,
    "Software Engineer": """
    We're hiring a Software Engineer to develop high-quality applications.
    Requirements:
    - Bachelor's degree in Computer Science or related field.
    - 2+ years experience in software development.
    - Proficiency in Java, Python, or C++.
    - Experience with web frameworks and version control systems.
    - Strong problem-solving skills.
    """,
    "Marketing Manager": """
    Seeking a Marketing Manager to develop and implement marketing strategies.
    Requirements:
    - Bachelor's degree in Marketing or related field.
    - 5+ years of marketing experience.
    - Expertise in digital marketing, social media, and content creation.
    - Experience with SEO/SEM and marketing analytics.
    - Excellent communication and presentation skills.
    """,
    "AI Engineer": """
    We are seeking an AI Engineer to design and develop innovative AI models.
    Requirements:
    - Advanced degree in Computer Science, AI, or a related field.
    - Strong programming skills in Python and experience with deep learning frameworks.
    - Knowledge of neural networks, NLP, and computer vision.
    - Proven experience in developing and deploying AI solutions.
    """,
    "Cybersecurity Analyst": """
    We are looking for a Cybersecurity Analyst to safeguard our systems and data.
    Requirements:
    - Degree in Cybersecurity, Information Technology, or a related field.
    - Experience with threat analysis, vulnerability assessments, and risk management.
    - Proficiency with firewalls, SIEM tools, and security protocols.
    - Strong analytical and problem-solving skills.
    """,
    "Data Analyst": """
    We are hiring a Data Analyst to interpret complex data sets and provide actionable insights.
    Requirements:
    - Degree in Statistics, Mathematics, or a related field.
    - Proficiency in SQL, Python or R, and data visualization tools.
    - Strong analytical skills and attention to detail.
    - Experience with data cleaning, mining, and reporting.
    """,
    "Full Stack Developer": """
    Join our team as a Full Stack Developer to work on both front-end and back-end systems.
    Requirements:
    - Bachelor's degree in Computer Science or related field.
    - Proficiency in HTML, CSS, JavaScript, and back-end languages like Python, Java, or Ruby.
    - Experience with databases, APIs, and version control.
    - Excellent problem-solving and communication skills.
    """,
    "UI/UX Designer": """
    We are seeking a creative UI/UX Designer to enhance our user experience.
    Requirements:
    - Degree in Design, HCI, or a related field.
    - Proficiency in design tools like Adobe XD, Sketch, or Figma.
    - A strong portfolio demonstrating user-centered design.
    - Excellent communication and collaboration skills.
    """,
    "HR Business Partner": """
    Our HR Business Partner will support business units with strategic HR initiatives.
    Requirements:
    - Degree in Human Resources, Business Administration, or related field.
    - Experience in HR management and employee relations.
    - Strong communication and organizational skills.
    - Ability to work in a dynamic, fast-paced environment.
    """,
    "Cloud Solutions Architect": """
    We are looking for a Cloud Solutions Architect to design and implement our cloud strategies.
    Requirements:
    - Degree in Computer Science or related field.
    - Proven experience with AWS, Azure, or Google Cloud Platform.
    - Strong understanding of cloud architecture, security, and scalability.
    - Excellent problem-solving and project management skills.
    """,
    "Software Tester": """
    We are hiring a Software Tester to ensure the quality of our software products.
    Requirements:
    - Experience with software testing methodologies and tools.
    - Ability to design and execute test cases.
    - Familiarity with programming languages and automation tools.
    - Strong analytical and troubleshooting skills.
    """,
    "Machine Learning Engineer": """
    We are seeking a Machine Learning Engineer to build and deploy machine learning models.
    Requirements:
    - Degree in Computer Science, AI, or a related field.
    - Proficiency in Python and machine learning libraries.
    - Experience with deep learning frameworks and data analysis.
    - Strong understanding of machine learning algorithms and techniques.
    """,
    "DevOps Engineer": """
    Join our team as a DevOps Engineer to streamline our development and deployment processes.
    Requirements:
    - Experience with continuous integration/delivery tools.
    - Knowledge of cloud platforms and containerization (Docker, Kubernetes).
    - Strong scripting and automation skills.
    - Excellent communication and teamwork abilities.
    """,
    "Front-End Developer": """
    We are hiring a Front-End Developer to build responsive and engaging web interfaces.
    Requirements:
    - Proficiency in HTML, CSS, and JavaScript.
    - Experience with frameworks like React, Angular, or Vue.
    - Strong design sense and attention to detail.
    - Ability to work collaboratively with designers and back-end developers.
    """,
    "Back-End Developer": """
    We are seeking a Back-End Developer to design robust server-side applications.
    Requirements:
    - Proficiency in Python, Java, or C#.
    - Experience with database systems, API development, and server architecture.
    - Strong problem-solving skills and attention to detail.
    - Familiarity with version control systems.
    """,
    "Database Administrator": """
    We are looking for a Database Administrator to manage and optimize our databases.
    Requirements:
    - Degree in Computer Science or a related field.
    - Experience with SQL databases, performance tuning, and backup strategies.
    - Strong analytical and troubleshooting skills.
    - Knowledge of database security and data integrity best practices.
    """,
    "IT Support Specialist": """
    We are hiring an IT Support Specialist to provide technical assistance and troubleshooting.
    Requirements:
    - Experience in IT support and customer service.
    - Knowledge of hardware, software, and networking fundamentals.
    - Excellent communication and problem-solving skills.
    - Ability to work under pressure and handle multiple tasks.
    """,
    "Product Manager": """
    We are seeking a Product Manager to oversee product development and lifecycle.
    Requirements:
    - Degree in Business, Engineering, or a related field.
    - Experience in product management and market research.
    - Strong leadership and strategic planning skills.
    - Ability to communicate effectively with cross-functional teams.
    """,
    "Business Intelligence Analyst": """
    We are hiring a Business Intelligence Analyst to convert data into actionable insights.
    Requirements:
    - Degree in Business, Statistics, or related field.
    - Proficiency in BI tools, SQL, and data visualization.
    - Strong analytical and problem-solving skills.
    - Experience with data warehousing and reporting.
    """,
    "IT Project Manager": """
    We are looking for an IT Project Manager to lead and coordinate technology projects.
    Requirements:
    - Degree in Information Technology or a related field.
    - Experience in project management and Agile methodologies.
    - Strong leadership, organizational, and communication skills.
    - Ability to manage timelines and deliverables effectively.
    """,
    "Digital Marketing Specialist": """
    Join our team as a Digital Marketing Specialist to drive online marketing efforts.
    Requirements:
    - Degree in Marketing or a related field.
    - Experience in SEO/SEM, social media, and digital advertising.
    - Proficiency with analytics and marketing tools.
    - Strong creative and strategic thinking skills.
    """,
    "Network Security Engineer": """
    We are seeking a Network Security Engineer to protect and monitor our network infrastructure.
    Requirements:
    - Degree in Computer Science, Cybersecurity, or a related field.
    - Experience with firewalls, VPNs, and intrusion detection systems.
    - Strong understanding of network security protocols.
    - Excellent analytical and troubleshooting abilities.
    """,
    "Penetration Tester (Ethical Hacker)": """
    We are hiring a Penetration Tester to identify vulnerabilities in our systems.
    Requirements:
    - Degree in Cybersecurity or a related field.
    - Proven experience with penetration testing and ethical hacking.
    - Proficiency with testing tools and scripting languages.
    - Strong analytical and communication skills.
    """,
    "Computer Vision Engineer": """
    We are looking for a Computer Vision Engineer to develop image processing and recognition systems.
    Requirements:
    - Degree in Computer Science or a related field.
    - Experience with computer vision libraries and deep learning frameworks.
    - Proficiency in Python or C++.
    - Strong analytical skills and creative problem solving.
    """,
    "NLP Engineer": """
    We are seeking an NLP Engineer to work on language processing and understanding.
    Requirements:
    - Degree in Computer Science, Linguistics, or a related field.
    - Experience with NLP libraries and models.
    - Proficiency in Python.
    - Strong understanding of language models and machine learning.
    """,
    "Scrum Master": """
    We are hiring a Scrum Master to facilitate Agile processes within our teams.
    Requirements:
    - Certification in Scrum or Agile methodologies.
    - Experience leading Agile teams.
    - Excellent communication and conflict resolution skills.
    - Ability to manage project timelines and adapt to changing requirements.
    """,
    "Mobile App Developer": """
    Join our team as a Mobile App Developer to create innovative mobile applications.
    Requirements:
    - Proficiency in mobile development frameworks (Swift, Kotlin, or React Native).
    - Experience with app design and user experience.
    - Strong problem-solving and debugging skills.
    - Ability to collaborate with designers and back-end developers.
    """,
    "Blockchain Developer": """
    We are seeking a Blockchain Developer to build decentralized applications.
    Requirements:
    - Degree in Computer Science or a related field.
    - Experience with blockchain technologies and smart contracts.
    - Proficiency in Solidity or similar languages.
    - Strong analytical skills and attention to detail.
    """,
    "Robotics Engineer": """
    We are hiring a Robotics Engineer to design, develop, and maintain robotic systems.
    Requirements:
    - Degree in Robotics, Mechanical Engineering, or related field.
    - Experience with robotic systems design and control.
    - Proficiency in relevant programming languages.
    - Excellent problem-solving and technical skills.
    """,
    "Technical Support Engineer": """
    We are seeking a Technical Support Engineer to provide technical assistance to customers.
    Requirements:
    - Experience in technical support and troubleshooting.
    - Knowledge of hardware, software, and networking.
    - Excellent communication and customer service skills.
    - Ability to work in a fast-paced environment.
    """,
    "Systems Administrator": """
    We are hiring a Systems Administrator to manage and maintain our IT infrastructure.
    Requirements:
    - Experience with system administration and network management.
    - Proficiency in Linux, Windows, or other operating systems.
    - Strong technical and troubleshooting skills.
    - Ability to work independently and manage multiple tasks.
    """,
    "IT Auditor": """
    We are looking for an IT Auditor to evaluate our systems and ensure regulatory compliance.
    Requirements:
    - Degree in Information Systems, Accounting, or related field.
    - Experience with IT audits and risk assessment.
    - Strong analytical and communication skills.
    - Knowledge of industry regulations and best practices.
    """,
    "Finance Manager": """
    We are hiring a Finance Manager to oversee financial operations and strategic planning.
    Requirements:
    - Degree in Finance, Accounting, or a related field.
    - Proven experience in financial management and budgeting.
    - Strong analytical and leadership skills.
    - Proficiency in financial software and reporting tools.
    """,
    "Project Manager": """
    We are seeking a Project Manager to coordinate projects across multiple departments.
    Requirements:
    - Degree in Business, Engineering, or related field.
    - Experience in project management and team leadership.
    - Excellent organizational and communication skills.
    - Ability to manage timelines, resources, and stakeholder expectations.
    """
}

# Create the enhanced UI for CV upload and analysis
def display_enhanced_cv_upload_ui():
    from google.colab import files
    import ipywidgets as widgets
    from IPython.display import display, HTML, JSON

    print("📄 ADVANCED RESUME ATS ANALYZER 📄")
    print("Upload your resume (PDF or DOCX format):")

    upload_button = widgets.Button(description="Upload Resume")
    output = widgets.Output()

    job_description_area = widgets.Textarea(
        value='',
        placeholder='Paste the job description here...',
        description='Job Description:',
        disabled=False,
        layout={'width': '100%', 'height': '200px'}
    )

    analyze_button = widgets.Button(description="Analyze Resume")
    keywords_button = widgets.Button(description="Get Keywords (JSON)")
    analysis_output = widgets.Output()
    json_output = widgets.Output()

    def on_upload_click(b):
        with output:
            output.clear_output()
            print("Please select a file...")
            global uploaded_file, cv_text
            uploaded_file = None
            cv_text = ""

            try:
                uploaded = files.upload()
                if not uploaded:
                    print("No file selected.")
                    return

                file_name = list(uploaded.keys())[0]
                file_data = list(uploaded.values())[0]
                file_ext = os.path.splitext(file_name)[1].lower()

                if file_ext == '.pdf':
                    pdf_io = io.BytesIO(file_data)
                    cv_text = extract_text_from_pdf(pdf_io)
                elif file_ext in ['.docx', '.doc']:
                    docx_io = io.BytesIO(file_data)
                    cv_text = extract_text_from_docx(docx_io)
                else:
                    print(f"Unsupported file format: {file_ext}. Please upload a PDF or DOCX file.")
                    return

                if cv_text:
                    print(f"✅ Successfully extracted text from {file_name} ({len(cv_text)} characters)")
                    uploaded_file = file_name
                else:
                    print("⚠️ Could not extract text from the file. Please try another file.")
            except Exception as e:
                print(f"Error processing file: {e}")

    def on_analyze_click(b):
        with analysis_output:
            analysis_output.clear_output()

            if not cv_text or len(cv_text) < 50:
                print("⚠️ Please upload a valid resume first.")
                return

            if not job_description_area.value.strip():
                print("⚠️ Please enter a job description.")
                return

            job_description = job_description_area.value
            job_title = "Job Position"  # Could extract from job description with NLP

            print("Analyzing resume compatibility...")
            report = enhanced_ats_report(cv_text, job_description, job_title)

            create_word_cloud(cv_text, f"Resume Word Cloud - {uploaded_file}")
            create_word_cloud(job_description, "Job Description Word Cloud")

            plt.figure(figsize=(10, 6))
            plt.barh(["Compatibility Score"], [report["score"]], color='steelblue')
            plt.xlim(0, 100)
            for i, v in enumerate([report["score"]]):
                plt.text(v + 1, i, f"{v:.1f}%", va='center')
            plt.title(f"ATS Compatibility Score")
            plt.tight_layout()
            plt.show()

    def on_keywords_click(b):
        with json_output:
            json_output.clear_output()

            if not cv_text or len(cv_text) < 50:
                print("⚠️ Please upload a valid resume first.")
                return

            if not job_description_area.value.strip():
                print("⚠️ Please enter a job description.")
                return

            job_description = job_description_area.value

            print("Extracting keywords...")
            keywords_json = get_keywords_json(cv_text, job_description)
            display(JSON(keywords_json))

    upload_button.on_click(on_upload_click)
    analyze_button.on_click(on_analyze_click)
    keywords_button.on_click(on_keywords_click)

    display(upload_button)
    display(output)
    display(job_description_area)
    display(analyze_button)
    display(keywords_button)
    display(analysis_output)
    display(json_output)

# Run the enhanced CV upload UI
display_enhanced_cv_upload_ui()

"""# Section 4: AI Job Application Workflow

"""

# Section D: AI Job Application Workflow

# Install required libraries
!pip install -q openai requests beautifulsoup4 selenium webdriver_manager

import os
import json
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from google.colab import userdata
from openai import OpenAI
from datetime import datetime

# DeepSeek Client Setup (reusing from previous section)
deepseek_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=userdata.get('DEEPSEEK_API_KEY')
)

def generate_top_job_roles(cv_text):
    """
    Generate top 25 job roles suitable for the candidate based on their CV
    """
    prompt = f"""Analyze the following CV and identify the top 25 job roles
    that best match the candidate's skills, experience, and background.

    CV DETAILS:
    {cv_text}

    For each job role, provide:
    1. Job Title
    2. Estimated Compatibility Percentage
    3. Key Matching Skills
    4. Potential Career Growth
    5. Recommended Skill Enhancements

    Ranking Criteria:
    - Skill Alignment
    - Experience Relevance
    - Career Potential
    - Industry Trends
    - Skill Transferability

    Output Format:
    [
        {{
            "job_title": "Job Title",
            "compatibility": 85,
            "matching_skills": ["Skill1", "Skill2"],
            "career_growth": "Short description",
            "skill_enhancements": ["Recommendation1", "Recommendation2"]
        }},
        // ... 24 more entries
    ]"""

    try:
        response = deepseek_client.chat.completions.create(
            model="deepseek/deepseek-r1:free",
            messages=[
                {"role": "system", "content": "You are an expert career matching AI that provides comprehensive job role recommendations."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.7,
            max_tokens=2000
        )

        # Parse the JSON response
        job_roles = json.loads(response.choices[0].message.content)
        return job_roles
    except Exception as e:
        print(f"Error generating job roles: {e}")
        return []

class JobApplicationWorkflow:
    def __init__(self, cv_text):
        self.cv_text = cv_text
        self.top_job_roles = []
        self.selected_roles = []
        self.selected_emirates = []
        self.application_strategy = None

    def generate_job_roles(self):
        """Generate top job roles based on CV"""
        self.top_job_roles = generate_top_job_roles(self.cv_text)

        print("🌟 TOP 25 RECOMMENDED JOB ROLES 🌟")
        for i, job in enumerate(self.top_job_roles, 1):
            print(f"{i}. {job['job_title']} (Compatibility: {job['compatibility']}%)")

        return self.top_job_roles

    def select_job_roles(self, num_roles=4):
        """Interactive job role selection"""
        while len(self.selected_roles) < num_roles:
            try:
                choice = int(input(f"Select a job role (1-{len(self.top_job_roles)}, or 0 to finish): "))
                if choice == 0 and self.selected_roles:
                    break
                if 1 <= choice <= len(self.top_job_roles):
                    selected_role = self.top_job_roles[choice - 1]
                    if selected_role not in self.selected_roles:
                        self.selected_roles.append(selected_role)
                        print(f"Added: {selected_role['job_title']}")
                else:
                    print("Invalid selection. Try again.")
            except ValueError:
                print("Please enter a valid number.")

        return self.selected_roles

    def select_emirates(self):
        """Emirates selection method"""
        uae_emirates = [
            "Abu Dhabi", "Dubai", "Sharjah", "Ajman",
            "Umm Al Quwain", "Ras Al Khaimah", "Fujairah"
        ]

        print("\n🏙️ SELECT EMIRATES FOR JOB SEARCH")
        for i, emirate in enumerate(uae_emirates, 1):
            print(f"{i}. {emirate}")

        while len(self.selected_emirates) < 1:
            try:
                choice = int(input(f"Select Emirates (1-{len(uae_emirates)}, or 0 to finish): "))
                if choice == 0 and self.selected_emirates:
                    break
                if 1 <= choice <= len(uae_emirates):
                    emirate = uae_emirates[choice - 1]
                    if emirate not in self.selected_emirates:
                        self.selected_emirates.append(emirate)
                        print(f"Added: {emirate}")
                else:
                    print("Invalid selection. Try again.")
            except ValueError:
                print("Please enter a valid number.")

        return self.selected_emirates

    def generate_application_strategy(self):
        """Generate comprehensive application strategy"""
        prompt = f"""Create a detailed job application strategy:

        Selected Job Roles: {[role['job_title'] for role in self.selected_roles]}
        Selected Emirates: {self.selected_emirates}

        Provide a comprehensive strategy including:
        1. Platform-specific application approach
        2. Networking strategies
        3. CV and cover letter customization tips
        4. Interview preparation recommendations
        5. Platforms to use in UAE:
           - LinkedIn
           - Bayt.com
           - GulfTalent
           - Indeed UAE
           - Naukrigulf
           - Monster Gulf

        Detailed Action Plan:
        - Platform-specific profile optimization
        - Networking techniques
        - Application tracking method
        - Follow-up strategies
        - Skill enhancement recommendations"""

        try:
            response = deepseek_client.chat.completions.create(
                model="deepseek/deepseek-r1:free",
                messages=[
                    {"role": "system", "content": "You are an expert career counselor providing a comprehensive job application strategy."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            self.application_strategy = response.choices[0].message.content

            print("\n🚀 COMPREHENSIVE JOB APPLICATION STRATEGY")
            print(self.application_strategy)

            return self.application_strategy
        except Exception as e:
            print(f"Error generating application strategy: {e}")
            return "Strategy generation failed."

    def start_application_workflow(self):
        """Execute full job application workflow"""
        print("🌐 AI JOB APPLICATION ASSISTANT 🌐")

        # Generate job roles
        self.generate_job_roles()

        # Select job roles
        self.select_job_roles()

        # Select emirates
        self.select_emirates()

        # Generate application strategy
        self.generate_application_strategy()

        return {
            "selected_roles": self.selected_roles,
            "selected_emirates": self.selected_emirates,
            "application_strategy": self.application_strategy
        }

# Example usage
def run_job_application_workflow(cv_text):
    workflow = JobApplicationWorkflow(cv_text)
    return workflow.start_application_workflow()

# Optional: Add platform-specific job application methods
class JobApplicationPlatforms:
    def __init__(self, cv_path, job_details, user_profile, selected_emirates):
        """
        Initialize job application agent with comprehensive details

        Args:
        - cv_path: Path to resume/CV
        - job_details: Dictionary containing job information
        - user_profile: Dictionary with user's professional details
        - selected_emirates: List of selected emirates for job search
        """
        self.cv_path = cv_path
        self.job_details = job_details
        self.user_profile = user_profile
        self.selected_emirates = selected_emirates

        # Selenium WebDriver setup
        self.driver = self._setup_webdriver()

        # AI-powered application tracking
        self.application_log = []

        # UAE Job Platforms
        self.uae_platforms = {
            "Dubai": [
                self.apply_on_bayt,
                self.apply_on_linkedin,
                self.apply_on_gulftalent,
                self.apply_on_naukrigulf,
                self.apply_on_indeed_uae
            ],
            "Abu Dhabi": [
                self.apply_on_bayt,
                self.apply_on_linkedin,
                self.apply_on_monster_gulf,
                self.apply_on_gulftalent
            ],
            "Sharjah": [
                self.apply_on_bayt,
                self.apply_on_linkedin,
                self.apply_on_gulftalent,
                self.apply_on_naukrigulf
            ],
            "Ajman": [
                self.apply_on_bayt,
                self.apply_on_linkedin,
                self.apply_on_gulftalent
            ],
            "Umm Al Quwain": [
                self.apply_on_bayt,
                self.apply_on_linkedin
            ],
            "Ras Al Khaimah": [
                self.apply_on_bayt,
                self.apply_on_linkedin,
                self.apply_on_gulftalent
            ],
            "Fujairah": [
                self.apply_on_bayt,
                self.apply_on_linkedin
            ]
        }

    def _setup_webdriver(self):
        """Set up automated web browser"""
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options

        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Run in background
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")

        driver = webdriver.Chrome(options=chrome_options)
        return driver

    def generate_personalized_cover_letter(self):
        """
        Generate AI-powered personalized cover letter
        """
        cover_letter_prompt = f"""
        Generate a personalized cover letter based on:

        Job Details:
        {self.job_details}

        Candidate Profile:
        {self.user_profile}

        Requirements:
        - Tailor to specific job description
        - Highlight matching skills
        - Professional and engaging tone
        - Maximum 500 words
        """

        try:
            response = deepseek_client.chat.completions.create(
                model="deepseek/deepseek-r1:free",
                messages=[
                    {"role": "system", "content": "You are an expert cover letter writer"},
                    {"role": "user", "content": cover_letter_prompt}
                ],
                temperature=0.7,
                max_tokens=800
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Cover letter generation error: {e}")
            return None

    def apply_on_linkedin(self):
        """
        Advanced LinkedIn Job Application Method
        Inspired by Auto_job_applier_linkedIn GitHub Repository
        """
        try:
            # Import necessary Selenium modules
            from selenium.webdriver.support.ui import WebDriverWait
            from selenium.webdriver.support import expected_conditions as EC
            from selenium.webdriver.common.by import By
            from selenium.webdriver.common.keys import Keys
            import time
            import random

            # LinkedIn Login
            self.driver.get("https://www.linkedin.com/login")

            # Credentials from secrets
            username = userdata.get('LINKEDIN_USERNAME')
            password = userdata.get('LINKEDIN_PASSWORD')

            # Advanced login with wait and error handling
            username_field = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "username"))
            )
            password_field = self.driver.find_element(By.ID, "password")

            # Humanize typing
            for char in username:
                username_field.send_keys(char)
                time.sleep(random.uniform(0.1, 0.3))

            for char in password:
                password_field.send_keys(char)
                time.sleep(random.uniform(0.1, 0.3))

            # Login with randomized delay
            login_button = self.driver.find_element(By.XPATH, "//button[@type='submit']")
            time.sleep(random.uniform(1.0, 2.5))
            login_button.click()

            # Job Search
            search_query = f"{self.job_details['title']} in {self.selected_emirates[0]}"
            search_field = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.XPATH, "//input[@aria-label='Search jobs']"))
            )
            search_field.send_keys(search_query)
            search_field.send_keys(Keys.RETURN)

            # Apply Filters
            time.sleep(random.uniform(2.0, 3.5))

            # Easy Apply Filter
            easy_apply_button = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//button[@aria-label='Easy Apply filter']"))
            )
            easy_apply_button.click()

            # Job Application Logic
            job_listings = self.driver.find_elements(By.CSS_SELECTOR, ".jobs-search-results__list-item")

            for job in job_listings[:10]:  # Limit to first 10 jobs
                try:
                    job.click()
                    time.sleep(random.uniform(1.5, 3.0))

                    # Check for Easy Apply button
                    easy_apply_btn = WebDriverWait(self.driver, 5).until(
                        EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'Easy Apply')]"))
                    )
                    easy_apply_btn.click()

                    # Application Form Handling
                    self._handle_linkedin_application_form()

                    # Log successful application
                    self.application_log.append({
                        "platform": "LinkedIn",
                        "job_title": self.job_details['title'],
                        "timestamp": datetime.now(),
                        "status": "Applied"
                    })

                except Exception as job_error:
                    print(f"Error applying to job: {job_error}")
                    continue

        except Exception as e:
            print(f"LinkedIn application error: {e}")
            self.application_log.append({
                "platform": "LinkedIn",
                "job_title": self.job_details['title'],
                "timestamp": datetime.now(),
                "status": "Failed",
                "error": str(e)
            })

    def _handle_linkedin_application_form(self):
        """
        Handle LinkedIn's dynamic application form
        Inspired by Auto_job_applier_linkedIn approach
        """
        try:
            # Wait for application form
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".jobs-easy-apply-modal"))
            )

            # Automated form filling logic
            questions = self.driver.find_elements(By.CSS_SELECTOR, ".jobs-easy-apply-form-section__grouping")

            for question in questions:
                try:
                    # Detect question type and fill accordingly
                    input_field = question.find_elements(By.TAG_NAME, "input")
                    select_field = question.find_elements(By.TAG_NAME, "select")
                    textarea_field = question.find_elements(By.TAG_NAME, "textarea")

                    if input_field:
                        self._fill_input_field(input_field[0])
                    elif select_field:
                        self._fill_select_field(select_field[0])
                    elif textarea_field:
                        self._fill_textarea_field(textarea_field[0])

                except Exception as question_error:
                    print(f"Error handling question: {question_error}")

            # Submit application
            submit_button = self.driver.find_element(By.XPATH, "//button[@aria-label='Submit application']")
            submit_button.click()

        except Exception as form_error:
            print(f"LinkedIn application form error: {form_error}")

    def apply_on_bayt(self):
        """Automated Bayt.com job application"""
        try:
            self.driver.get("https://www.bayt.com/en/login/")

            # Login and application logic similar to LinkedIn
            username = userdata.get('BAYT_USERNAME')
            password = userdata.get('BAYT_PASSWORD')

            # Implement Bayt.com specific application flow
            self.application_log.append({
                "platform": "Bayt.com",
                "job_title": self.job_details['title'],
                "timestamp": datetime.now()
            })

        except Exception as e:
            print(f"Bayt.com application error: {e}")

    def apply_on_gulftalent(self):
        """Automated GulfTalent job application"""
        try:
            self.driver.get("https://www.gulftalent.com/")

            # Implement GulfTalent specific application logic
            self.application_log.append({
                "platform": "GulfTalent",
                "job_title": self.job_details['title'],
                "timestamp": datetime.now()
            })

        except Exception as e:
            print(f"GulfTalent application error: {e}")

    def apply_on_naukrigulf(self):
        """Automated Naukrigulf job application"""
        try:
            self.driver.get("https://www.naukrigulf.com/")

            # Login and search logic
            username = userdata.get('NAUKRIGULF_USERNAME')
            password = userdata.get('NAUKRIGULF_PASSWORD')

            # Search for jobs in selected emirates
            search_query = f"{self.job_details['title']} in {' '.join(self.selected_emirates)}"

            self.application_log.append({
                "platform": "Naukrigulf",
                "job_title": self.job_details['title'],
                "emirates": self.selected_emirates,
                "timestamp": datetime.now()
            })

        except Exception as e:
            print(f"Naukrigulf application error: {e}")

    def apply_on_indeed_uae(self):
        """Automated Indeed UAE job application"""
        try:
            self.driver.get("https://ae.indeed.com/")

            # Job search and application logic
            search_query = f"{self.job_details['title']} jobs in {' '.join(self.selected_emirates)}"

            self.application_log.append({
                "platform": "Indeed UAE",
                "job_title": self.job_details['title'],
                "emirates": self.selected_emirates,
                "timestamp": datetime.now()
            })

        except Exception as e:
            print(f"Indeed UAE application error: {e}")

    def apply_on_monster_gulf(self):
        """Automated Monster Gulf job application"""
        try:
            self.driver.get("https://www.monsterindia.com/jobs-in-gulf")

            # Job search and application logic
            search_query = f"{self.job_details['title']} in {' '.join(self.selected_emirates)}"

            self.application_log.append({
                "platform": "Monster Gulf",
                "job_title": self.job_details['title'],
                "emirates": self.selected_emirates,
                "timestamp": datetime.now()
            })

        except Exception as e:
            print(f"Monster Gulf application error: {e}")

    def multi_platform_application(self):
        """
        Comprehensive multi-platform job application strategy
        Applies to platforms based on selected emirates
        """
        # Generate personalized cover letter
        cover_letter = self.generate_personalized_cover_letter()

        # Apply to platforms for each selected emirate
        for emirate in self.selected_emirates:
            print(f"\n🏙️ Applying to jobs in {emirate}")

            # Get platforms for this emirate
            emirate_platforms = self.uae_platforms.get(emirate, [])

            # Apply to each platform
            for platform_application in emirate_platforms:
                try:
                    platform_application()
                except Exception as e:
                    print(f"Error applying through {platform_application.__name__}: {e}")

        # Generate comprehensive application report
        self._generate_application_report()

    def _generate_application_report(self):
        """
        Generate comprehensive application submission report
        """
        report_prompt = f"""
        Analyze the job application submissions:

        Application Log: {self.application_log}
        Job Details: {self.job_details}

        Provide:
        - Success probability
        - Recommended follow-up actions
        - Potential improvements
        """

        try:
            report = deepseek_client.chat.completions.create(
                model="deepseek/deepseek-r1:free",
                messages=[
                    {"role": "system", "content": "You are an expert career counselor analyzing job applications"},
                    {"role": "user", "content": report_prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            print("\n📋 APPLICATION SUBMISSION REPORT")
            print(report.choices[0].message.content)

        except Exception as e:
            print(f"Report generation error: {e}")

# Updated usage function
def automated_job_application(cv_path, job_details, user_profile, selected_emirates):
    """
    Orchestrate automated job application process across UAE platforms

    Args:
    - cv_path: Path to resume
    - job_details: Dictionary with job information
    - user_profile: Dictionary with candidate profile
    - selected_emirates: List of emirates to apply in
    """
    application_agent = JobApplicationPlatforms(
        cv_path,
        job_details,
        user_profile,
        selected_emirates
    )

    application_agent.multi_platform_application()

# Example usage in job application workflow
def start_automated_applications(workflow_result):
    """
    Start automated applications based on workflow results
    """
    cv_path = "path/to/uploaded/cv.pdf"  # Update with actual CV path

    # Extract details from workflow result
    job_details = {
        "title": workflow_result['selected_roles'][0]['job_title'],
        # Add more job details as needed
    }

    user_profile = {
        "name": "User Name",
        "skills": workflow_result['selected_roles'][0]['matching_skills'],
        # Add more profile details
    }

    # Start applications
    automated_job_application(
        cv_path,
        job_details,
        user_profile,
        workflow_result['selected_emirates']
    )

"""# Section 5: API SETUP"""

# ============== SECTION 2: API SETUP ==============
# Set up the Hugging Face API

import os
from getpass import getpass
import sys

# Detect if running in Colab
USING_COLAB = 'google.colab' in sys.modules
if USING_COLAB:
    print("Running in Google Colab environment")
    try:
        from google.colab import userdata as user_data
        print("Google Colab userdata module available")
    except ImportError:
        print("Google Colab userdata module not available")
else:
    print("Running in local environment")

# Import Hugging Face login function
try:
    from huggingface_hub import login
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    print("huggingface_hub not installed. Installing...")
    !pip install -q huggingface_hub
    try:
        from huggingface_hub import login
        HUGGINGFACE_AVAILABLE = True
    except ImportError:
        print("Failed to install huggingface_hub")
        HUGGINGFACE_AVAILABLE = False

def setup_huggingface_api():
    """Get Hugging Face API token from Colab userdata, environment, or user input"""
    if not HUGGINGFACE_AVAILABLE:
        print("Hugging Face API not available")
        return None

    hf_token = None

    # First try to get from Colab userdata
    if USING_COLAB:
        try:
            hf_token = user_data.get('HF_TOKEN')  # Changed from HUGGINGFACE_API_KEY
            print("Retrieved Hugging Face API key from Colab secrets")
        except Exception as e:
            print(f"Could not retrieve API key from Colab secrets: {e}")

    # If not found in Colab, check environment
    if not hf_token:
        hf_token = os.environ.get("HF_TOKEN")  # Changed from HUGGINGFACE_API_KEY
        if hf_token:
            print("Retrieved Hugging Face API key from environment variable")

    # If still not found, ask user
    if not hf_token:
        print("No Hugging Face API token found in secrets or environment")
        hf_token = getpass("Enter your Hugging Face API token: ")
        os.environ["HF_TOKEN"] = hf_token  # Changed from HUGGINGFACE_API_KEY
        print("API token saved to environment variable")

    # Log in to Hugging Face
    login(token=hf_token)
    print("Hugging Face API setup complete!")
    return hf_token

# Execute this section
try:
    api_token = setup_huggingface_api()
    print("Section 2 complete: API setup successful.\n")
except Exception as e:
    print(f"Error setting up API: {e}")
    print("Please ensure you have a valid Hugging Face API token.\n")

# ============== SECTION 3: LANGUAGE MODEL SETUP ==============
import os
import time
import requests
import json
from getpass import getpass
import sys
import random
import traceback

# Alternative API Endpoints
ALTERNATIVE_ENDPOINTS = [
    "https://api-inference.huggingface.co/models/gpt2",
    "https://api-inference.huggingface.co/models/distilgpt2",
    "https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-125M"
]

def get_alternative_model(token):
    """
    Cycle through alternative models when primary model is unavailable
    """
    for endpoint in ALTERNATIVE_ENDPOINTS:
        try:
            model_name = endpoint.split("/")[-1]
            print(f"🔄 Trying alternative model: {model_name}")

            headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }

            payload = {
                "inputs": "Explain AI in simple terms",
                "parameters": {
                    "max_new_tokens": 50,
                    "temperature": 0.7
                }
            }

            response = requests.post(
                endpoint,
                headers=headers,
                json=payload,
                timeout=30
            )

            if response.status_code == 200:
                return endpoint, response.json()

        except Exception as e:
            print(f"Alternative model {model_name} failed: {e}")

    return None, None

def generate_with_api(prompt, model_id="microsoft/phi-2", max_retries=3):
    """
    Robust text generation with multiple fallback strategies
    """
    # Retrieve API Token
    token = os.environ.get('HF_TOKEN')
    if not token:
        if USING_COLAB and user_data:
            token = user_data.get('HF_TOKEN')

        if not token:
            token = getpass("Enter Hugging Face API Token: ")

    # Primary Endpoint
    API_URL = f"https://api-inference.huggingface.co/models/{model_id}"

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }

    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 1024,
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True
        }
    }

    for attempt in range(max_retries):
        try:
            print(f"🚀 Attempt {attempt + 1}/{max_retries}: Generating response")

            response = requests.post(
                API_URL,
                headers=headers,
                json=payload,
                timeout=60
            )

            # Successful response
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and result:
                    generated_text = result[0].get("generated_text", "")
                    return generated_text.strip()
                return str(result)

            # Service Unavailable - Try Alternative
            elif response.status_code == 503:
                print("🔄 Service Unavailable. Trying alternative models...")
                alt_endpoint, alt_result = get_alternative_model(token)

                if alt_endpoint and alt_result:
                    print(f"✅ Successfully used alternative model: {alt_endpoint}")
                    return alt_result[0].get("generated_text", "Alternative generation failed")

                # Wait and retry
                wait_time = (2 ** attempt) + random.random()
                print(f"⏳ Waiting {wait_time:.2f} seconds before retry")
                time.sleep(wait_time)

            else:
                print(f"❌ API Error: {response.status_code} - {response.text}")
                return f"API Error: {response.status_code}"

        except Exception as e:
            print(f"❌ Generation Error: {e}")
            traceback.print_exc()

            # Try alternative models on exception
            alt_endpoint, alt_result = get_alternative_model(token)
            if alt_endpoint and alt_result:
                return alt_result[0].get("generated_text", "Alternative generation failed")

    return "❌ Failed to generate response after multiple attempts"

# Test the API
try:
    test_prompt = "Explain artificial intelligence in simple terms."
    response = generate_with_api(test_prompt)

    print("\n🤖 API Test Response:")
    print(response)

    # Create a reusable response generator
    generate_response = lambda prompt: generate_with_api(prompt)

    print("\n✅ Section 3 complete: Language model setup successful.\n")

except Exception as e:
    print(f"❌ Error testing API: {e}")
    print("Section 3 incomplete: Language model setup failed.\n")

"""# Section 6: TTS"""

# Install required packages
print("Installing required packages...")
!pip install -q gtts SpeechRecognition pygame

# ============== SECTION 4: TEXT-TO-SPEECH AND SPEECH RECOGNITION ==============
# Setup for bilingual text-to-speech and speech recognition

import tempfile
import os
import pygame
from gtts import gTTS
import speech_recognition as sr

# Check if packages are available
try:
    from gtts import gTTS
    TTS_AVAILABLE = True
except ImportError:
    TTS_AVAILABLE = False
    print("gTTS not available. Installing...")
    !pip install -q gtts
    try:
        from gtts import gTTS
        TTS_AVAILABLE = True
    except ImportError:
        print("Failed to install gTTS")

try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    print("SpeechRecognition not available. Installing...")
    !pip install -q SpeechRecognition
    try:
        import speech_recognition as sr
        SPEECH_RECOGNITION_AVAILABLE = True
    except ImportError:
        print("Failed to install SpeechRecognition")

try:
    import pygame
    PYGAME_AVAILABLE = True
    # Initialize pygame
    pygame.init()
except ImportError:
    PYGAME_AVAILABLE = False
    print("pygame not available. Installing...")
    !pip install -q pygame
    try:
        import pygame
        PYGAME_AVAILABLE = True
        pygame.init()
    except ImportError:
        print("Failed to install pygame")

def text_to_speech(text, lang='en'):
    """Convert text to speech using gTTS in the specified language"""
    if not TTS_AVAILABLE:
        print(f"Text-to-speech not available. Text content: {text}")
        return False

    try:
        # Check for a valid language code
        valid_langs = {'en': 'English', 'ar': 'Arabic'}
        if lang not in valid_langs:
            print(f"Language '{lang}' not supported. Defaulting to English.")
            lang = 'en'

        print(f"Converting text to speech ({valid_langs[lang]})...")

        # Create a temporary file for the audio
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as temp_file:
            temp_filename = temp_file.name

        # Generate the speech file
        tts = gTTS(text=text, lang=lang, slow=False)
        tts.save(temp_filename)

        # Detect if running in Colab
        in_colab = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ

        if not in_colab and PYGAME_AVAILABLE:
            # Use pygame for audio playback
            try:
                # Initialize mixer if not already done
                if not pygame.mixer.get_init():
                    pygame.mixer.init()

                # Play the audio
                sound = pygame.mixer.Sound(temp_filename)
                sound.play()

                # Wait for it to finish
                pygame.time.delay(int(sound.get_length() * 1000))
                print(f"Audio played successfully ({valid_langs[lang]})")
            except Exception as e:
                print(f"Error playing audio: {e}")
        elif not in_colab:
            # Fallback to system commands if not in Colab and pygame not available
            try:
                if os.name == 'posix':  # For Linux/Mac
                    os.system(f"afplay {temp_filename}" if os.uname().sysname == 'Darwin' else f"mpg123 {temp_filename}")
                else:  # For Windows
                    os.system(f"start {temp_filename}")
                print(f"Audio played using system command ({valid_langs[lang]})")
            except Exception as e:
                print(f"Error playing audio with system command: {e}")
        else:
            # In Colab - just acknowledge file creation
            print(f"Audio generated for {valid_langs[lang]} text (Colab environment - playback skipped)")

        # Clean up temporary file
        try:
            os.unlink(temp_filename)
        except Exception as e:
            print(f"Warning: Could not delete temporary file: {e}")

        return True
    except Exception as e:
        print(f"Error generating speech: {e}")
        print(f"Text content: {text}")
        return False

def speech_to_text(lang='en'):
    """Convert speech to text using the specified language"""
    if not SPEECH_RECOGNITION_AVAILABLE:
        print("Speech recognition not available.")
        text = input("Please type your response instead: ")
        return text

    recognizer = sr.Recognizer()

    try:
        # Map language codes to recognition language
        lang_map = {'en': 'en-US', 'ar': 'ar-SA'}
        recognition_lang = lang_map.get(lang, 'en-US')

        # Detect if running in Colab
        in_colab = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ

        if in_colab:
            print("Speech recognition requires microphone access, which is limited in Colab.")
            text = input("Please type your response instead: ")
            return text

        with sr.Microphone() as source:
            print(f"Listening for {lang} speech... (speak now)")
            recognizer.adjust_for_ambient_noise(source)
            audio = recognizer.listen(source, timeout=10)

        print("Processing speech...")
        text = recognizer.recognize_google(audio, language=recognition_lang)
        print(f"Recognized: {text}")
        return text
    except sr.RequestError:
        print("Could not request results from speech recognition service")
    except sr.UnknownValueError:
        print("Could not understand audio")
    except Exception as e:
        print(f"Error in speech recognition: {e}")

    # Fallback to text input
    text = input("Please type your response instead: ")
    return text

# Initialize audio environment for Colab if needed
in_colab = 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ
if in_colab and PYGAME_AVAILABLE:
    os.environ['SDL_AUDIODRIVER'] = 'dummy'
    if pygame.mixer.get_init():
        pygame.mixer.quit()
    pygame.mixer.init()
    print("Audio environment configured for Colab")

# Add a function to easily use text-to-speech throughout the application
def read_aloud(text, lang='en'):
    """Read text aloud and print it to the console"""
    print(text)  # Always print the text

    # Also read it aloud if TTS is available
    if TTS_AVAILABLE:
        text_to_speech(text, lang)

# Test TTS with both languages if available
if TTS_AVAILABLE:
    print("Text-to-Speech is available and ready to use")

    # Test English TTS
    read_aloud("Welcome to Tamkeen AI Career Intelligence System. Text-to-speech is now ready to use in English.", lang='en')

    # Test Arabic TTS
    arabic_text = "مرحبًا بك في نظام الذكاء الوظيفي من تمكين. تقنية تحويل النص إلى كلام جاهزة للاستخدام باللغة العربية."
    read_aloud(arabic_text, lang='ar')

    print("\nBilingual text-to-speech capability tested successfully")
else:
    print("Warning: Text-to-Speech is not available")

# Note message about speech recognition in Colab
if in_colab and SPEECH_RECOGNITION_AVAILABLE:
    print("Note: Speech recognition will use text input fallback in Colab environment")

# Test speech recognition if available and not in Colab
if SPEECH_RECOGNITION_AVAILABLE and not in_colab:
    print("\nWould you like to test speech recognition? (Type 'yes' or speak 'yes' to test)")
    response = input("Your choice: ")

    if response.lower() == 'yes':
        print("\nTesting English speech recognition:")
        print("Please speak something in English when prompted...")
        english_speech = speech_to_text(lang='en')
        print(f"You said (English): {english_speech}")

        print("\nTesting Arabic speech recognition:")
        print("Please speak something in Arabic when prompted...")
        arabic_speech = speech_to_text(lang='ar')
        print(f"You said (Arabic): {arabic_speech}")
    else:
        print("Speech recognition testing skipped.")

print("\nSection 4 complete: Bilingual text-to-speech and speech recognition setup finished.\n")

"""# section 7 Face Expression Detection"""

# ============================
# SECTION 6c: FACE EXPRESSION DETECTION WITH HUGGING FACE DATASET
# ============================

import tensorflow as tf
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
import time
from collections import Counter
import random
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Dropout, SpatialDropout2D, Flatten
from tensorflow.keras.regularizers import l1
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
import pandas as pd

# Install required packages if needed
try:
    import kagglehub
    from kagglehub import KaggleDatasetAdapter
except ImportError:
    print("Installing kagglehub...")
    !pip install -q kagglehub[hf-datasets]
    import kagglehub
    from kagglehub import KaggleDatasetAdapter

print("Initializing Face Expression Detection System...")

# Define emotion categories (ensure they match dataset labels)
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# Function to load the dataset using kagglehub
def load_emotion_dataset():
    """Load facial expression dataset from Hugging Face via kagglehub"""
    print("Loading facial expression dataset...")

    try:
        # Load the dataset
        dataset = kagglehub.load_dataset(
            KaggleDatasetAdapter.HUGGING_FACE,
            "ashishpatel26/facial-expression-recognitionferchallenge",
            ""  # Empty string to load the default version
        )

        print(f"Dataset loaded successfully")
        return dataset

    except Exception as e:
        print(f"Error loading dataset: {e}")
        # Fallback: Try loading from local file if available
        try:
            if os.path.exists('fer2013.csv'):
                print("Loading dataset from local fer2013.csv file...")
                return pd.read_csv('fer2013.csv')
            elif os.path.exists('fer2013.csv.zip'):
                print("Extracting dataset from local fer2013.csv.zip file...")
                !unzip -o fer2013.csv.zip
                return pd.read_csv('fer2013.csv')
        except Exception as e2:
            print(f"Error loading local dataset: {e2}")

        return None

# Function to prepare data from pandas DataFrame
def prepare_data_from_dataframe(df):
    """Prepare dataset from pandas DataFrame for training emotion detection model"""
    print("Preparing emotion detection dataset from DataFrame...")

    # Check dataframe structure
    print(f"DataFrame columns: {df.columns}")

    try:
        # Assuming DataFrame has 'emotion', 'pixels', and 'Usage' columns
        # Split into train and test
        train_data = df[df['Usage'] == 'Training']
        test_data = df[df['Usage'] == 'PublicTest']

        print(f"Training samples: {len(train_data)}")
        print(f"Testing samples: {len(test_data)}")

        # Extract features and labels for training data
        X_train = []
        for pixel_string in train_data['pixels']:
            pixel_values = [int(p) for p in pixel_string.split()]
            img = np.array(pixel_values).reshape(48, 48) / 255.0  # Normalize
            X_train.append(img)

        X_train = np.array(X_train).reshape(-1, 48, 48, 1)
        y_train = to_categorical(train_data['emotion'], num_classes=7)

        # Extract features and labels for test data
        X_test = []
        for pixel_string in test_data['pixels']:
            pixel_values = [int(p) for p in pixel_string.split()]
            img = np.array(pixel_values).reshape(48, 48) / 255.0  # Normalize
            X_test.append(img)

        X_test = np.array(X_test).reshape(-1, 48, 48, 1)
        y_test = to_categorical(test_data['emotion'], num_classes=7)

        return X_train, y_train, X_test, y_test

    except Exception as e:
        print(f"Error preparing data from DataFrame: {e}")
        return None, None, None, None

# Function to create the emotion detection model
def create_emotion_model(model_type='deep'):
    """Create and return a face expression detection model"""

    if model_type == 'shallow':
        model = Sequential()
        model.add(Conv2D(16, kernel_size=3, activation='relu', padding="same", input_shape=(48,48,1)))
        model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(SpatialDropout2D(0.5))

        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
        model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(SpatialDropout2D(0.5))
        model.add(Flatten())

        model.add(Dense(128, activation='relu'))
        model.add(Dense(7, activation='softmax'))

    else:  # deep model
        model = Sequential()
        model.add(Conv2D(64, kernel_size=3, activation='relu', padding="same", input_shape=(48,48,1), kernel_regularizer=l1(1e-6)))
        model.add(BatchNormalization())
        model.add(Dropout(0.2))
        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(Conv2D(128, (5, 5), activation='relu', padding='same', kernel_regularizer=l1(1e-6)))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))
        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l1(1e-6)))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))
        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=l1(1e-6)))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))
        model.add(MaxPooling2D(pool_size=(2, 2)))

        model.add(Flatten())

        model.add(Dense(256, activation='relu', kernel_regularizer=l1(1e-6)))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))
        model.add(Dense(512, activation='relu', kernel_regularizer=l1(1e-6)))
        model.add(BatchNormalization())
        model.add(Dropout(0.4))

        model.add(Dense(7, activation='softmax'))

    # Compile the model
    model.compile(
        loss='categorical_crossentropy',
        optimizer=Adam(learning_rate=0.0001),
        metrics=['accuracy']
    )

    return model

# Function to train the emotion detection model
def train_emotion_model(X_train, y_train, X_test, y_test, model_type='deep', epochs=30, batch_size=64):
    """Train the emotion detection model on the provided dataset"""
    print(f"Training {model_type} emotion detection model...")

    # Create model
    model = create_emotion_model(model_type)

    # Callbacks for better training
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
    ]

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks
    )

    # Evaluate the model
    print("Evaluating model performance...")
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test accuracy: {accuracy:.4f}")

    # Save the model
    os.makedirs('models', exist_ok=True)
    model.save('models/emotion_model.h5')
    print("Model saved to models/emotion_model.h5")

    # Plot training history
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()

    return model

# Function to load a pre-trained emotion detection model
def load_emotion_model(model_path='models/emotion_model.h5'):
    """Load a pre-trained emotion detection model"""
    if os.path.exists(model_path):
        print(f"Loading pre-trained emotion model from {model_path}")
        try:
            model = load_model(model_path)
            return model
        except Exception as e:
            print(f"Error loading model: {e}")

    print(f"Pre-trained model not found at {model_path}")
    return None

# Function to initialize the emotion detection system
def initialize_emotion_detection(force_train=False):
    """Initialize the emotion detection system, training if necessary"""
    model_path = 'models/emotion_model.h5'

    # Check if pre-trained model exists and we're not forcing a retrain
    if os.path.exists(model_path) and not force_train:
        print("Using existing pre-trained emotion detection model")
        return load_emotion_model(model_path)

    print("Training new emotion detection model...")

    # Load dataset
    dataset = load_emotion_dataset()
    if dataset is None:
        print("Failed to load dataset. Cannot train emotion detection model.")
        return None

    # Prepare dataset for training
    if isinstance(dataset, pd.DataFrame):
        X_train, y_train, X_test, y_test = prepare_data_from_dataframe(dataset)
    else:
        # Handle huggingface dataset format if needed
        print("Non-DataFrame dataset format detected. Implementation may need adjustment.")
        X_train, y_train, X_test, y_test = None, None, None, None

    if X_train is None:
        print("Failed to prepare training data.")
        return None

    # Train the model
    model = train_emotion_model(X_train, y_train, X_test, y_test)
    return model

# Function to detect emotions in an image
def detect_emotion(image, face_cascade=None, model=None, return_cropped=False):
    """
    Detect emotions in an image

    Parameters:
    - image: Input image (can be filename or numpy array)
    - face_cascade: Pre-loaded face detector (optional)
    - model: Pre-loaded emotion model (optional)
    - return_cropped: Whether to return the cropped face images

    Returns:
    - List of dictionaries with detected emotions
    """
    # Load the image if it's a filename
    if isinstance(image, str):
        if os.path.exists(image):
            img = cv2.imread(image)
        else:
            print(f"Image file not found: {image}")
            return []
    else:
        img = image.copy()

    # Convert to grayscale
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img

    # Load face detector if not provided
    if face_cascade is None:
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Load emotion model if not provided
    if model is None:
        model = load_emotion_model()
        if model is None:
            print("No emotion detection model available")
            return []

    # Detect faces
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)

    results = []
    cropped_faces = []

    # Process each face
    for (x, y, w, h) in faces:
        # Extract face ROI
        face_roi = gray[y:y+h, x:x+w]

        # Resize to 48x48 (model input size)
        resized_face = cv2.resize(face_roi, (48, 48))

        # Normalize
        normalized_face = resized_face / 255.0

        # Reshape for model input
        input_face = normalized_face.reshape(1, 48, 48, 1)

        # Predict emotion
        preds = model.predict(input_face, verbose=0)[0]
        emotion_idx = np.argmax(preds)
        emotion = emotion_labels[emotion_idx]
        confidence = preds[emotion_idx]

        # Store result
        result = {
            'bbox': (x, y, w, h),
            'emotion': emotion,
            'confidence': float(confidence),
            'all_emotions': {emotion_labels[i]: float(preds[i]) for i in range(len(emotion_labels))}
        }
        results.append(result)

        if return_cropped:
            cropped_faces.append(resized_face)

    if return_cropped:
        return results, cropped_faces
    return results

# Function to track emotions during interview question answering
def analyze_question_response_emotions(question, duration=30, display=True):
    """
    Analyze facial expressions while answering a specific interview question

    Parameters:
    - question: The interview question being asked
    - duration: How long to record emotions for this question (in seconds)
    - display: Whether to show live video feed with emotion detection

    Returns:
    - Dictionary containing emotion analysis for this question
    """
    print(f"\nQuestion: {question}")
    print(f"Please answer the question. Recording emotions for {duration} seconds...")

    # Initialize webcam
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Error: Could not open webcam")
        return None

    # Load face detector and emotion model
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    # Initialize model
    model = load_emotion_model()
    if model is None:
        print("No emotion detection model available. Initializing...")
        model = initialize_emotion_detection()
        if model is None:
            print("Failed to initialize emotion detection model")
            return None

    # Variables for statistics
    emotion_counts = Counter()
    start_time = time.time()
    frame_count = 0
    emotion_timeline = []  # Track emotions over time

    print("Recording started. Answer the question now...")

    while True:
        # Read frame from webcam
        ret, frame = cap.read()
        if not ret:
            print("Error: Failed to capture frame")
            break

        # Detect emotions
        results = detect_emotion(frame, face_cascade, model)

        # Record timestamp and emotions
        current_time = time.time() - start_time
        frame_emotions = {r['emotion']: r['confidence'] for r in results} if results else {}
        emotion_timeline.append((current_time, frame_emotions))

        if display:
            # Add question text at the top of the frame
            cv2.putText(frame, f"Q: {question[:50]}{'...' if len(question) > 50 else ''}",
                       (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            # Add timer
            remaining = max(0, duration - current_time)
            cv2.putText(frame, f"Time: {remaining:.1f}s",
                       (frame.shape[1] - 150, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            # Draw results on frame
            for result in results:
                x, y, w, h = result['bbox']
                emotion = result['emotion']
                confidence = result['confidence']

                # Draw rectangle around face
                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

                # Display emotion and confidence
                text = f"{emotion}: {confidence:.2f}"
                cv2.putText(frame, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                # Record emotion
                emotion_counts[emotion] += 1

            # Display frame
            cv2.imshow('Interview Question Analysis', frame)

            # Check for early exit
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("Early exit requested")
                break

        # Check if duration has elapsed
        frame_count += 1
        elapsed_time = time.time() - start_time
        if elapsed_time >= duration:
            break

    # Release resources
    cap.release()
    if display:
        cv2.destroyAllWindows()

    # Calculate statistics
    total_detections = sum(emotion_counts.values())

    # Handle the case where no faces were detected
    if total_detections == 0:
        print("No faces were detected during the response.")
        return {
            'question': question,
            'emotion_counts': {},
            'emotion_percentages': {},
            'dominant_emotion': None,
            'positive_ratio': 0,
            'negative_ratio': 0,
            'neutral_ratio': 0,
            'confidence_score': 0,
            'engagement_score': 0,
            'detection_rate': 0,
            'insights': ["No face detected. Make sure you're visible to the camera."]
        }

    emotion_percentages = {emotion: count/total_detections for emotion, count in emotion_counts.items()}

    # Add summary stats
    dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else "No face detected"

    # Group emotions into categories
    positive_emotions = emotion_percentages.get('Happy', 0) + emotion_percentages.get('Surprise', 0) * 0.5
    negative_emotions = (emotion_percentages.get('Angry', 0) +
                        emotion_percentages.get('Sad', 0) +
                        emotion_percentages.get('Fear', 0) +
                        emotion_percentages.get('Disgust', 0))
    neutral_emotions = emotion_percentages.get('Neutral', 0) + emotion_percentages.get('Surprise', 0) * 0.5

    # Detailed analysis results
    analysis_results = {
        'question': question,
        'emotion_counts': dict(emotion_counts),
        'emotion_percentages': emotion_percentages,
        'dominant_emotion': dominant_emotion,
        'positive_ratio': positive_emotions,
        'negative_ratio': negative_emotions,
        'neutral_ratio': neutral_emotions,
        'confidence_score': min(1.0, max(0.0, positive_emotions * 1.2 - negative_emotions * 0.8 + 0.5)),
        'engagement_score': min(1.0, max(0.0, 1.0 - neutral_emotions * 1.5)),
        'detection_rate': len([t for t, e in emotion_timeline if e]) / len(emotion_timeline) if emotion_timeline else 0
    }

    # Generate insights based on emotion patterns
    insights = []

    if analysis_results['detection_rate'] < 0.3:
        insights.append("Low face detection rate. Make sure you're visible to the camera.")

    if analysis_results['positive_ratio'] > 0.6:
        insights.append("You showed strong positive emotions, which is excellent for interview engagement.")
    elif analysis_results['positive_ratio'] < 0.2:
        insights.append("Consider showing more positive emotions like smiling during interviews.")

    if analysis_results['negative_ratio'] > 0.3:
        insights.append("You displayed significant negative emotions, which might impact interviewer perception.")

    if analysis_results['neutral_ratio'] > 0.7:
        insights.append("Your expression was mostly neutral. Consider showing more engagement and enthusiasm.")

    if dominant_emotion == 'Neutral':
        insights.append("Your dominant expression was neutral. Consider showing more emotional engagement.")
    elif dominant_emotion == 'Happy':
        insights.append("You primarily displayed happiness, which is generally positive in interviews.")
    elif dominant_emotion in ['Fear', 'Sad']:
        insights.append(f"Your primary emotion was {dominant_emotion.lower()}, which may be perceived as lack of confidence.")

    analysis_results['insights'] = insights

    # Print a summary
    print("\nEmotion Analysis Summary:")
    print(f"Dominant emotion: {dominant_emotion}")
    for emotion, percentage in sorted(emotion_percentages.items(), key=lambda x: x[1], reverse=True):
        if percentage > 0:
            print(f"  {emotion}: {percentage:.1%}")

    print("\nInsights:")
    for insight in insights:
        print(f"- {insight}")

    return analysis_results

# Function to visualize emotion analysis results
def visualize_emotion_analysis(emotion_data):
    """Visualize emotion analysis results"""
    if not emotion_data or 'emotion_percentages' not in emotion_data or not emotion_data['emotion_percentages']:
        print("No emotion data available for visualization")
        return

    percentages = emotion_data['emotion_percentages']

    # Prepare data for plotting
    emotions = list(percentages.keys())
    values = list(percentages.values())

    # Set up the figure
    plt.figure(figsize=(12, 10))

    # Create bar chart
    plt.subplot(2, 1, 1)
    colors = ['red', 'purple', 'orange', 'green', 'blue', 'yellow', 'gray']
    plt.bar(emotions, values, color=colors[:len(emotions)])
    plt.title('Emotion Distribution During Interview Answer')
    plt.xlabel('Emotions')
    plt.ylabel('Percentage')
    plt.ylim(0, max(values) * 1.2)  # Add some space above the highest bar

    # Add percentages above bars
    for i, v in enumerate(values):
        plt.text(i, v + 0.01, f'{v:.1%}', ha='center')

    # Create pie chart
    plt.subplot(2, 1, 2)
    plt.pie(values, labels=emotions, autopct='%1.1f%%', startangle=90, colors=colors[:len(emotions)])
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
    plt.title('Emotion Proportion During Answer')

    plt.tight_layout()
    plt.show()

# Function to generate interview questions based on role
def generate_interview_questions(role, num_questions=5):
    """Generate interview questions based on job role"""
    # Common interview questions
    common_questions = [
        "Tell me about yourself and your background.",
        "What are your greatest strengths and weaknesses?",
        "Why are you interested in this position?",
        "Where do you see yourself in five years?",
        "Describe a challenge you faced at work and how you overcame it.",
        "How do you handle stress and pressure?",
        "What is your greatest professional achievement?",
        "Tell me about a time you demonstrated leadership skills.",
        "How do you prioritize your work?",
        "What are your salary expectations?"
    ]

    # Role-specific questions
    role_specific_questions = {
        "Data Scientist": [
            "Explain the difference between supervised and unsupervised learning.",
            "How would you handle missing or outlier values in a dataset?",
            "Describe a data analysis project you've worked on from start to finish.",
            "What evaluation metrics would you use for a classification problem?",
            "How do you communicate technical findings to non-technical stakeholders?"
        ],
        "Software Engineer": [
            "Describe your experience with different programming languages and frameworks.",
            "How do you ensure your code is maintainable and scalable?",
            "Explain your debugging process when encountering an issue.",
            "How do you stay updated with the latest technology trends?",
            "Tell me about a time you improved the performance of an application."
        ],
        "Product Manager": [
            "How do you prioritize competing features for a product roadmap?",
            "Describe your process for gathering and incorporating user feedback.",
            "How do you communicate with different stakeholders like engineers, designers, and executives?",
            "Tell me about a product launch you managed and what you learned.",
            "How do you measure the success of a product feature?"
        ],
        "Marketing Specialist": [
            "Describe a successful marketing campaign you developed.",
            "How do you measure the ROI of marketing initiatives?",
            "How do you stay up-to-date with digital marketing trends?",
            "Describe your experience with different marketing channels.",
            "How would you approach marketing to a new target audience?"
        ]
    }

    # Get role-specific questions or use defaults
    specific_questions = role_specific_questions.get(role, [
        f"What specific skills do you have that are relevant to this {role} position?",
        f"What tools or technologies are you familiar with that are used by {role}s?",
        f"How do you stay updated with trends in the {role} field?",
        f"Describe a project you worked on that demonstrates your abilities as a {role}.",
        f"What do you think are the biggest challenges facing {role}s today?"
    ])

    # Combine common and role-specific questions
    all_questions = common_questions + specific_questions

    # Select random subset of questions
    if num_questions < len(all_questions):
        selected_questions = random.sample(all_questions, num_questions)
    else:
        selected_questions = all_questions

    return selected_questions

# Function to conduct a complete interview with emotion analysis
def conduct_emotion_analyzed_interview(questions, answer_time=30):
    """
    Conduct a complete interview with emotion analysis for each question

    Parameters:
    - questions: List of interview questions to ask
    - answer_time: Time allowed for each answer in seconds

    Returns:
    - List of emotion analysis results for each question
    """
    results = []

    print("\n===== STARTING INTERVIEW WITH EMOTION ANALYSIS =====")
    print(f"You will be asked {len(questions)} questions.")
    print(f"You will have {answer_time} seconds to answer each question.")
    print("Your facial expressions will be analyzed during each answer.")
    print("Press 'q' at any time to skip to the next question.")

    input("\nPress Enter when you're ready to begin...")

    for i, question in enumerate(questions):
        print(f"\nQuestion {i+1}/{len(questions)}:")

        # Analyze emotions during answer
        analysis = analyze_question_response_emotions(question, duration=answer_time)

        if analysis:
            results.append(analysis)

            # Show brief summary of this answer
            confidence_score = analysis['confidence_score']
            engagement_score = analysis['engagement_score']

            print("\nQuick Feedback:")
            print(f"Confidence: {confidence_score:.2f}/1.0")
            print(f"Engagement: {engagement_score:.2f}/1.0")

            # Wait briefly between questions
            if i < len(questions) - 1:
                print("\nPreparing next question...")
                time.sleep(3)

    print("\n===== INTERVIEW COMPLETE =====")
    print(f"Analyzed {len(results)}/{len(questions)} questions.")

    return results

# Function to visualize complete interview results
def visualize_interview_emotions(results):
    """Visualize emotion analysis results for the entire interview"""
    if not results:
        print("No interview results to visualize")
        return

    # Extract data for visualization
    questions = [r['question'][:30] + '...' for r in results]
    confidence_scores = [r['confidence_score'] for r in results]
    engagement_scores = [r['engagement_score'] for r in results]
    positive_ratios = [r['positive_ratio'] for r in results]
    negative_ratios = [r['negative_ratio'] for r in results]
    neutral_ratios = [r['neutral_ratio'] for r in results]

    # Set up the figure
    plt.figure(figsize=(15, 12))

    # 1. Plot confidence and engagement scores per question
    plt.subplot(2, 2, 1)
    x = range(len(questions))
    width = 0.35
    plt.bar([i - width/2 for i in x], confidence_scores, width, label='Confidence')
    plt.bar([i + width/2 for i in x], engagement_scores, width, label='Engagement')
    plt.xlabel('Questions')
    plt.ylabel('Score (0-1)')
    plt.title('Confidence and Engagement by Question')
    plt.xticks(x, [f"Q{i+1}" for i in range(len(questions))], rotation=45)
    plt.ylim(0, 1.0)
    plt.legend()

    # 2. Plot emotional balance per question
    plt.subplot(2, 2, 2)
    emotions_data = {
        'Positive': positive_ratios,
        'Negative': negative_ratios,
        'Neutral': neutral_ratios
    }

    bottom = np.zeros(len(questions))
    for emotion, ratios in emotions_data.items():
        plt.bar(x, ratios, bottom=bottom, label=emotion)
        bottom += np.array(ratios)

    plt.xlabel('Questions')
    plt.ylabel('Proportion')
    plt.title('Emotional Balance by Question')
    plt.xticks(x, [f"Q{i+1}" for i in range(len(questions))], rotation=45)
    plt.ylim(0, 1.0)
    plt.legend()

    # 3. Plot average emotions across all questions
    plt.subplot(2, 2, 3)

    # Collect all emotions
    all_emotions = Counter()
    for result in results:
        for emotion, count in result['emotion_counts'].items():
            all_emotions[emotion] += count

    # Calculate percentages
    total = sum(all_emotions.values())
    emotion_labels = list(all_emotions.keys())
    emotion_values = [count/total for count in all_emotions.values()]

    colors = plt.cm.tab10(np.linspace(0, 1, len(emotion_labels)))
    plt.pie(emotion_values, labels=emotion_labels, autopct='%1.1f%%', startangle=90, colors=colors)
    plt.axis('equal')
    plt.title('Average Emotions Across All Questions')

    # 4. Performance radar chart
    plt.subplot(2, 2, 4)

    # Calculate interview metrics
    avg_confidence = sum(confidence_scores) / len(confidence_scores)
    avg_engagement = sum(engagement_scores) / len(engagement_scores)
    emotional_variety = len(all_emotions) / 7.0  # Normalize by total possible emotions
    consistency = 1.0 - np.std(confidence_scores)  # Higher consistency = lower std dev
    positive_ratio = sum(positive_ratios) / len(positive_ratios)

    # Create radar chart
    categories = ['Confidence', 'Engagement', 'Emotional Variety', 'Consistency', 'Positivity']
    values = [avg_confidence, avg_engagement, emotional_variety, consistency, positive_ratio]

    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
    values += values[:1]  # Close the polygon
    angles += angles[:1]  # Close the polygon
    categories += categories[:1]  # Close the polygon

    ax = plt.subplot(2, 2, 4, polar=True)
    ax.plot(angles, values, 'o-', linewidth=2)
    ax.fill(angles, values, alpha=0.25)
    ax.set_thetagrids(np.degrees(angles[:-1]), categories[:-1])
    ax.set_ylim(0, 1)
    plt.title('Interview Performance Metrics')

    plt.tight_layout()
    plt.show()

    # Print interview insights
    print("\n===== INTERVIEW INSIGHTS =====")

    # Identify strongest and weakest responses
    strongest_q_idx = confidence_scores.index(max(confidence_scores))
    weakest_q_idx = confidence_scores.index(min(confidence_scores))

    print(f"Strongest response: Question {strongest_q_idx+1}")
    print(f"  \"{questions[strongest_q_idx]}\"")
    print(f"  Confidence: {confidence_scores[strongest_q_idx]:.2f}, Engagement: {engagement_scores[strongest_q_idx]:.2f}")

    print(f"\nWeakest response: Question {weakest_q_idx+1}")
    print(f"  \"{questions[weakest_q_idx]}\"")
    print(f"  Confidence: {confidence_scores[weakest_q_idx]:.2f}, Engagement: {engagement_scores[weakest_q_idx]:.2f}")

    print("\nOverall performance:")
    print(f"  Average confidence: {avg_confidence:.2f}/1.0")
    print(f"  Average engagement: {avg_engagement:.2f}/1.0")
    print(f"  Emotional positivity: {positive_ratio:.2f}/1.0")
    print(f"  Response consistency: {consistency:.2f}/1.0")

    # General advice
    print("\nGeneral advice:")
    if avg_confidence < 0.4:
        print("- Work on building confidence in your responses. Practice answering common interview questions.")
    if avg_engagement < 0.4:
        print("- Try to show more engagement and enthusiasm during interviews.")
    if positive_ratio < 0.3:
        print("- Consider showing more positive emotions like smiling during your responses.")
    if consistency < 0.7:
        print("- Your emotional expression varied considerably between questions. Aim for more consistency.")

    return

# Function to start a complete interview with emotion analysis
def start_interview_emotion_analysis(role=None, num_questions=5, answer_time=30):
    """
    Start a complete interview with emotion analysis

    Parameters:
    - role: The job role to customize questions for (optional)
    - num_questions: Number of interview questions to ask
    - answer_time: Time allowed for each answer in seconds

    Returns:
    - Interview analysis results
    """
    print("\n===== TAMKEEN AI INTERVIEW EMOTION ANALYSIS =====")

    if role is None:
        print("Choose a job role for your practice interview:")
        print("1. Data Scientist")
        print("2. Software Engineer")
        print("3. Product Manager")
        print("4. UI/UX Designer")
        print("5. Marketing Specialist")
        print("6. Other (specify)")

        choice = input("Enter your choice (1-6): ")

        if choice == '1':
            role = "Data Scientist"
        elif choice == '2':
            role = "Software Engineer"
        elif choice == '3':
            role = "Product Manager"
        elif choice == '4':
            role = "UI/UX Designer"
        elif choice == '5':
            role = "Marketing Specialist"
        elif choice == '6':
            role = input("Enter the specific job role: ")
        else:
            role = "General Position"

    print(f"\nPreparing interview questions for: {role}")

    # Check if emotion model is available
    model_available = os.path.exists('models/emotion_model.h5')
    if not model_available:
        print("\nEmotion detection model not found. Do you want to train a new model?")
        print("This may take some time but is required for emotion analysis.")
        train_choice = input("Train model now? (yes/no): ")

        if train_choice.lower() in ['yes', 'y']:
            model = initialize_emotion_detection(force_train=True)
            if model is None:
                print("Failed to train emotion detection model. Cannot proceed with emotion analysis.")
                return None
        else:
            print("Cannot proceed with emotion analysis without a trained model.")
            return None

    # Generate questions for this role
    questions = generate_interview_questions(role, num_questions)

    print("\nInterview Setup:")

    # Function to generate interview questions based on job role
def generate_interview_questions(role, num_questions=5):
    """Generate role-specific interview questions"""

    # Common interview questions for all roles
    common_questions = [
        "Tell me about yourself and your background.",
        "What are your greatest strengths?",
        "What do you consider to be your weaknesses?",
        "Why are you interested in this position?",
        "Where do you see yourself in five years?",
        "Describe a challenge you faced at work and how you handled it.",
        "How do you handle pressure or stressful situations?",
        "What are your salary expectations?",
        "Do you have any questions for me about the role or company?"
    ]

    # Role-specific questions
    role_specific_questions = {
        "Data Scientist": [
            "Can you explain a complex data science concept in simple terms?",
            "Describe a data project you've worked on from start to finish.",
            "How do you ensure your data analyses are both accurate and actionable?",
            "How do you handle missing or corrupted data in your datasets?",
            "Explain the difference between supervised and unsupervised learning.",
            "What metrics would you use to evaluate a classification model?",
            "How do you communicate technical findings to non-technical stakeholders?"
        ],
        "Software Engineer": [
            "Describe your experience with different programming languages and frameworks.",
            "How do you approach debugging a complex software issue?",
            "Explain your process for designing scalable software architecture.",
            "How do you ensure code quality and maintainability?",
            "Tell me about a time you had to refactor a piece of code and why.",
            "How do you stay updated with the latest technology trends?",
            "Describe your experience with agile development methodologies."
        ],
        "Product Manager": [
            "How do you prioritize product features?",
            "Describe how you would launch a new product from concept to market.",
            "How do you gather and incorporate user feedback?",
            "Tell me about a product you managed that wasn't successful. What did you learn?",
            "How do you balance stakeholder requests with user needs?",
            "Describe your experience working with engineering and design teams.",
            "How do you measure the success of a product?"
        ],
        "UI/UX Designer": [
            "Walk me through your design process from concept to implementation.",
            "How do you incorporate user research into your designs?",
            "Describe a UI/UX problem you solved and your approach.",
            "How do you handle feedback and criticism on your designs?",
            "How do you balance aesthetics with functionality?",
            "Describe how you would design for accessibility.",
            "What design tools and software are you proficient with?"
        ],
        "Marketing Specialist": [
            "Describe a successful marketing campaign you developed.",
            "How do you measure the success of marketing initiatives?",
            "How do you stay current with marketing trends and technologies?",
            "Describe your experience with digital marketing channels.",
            "How would you approach marketing to a new target audience?",
            "Describe your experience with content creation and management.",
            "How do you analyze market research and apply it to marketing strategies?"
        ]
    }

    # Get role-specific questions or use general questions if role not found
    specific_questions = role_specific_questions.get(role, [
        "What specific skills do you bring to this role?",
        "How does your experience align with this position?",
        "Describe your ideal working environment.",
        "What motivates you in your work?",
        "How do you approach learning new skills?"
    ])

    # Combine questions and select the required number
    all_questions = common_questions + specific_questions
    if num_questions >= len(all_questions):
        return all_questions
    else:
        # Ensure we have at least one role-specific question if available
        selected = []
        if specific_questions:
            selected.append(random.choice(specific_questions))

        # Always include "Tell me about yourself" as the first question
        if "Tell me about yourself and your background." in all_questions:
            selected.append("Tell me about yourself and your background.")
            all_questions.remove("Tell me about yourself and your background.")

        # Add remaining random questions
        remaining_slots = num_questions - len(selected)
        if remaining_slots > 0:
            remaining_questions = random.sample(all_questions, min(remaining_slots, len(all_questions)))
            selected.extend(remaining_questions)

        # Make "Tell me about yourself" the first question if selected
        if "Tell me about yourself and your background." in selected:
            selected.remove("Tell me about yourself and your background.")
            selected.insert(0, "Tell me about yourself and your background.")

        return selected

# Function to conduct a complete interview with emotion analysis
def conduct_emotion_analyzed_interview(questions, answer_time=30):
    """
    Conduct a complete interview with real-time emotion analysis

    Parameters:
    - questions: List of interview questions to ask
    - answer_time: Time allowed for each answer in seconds

    Returns:
    - List of question-specific emotion analysis results
    """
    print("\n===== STARTING INTERVIEW =====")
    print(f"You will be asked {len(questions)} questions.")
    print(f"You will have {answer_time} seconds to answer each question.")
    print("Your facial expressions will be analyzed during each answer.")
    print("Press ENTER when you're ready to start, and after each question to continue.")

    input("\nPress ENTER to begin the interview...")

    results = []

    # Load face detection and emotion model
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    emotion_model = load_emotion_model()

    # Ask each question and analyze responses
    for i, question in enumerate(questions):
        print(f"\nQuestion {i+1}: {question}")
        input("Press ENTER when ready to answer. The camera will start recording...")

        # Analyze emotions during response
        question_result = analyze_question_response_emotions(question, answer_time, display=True)
        results.append(question_result)

        print("\n--- Response Analysis ---")
        confidence = question_result['confidence_score']
        engagement = question_result['engagement_score']

        print(f"Confidence Score: {confidence:.2f}/1.0")
        print(f"Engagement Score: {engagement:.2f}/1.0")

        # Show the top emotions detected
        print("Top emotions detected:")
        for emotion, percentage in sorted(question_result['emotion_percentages'].items(),
                                         key=lambda x: x[1], reverse=True)[:3]:
            if percentage > 0:
                print(f"  {emotion}: {percentage:.1%}")

        # Show personalized feedback
        print("\nFeedback:")
        for insight in question_result['insights'][:2]:  # Limit to top 2 insights
            print(f"- {insight}")

        # Pause before next question
        if i < len(questions) - 1:
            input("\nPress ENTER to continue to the next question...")

    print("\n===== INTERVIEW COMPLETE =====")
    print("Analyzing overall performance...")

    # Return all results for overall analysis
    return results

# Create essential directories
os.makedirs('models', exist_ok=True)

print("\nFace Expression Detection System initialized successfully.")
print("Use 'start_interview_emotion_analysis()' to begin an interview with emotion analysis.")
print("\nSection 6c complete: Face Expression Detection System ready.\n")

"""# Section 8: Generate Data"""

!pip install datasets

"""# Section 5a: Generate Data"""

# ============== SECTION 5: DATASET LOADING AND PREPARATION ==============
# Load and process the Software Engineering interview datasets

import pandas as pd
import numpy as np
import os
import traceback
from datasets import load_dataset
import time

# Check ML libraries availability
try:
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("Warning: Machine learning libraries not available.")

def load_software_engineer_datasets(use_huggingface=True, generate_if_missing=False, num_samples=100000):
    """Load Software Engineering interview datasets from Hugging Face or generate synthetic data

    This function:
    1. Attempts to load data from Hugging Face
    2. Falls back to locally processed data if available
    3. Generates synthetic data as a last resort

    Args:
        use_huggingface: Whether to try loading from Hugging Face first
        generate_if_missing: If True and datasets are missing, generate synthetic data
        num_samples: Number of samples to generate if needed

    Returns:
        DataFrame with processed Software Engineering interview data
    """
    if not ML_AVAILABLE:
        print("Machine learning libraries not available. Cannot load and process datasets.")
        return None

    # First try loading from Hugging Face if requested
    if use_huggingface:
        try:
            print("Attempting to load Software Engineering interview dataset from Hugging Face...")
            # Load from Hugging Face
            splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}
            df = pd.read_parquet("hf://datasets/Vineeshsuiii/Software_Engineering_interview_datasets/" + splits["train"])

            print(f"Loaded {len(df)} records from Hugging Face dataset")

            # Generate synthetic scores for each record
            print("Generating synthetic scores with controlled noise for realistic training...")

            # Function to add controlled noise to scores
            def add_noise(score, noise_level=0.15):
                noise = np.random.normal(0, noise_level * score, size=len(score))
                return np.clip(score + noise, 0, 100)

            # Generate initial scores based on text features
            def calculate_base_score(row):
                # Extract text features that might indicate question complexity
                question = str(row.get('question', ''))
                answer = str(row.get('answer', ''))

                # Simple complexity indicators
                complexity_indicators = ['complex', 'difficult', 'advanced', 'explain', 'design', 'implement']
                basic_indicators = ['basic', 'simple', 'describe', 'what is', 'define']

                # Calculate base score
                base = 75  # Start with average score

                # Adjust based on text length and complexity
                word_count = len(question.split())
                base += min(10, word_count / 20)  # Longer questions might be more complex

                # Adjust for complexity indicators
                complex_count = sum(1 for ind in complexity_indicators if ind in question.lower())
                basic_count = sum(1 for ind in basic_indicators if ind in question.lower())

                base += complex_count * 3
                base -= basic_count * 2

                # Add some randomness
                base += np.random.normal(0, 5)

                return np.clip(base, 50, 95)

            # Calculate initial scores based on question content
            df['base_score'] = df.apply(calculate_base_score, axis=1)

            # Generate component scores with realistic correlations and noise
            df['technical'] = add_noise(df['base_score'] + np.random.normal(0, 8, size=len(df)), noise_level=0.2)
            df['problem_solving'] = add_noise(df['base_score'] + np.random.normal(0, 7, size=len(df)), noise_level=0.18)
            df['communication'] = add_noise(df['base_score'] + np.random.normal(-2, 10, size=len(df)), noise_level=0.25)
            df['cultural_fit'] = add_noise(df['base_score'] + np.random.normal(-5, 12, size=len(df)), noise_level=0.3)

            # Introduce some outliers (about 5% of the data)
            outlier_mask = np.random.choice([True, False], size=len(df), p=[0.05, 0.95])
            for col in ['technical', 'problem_solving', 'communication', 'cultural_fit']:
                df.loc[outlier_mask, col] = np.random.choice([
                    np.random.uniform(30, 50),  # Low outliers
                    np.random.uniform(90, 100)  # High outliers
                ], size=sum(outlier_mask))

            # Calculate overall score with weighted average and additional noise
            df['overall'] = add_noise(
                0.4 * df['technical'] +
                0.3 * df['problem_solving'] +
                0.2 * df['communication'] +
                0.1 * df['cultural_fit'],
                noise_level=0.1
            )

            # Round all scores to integers
            score_columns = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']
            for col in score_columns:
                df[col] = df[col].round().astype(int)
                df[col] = df[col].clip(0, 100)  # Ensure within valid range

            # Drop the temporary base_score column
            df = df.drop('base_score', axis=1)

            print("\nGenerated synthetic scores with realistic noise and correlations")
            print(f"Score statistics:")
            for col in score_columns:
                print(f"- {col}:")
                print(f"  Range: {df[col].min()}-{df[col].max()}")
                print(f"  Mean: {df[col].mean():.1f}")
                print(f"  Std: {df[col].std():.1f}")

            # Process and save the dataset
            ml_dataset = process_dataset_for_ml(df)
            ml_dataset.to_csv("processed_se_interview_data.csv", index=False)
            print(f"\nProcessed dataset saved with {len(ml_dataset)} records")
            return ml_dataset

        except Exception as e:
            print(f"Error loading from Hugging Face: {e}")
            print("Falling back to local processed dataset or synthetic data generation...")

    # Then try to load local processed dataset
    try:
        ml_dataset = pd.read_csv("processed_se_interview_data.csv")
        print(f"Loaded pre-processed Software Engineering dataset with {len(ml_dataset)} records")
        return ml_dataset
    except FileNotFoundError:
        print("No processed Software Engineering dataset found, checking raw data files...")

    # Try loading from raw files
    try:
        # Load the datasets
        questions_df = pd.read_csv("se_interview_questions.csv")
        evaluations_df = pd.read_csv("se_evaluation_scores.csv")

        print(f"Loaded datasets:")
        print(f"- SE Interview Questions: {len(questions_df)} records")
        print(f"- SE Evaluation Scores: {len(evaluations_df)} records")

        # Merge datasets
        merged_df = pd.merge(evaluations_df, questions_df, on='question_id', how='inner')
        print(f"- Merged Dataset: {len(merged_df)} records after joining")

        # Check if we have enough data
        if len(merged_df) < 1000:
            print(f"Warning: Only {len(merged_df)} records available for training.")
            if generate_if_missing:
                print(f"Generating {num_samples} synthetic records as requested...")
                from generate_dataset import generate_software_engineer_dataset
                return generate_software_engineer_dataset(num_samples)
            else:
                print("Consider generating more data for better model training.")

        # Process for ML using the helper function
        ml_dataset = process_dataset_for_ml(merged_df)
        ml_dataset.to_csv("processed_se_interview_data.csv", index=False)

        return ml_dataset

    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("Required dataset files not found.")

        if generate_if_missing:
            print(f"Generating {num_samples} synthetic Software Engineer records...")
            from generate_dataset import generate_software_engineer_dataset
            return generate_software_engineer_dataset(num_samples)
        else:
            print("To generate synthetic data, call load_software_engineer_datasets(generate_if_missing=True)")
            return None
    except Exception as e:
        print(f"Error processing datasets: {e}")
        traceback.print_exc()
        return None

def process_dataset_for_ml(data):
    """Process a dataset to create ML-ready features for Software Engineering interviews.

    Handles both HuggingFace datasets and synthetic data formats.
    """
    print("Processing dataset for machine learning...")
    processed_df = data.copy()
    feature_columns = []

    # Ensure we have the expected target columns
    required_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']
    missing_targets = [target for target in required_targets if target not in processed_df.columns]

    if missing_targets:
        print(f"Generating synthetic values for missing targets: {missing_targets}")

        # Generate base score if no score/overall exists
        if 'score' in processed_df.columns:
            base = processed_df['score']
        elif 'overall' in processed_df.columns:
            base = processed_df['overall']
        else:
            base = np.random.randint(60, 100, size=len(processed_df))

        # Generate missing targets with correlation to base score
        for target in missing_targets:
            processed_df[target] = np.clip(base + np.random.randint(-10, 11, size=len(processed_df)), 0, 100)
            print(f"Generated synthetic values for {target}")

    # Basic numeric features
    if 'question_length' not in processed_df.columns and 'question' in processed_df.columns:
        processed_df['question_length'] = processed_df['question'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)
        feature_columns.append('question_length')
    else:
        if 'question_length' in processed_df.columns:
            feature_columns.append('question_length')

    if 'word_count' not in processed_df.columns and 'question' in processed_df.columns:
        processed_df['word_count'] = processed_df['question'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)
        feature_columns.append('word_count')
    else:
        if 'word_count' in processed_df.columns:
            feature_columns.append('word_count')

    # Answer-based features if available
    if 'answer' in processed_df.columns:
        if 'answer_length' not in processed_df.columns:
            processed_df['answer_length'] = processed_df['answer'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)
        if 'answer_word_count' not in processed_df.columns:
            processed_df['answer_word_count'] = processed_df['answer'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)

        feature_columns.extend(['answer_length', 'answer_word_count'])

    # Handle candidate_id if present
    if 'candidate_id' in processed_df.columns:
        feature_columns.append('candidate_id')

    # Date features
    if 'evaluation_date' in processed_df.columns:
        processed_df['evaluation_date'] = pd.to_datetime(processed_df['evaluation_date'])
        processed_df['month'] = processed_df['evaluation_date'].dt.month
        processed_df['year'] = processed_df['evaluation_date'].dt.year
        feature_columns.extend(['month', 'year'])

    # Handle categorical features - create dummies
    categorical_features = []

    if 'question_type' in processed_df.columns:
        question_type_dummies = pd.get_dummies(processed_df['question_type'], prefix='question_type')
        categorical_features.append(question_type_dummies)

    if 'difficulty_level' in processed_df.columns:
        difficulty_dummies = pd.get_dummies(processed_df['difficulty_level'], prefix='difficulty')
        categorical_features.append(difficulty_dummies)

    if 'topic' in processed_df.columns:
        topic_dummies = pd.get_dummies(processed_df['topic'], prefix='topic')
        categorical_features.append(topic_dummies)

    # Create the ML dataset
    ml_df = processed_df[feature_columns].copy() if feature_columns else pd.DataFrame()

    # Add categorical features
    for cat_feature in categorical_features:
        ml_df = pd.concat([ml_df, cat_feature], axis=1)

    # Add target variables (including the synthetic ones we generated)
    for target in required_targets:
        if target in processed_df.columns:
            ml_df[target] = processed_df[target]

    # Create recommendation classification if overall score is available
    if 'overall' in ml_df.columns:
        ml_df['recommendation'] = pd.cut(
            ml_df['overall'],
            bins=[0, 60, 80, 101],
            labels=['Not Recommended', 'Consider', 'Strongly Recommended']
        )

    # Keep original text columns for reference
    text_columns = ['question', 'answer', 'question_type', 'difficulty_level', 'topic']
    for col in text_columns:
        if col in processed_df.columns:
            ml_df[col + '_text'] = processed_df[col]

    print(f"Dataset processed with {len(ml_df)} samples and {len(ml_df.columns)} features.")
    return ml_df

# Replace the previous data loading with our new Software Engineering focused approach
if ML_AVAILABLE:
    print("Loading and preprocessing Software Engineering interview datasets...")
    training_data = load_software_engineer_datasets(use_huggingface=True)
    if training_data is not None:
        print("\nTraining data sample:")
        print(training_data.head())
    else:
        print("Failed to load Software Engineering interview datasets. Please check for errors above.")
else:
    print("Skipping dataset loading due to missing ML libraries.")

print("Section 5 complete: Software Engineering dataset preparation finished.\n")

"""# Section 9: Data Retrieval"""

"""# Section 5b: Data Retrieval"""

import pandas as pd
import numpy as np
import random
import os
import re
import string
from datetime import datetime, timedelta
from datasets import load_dataset

# Import our hybrid dataset generator
try:
    from hybrid_dataset_generator import generate_hybrid_dataset
    HYBRID_GENERATOR_AVAILABLE = True
except ImportError:
    HYBRID_GENERATOR_AVAILABLE = False
    print("Hybrid dataset generator not available, will use standard methods")

def load_hf_dataset():
    """Load Software Engineering interview dataset from Hugging Face"""
    print("Loading Software Engineering interview dataset from Hugging Face...")

    try:
        # Load the dataset from Hugging Face
        splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}
        df = pd.read_parquet("hf://datasets/Vineeshsuiii/Software_Engineering_interview_datasets/" + splits["train"])

        # Filter for Software Engineer roles only
        if 'profession' in df.columns:
            df = df[df['profession'] == 'Software Engineer']
            print(f"Filtered dataset to include only Software Engineer roles: {len(df)} records")
        else:
            print("Warning: 'profession' column not found in dataset. Using all data.")

        # Ensure we have all required target columns
        required_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']
        missing_targets = [target for target in required_targets if target not in df.columns]

        if missing_targets:
            print(f"Warning: Missing target columns: {missing_targets}")
            print("Will generate synthetic values for missing targets")

            # Generate missing targets with some correlation to existing ones
            for target in missing_targets:
                if 'score' in df.columns:  # Use overall score as base if available
                    base = df['score']
                elif 'overall' in df.columns and target != 'overall':
                    base = df['overall']
                else:
                    base = np.random.randint(60, 100, size=len(df))

                # Add some random variation
                df[target] = np.clip(base + np.random.randint(-10, 11, size=len(df)), 0, 100)

        # Create any missing features that might be needed for modeling
        if 'question_length' not in df.columns and 'question' in df.columns:
            df['question_length'] = df['question'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)

        if 'word_count' not in df.columns and 'question' in df.columns:
            df['word_count'] = df['question'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)

        print(f"Dataset loaded successfully with {len(df)} records")
        return df

    except Exception as e:
        print(f"Error loading dataset from Hugging Face: {str(e)}")
        print("Falling back to synthetic data generation...")
        return None

def generate_software_engineer_dataset(num_samples=10000):
    """Generate a synthetic dataset focused only on Software Engineer roles"""
    print(f"Generating {num_samples} Software Engineer interview samples...")

    # Define data distributions for Software Engineer questions
    question_types = ["Technical", "Behavioral", "Problem-Solving", "Cultural Fit", "Leadership"]
    difficulty_levels = ["Easy", "Medium", "Hard"]

    # Software Engineer specific topics
    se_topics = [
        "Algorithms", "Data Structures", "Object-Oriented Programming",
        "System Design", "Database Design", "Web Development",
        "DevOps", "Cloud Infrastructure", "Testing", "Debugging",
        "Version Control", "API Design", "Microservices", "Security"
    ]

    # Ensure a more balanced distribution across categories
    # For a 100K dataset, aim for roughly 25K weak, 50K average, 25K strong
    weak_proportion = 0.25
    average_proportion = 0.5
    strong_proportion = 0.25

    # Calculate sample counts
    weak_count = int(num_samples * weak_proportion)
    average_count = int(num_samples * average_proportion)
    strong_count = num_samples - weak_count - average_count

    # Generate questions and evaluations
    questions_data = []
    evaluations_data = []

    # Track the current counts for each category
    current_weak = 0
    current_average = 0
    current_strong = 0

    # Generate short responses for some of the weak examples (minimal/poor answers)
    minimal_responses = [
        "I don't know.",
        "Not sure",
        "No idea",
        "idk",
        "Skip",
        "I have no experience with that",
        "Never done that before",
        "What do you mean?",
        "Can you explain the question?",
        "This isn't my area",
        "I'm not familiar with this",
        "I'd have to look that up",
        "I haven't worked on that",
        "I'll get back to you on that",
        "That's not in my skill set",
        "Haven't learned that yet",
        "I'm confused by your question",
        "Can we move to another question?",
        "I'm drawing a blank",
        "I'd need to ask someone else",
    ]

    for i in range(1, num_samples + 1):
        # Generate question
        question_id = i
        topic = random.choice(se_topics)
        question_type = random.choice(question_types)
        difficulty = random.choice(difficulty_levels)

        # Determine which category this sample should be (weak, average, strong)
        if current_weak < weak_count:
            target_category = "weak"
            current_weak += 1
        elif current_average < average_count:
            target_category = "average"
            current_average += 1
        else:
            target_category = "strong"
            current_strong += 1

        # Generate questions based on type
        if question_type == "Technical":
            question = f"Explain how you would implement {topic} in a real-world software project."
        elif question_type == "Behavioral":
            question = f"Describe a time when you had to collaborate with others to solve a {topic.lower()} challenge."
        elif question_type == "Problem-Solving":
            question = f"How would you approach debugging a complex issue related to {topic}?"
        elif question_type == "Cultural Fit":
            question = f"How do you stay up-to-date with the latest developments in {topic}?"
        else:  # Leadership
            question = f"Tell me about a time you led a project involving {topic}."

        # Add to questions data
        questions_data.append({
            "id": question_id,
            "question_text": question,
            "question_type": question_type,
            "topic": topic,
            "difficulty": difficulty
        })

        # Generate answer based on target category
        if target_category == "weak":
            # For weak responses, we'll have some minimal responses and some longer but low-quality ones
            if random.random() < 0.4:  # 40% chance of minimal response
                answer = random.choice(minimal_responses)
                answer_length = "Very Short"
                technical_accuracy = "Low"
                communication_clarity = "Low"
                problem_solving = "Weak"
                cultural_fit = "Poor"
            else:
                answer = f"I think {topic} is important but I'm not very familiar with it. I would probably need to do some research to understand how to apply it properly."
                answer_length = "Short"
                technical_accuracy = random.choices(["Low", "Medium"], weights=[0.8, 0.2])[0]
                communication_clarity = random.choices(["Low", "Medium"], weights=[0.7, 0.3])[0]
                problem_solving = random.choices(["Weak", "Adequate"], weights=[0.8, 0.2])[0]
                cultural_fit = random.choices(["Poor", "Neutral"], weights=[0.7, 0.3])[0]

            # Set scores in the appropriate range for weak responses
            technical_score = random.uniform(10, 35)
            communication_score = random.uniform(10, 35)
            problem_solving_score = random.uniform(10, 35)
            cultural_fit_score = random.uniform(10, 35)
            overall_score = (technical_score + communication_score + problem_solving_score + cultural_fit_score) / 4

        elif target_category == "average":
            answer = f"When working with {topic}, I try to follow best practices. I've used it in a few projects before. " \
                     f"For example, in one project, we implemented a {topic} solution that helped improve our process."

            answer_length = random.choices(["Medium", "Long"], weights=[0.7, 0.3])[0]
            technical_accuracy = random.choices(["Medium", "High"], weights=[0.8, 0.2])[0]
            communication_clarity = random.choices(["Medium", "High"], weights=[0.7, 0.3])[0]
            problem_solving = random.choices(["Adequate", "Strong"], weights=[0.7, 0.3])[0]
            cultural_fit = random.choices(["Neutral", "Good"], weights=[0.6, 0.4])[0]

            # Set scores in the appropriate range for average responses
            technical_score = random.uniform(40, 70)
            communication_score = random.uniform(40, 70)
            problem_solving_score = random.uniform(40, 70)
            cultural_fit_score = random.uniform(40, 70)
            overall_score = (technical_score + communication_score + problem_solving_score + cultural_fit_score) / 4

        else:  # strong
            answer = f"I have extensive experience with {topic}. In my previous role, I led the implementation of a " \
                     f"{topic} solution that resulted in a 30% improvement in performance. " \
                     f"I follow a systematic approach where I first analyze requirements, then design a solution " \
                     f"considering scalability and maintainability. I've also mentored junior engineers on best practices for {topic}."

            answer_length = "Long"
            technical_accuracy = "High"
            communication_clarity = random.choices(["Medium", "High"], weights=[0.2, 0.8])[0]
            problem_solving = "Strong"
            cultural_fit = random.choices(["Neutral", "Good"], weights=[0.1, 0.9])[0]

            # Set scores in the appropriate range for strong responses
            technical_score = random.uniform(75, 100)
            communication_score = random.uniform(75, 100)
            problem_solving_score = random.uniform(75, 100)
            cultural_fit_score = random.uniform(75, 100)
            overall_score = (technical_score + communication_score + problem_solving_score + cultural_fit_score) / 4

        # Add to evaluations data
        evaluations_data.append({
            "id": question_id,
            "answer_text": answer,
            "technical_score": technical_score,
            "communication_score": communication_score,
            "problem_solving_score": problem_solving_score,
            "cultural_fit_score": cultural_fit_score,
            "overall_score": overall_score,
            "answer_length": answer_length,
            "technical_accuracy": technical_accuracy,
            "communication_clarity": communication_clarity,
            "problem_solving": problem_solving,
            "cultural_fit": cultural_fit,
            "category": target_category.capitalize(),  # "Weak", "Average", or "Strong"
            "evaluation_date": datetime.now().strftime("%Y-%m-%d")  # Add today's date
        })

        # Show progress
        if i % 2000 == 0 or i == num_samples:
            print(f"Generated {i}/{num_samples} questions and {len(evaluations_data)} evaluations")

    # Create DataFrames
    questions_df = pd.DataFrame(questions_data)
    evaluations_df = pd.DataFrame(evaluations_data)

    # Merge the datasets
    merged_df = pd.merge(questions_df, evaluations_df, on="id")

    # Save to CSV
    print("Saving CSV files...")
    merged_df.to_csv("se_interview_questions.csv", index=False)

    print(f"Dataset generation complete!")
    print(f"- Questions dataset: {len(questions_df)} records")
    print(f"- Evaluations dataset: {len(evaluations_df)} records")

    # Now create a merged dataset for ML
    print("Creating merged ML dataset...")
    merged_df = pd.merge(evaluations_df, questions_df, on='id', how='inner')

    # Create dummy variables for categorical features
    question_type_dummies = pd.get_dummies(merged_df['question_type'], prefix='question_type')
    difficulty_dummies = pd.get_dummies(merged_df['difficulty'], prefix='difficulty')
    topic_dummies = pd.get_dummies(merged_df['topic'], prefix='topic')

    # Create date-related features
    try:
        merged_df['evaluation_date'] = pd.to_datetime(merged_df['evaluation_date'])
        merged_df['month'] = merged_df['evaluation_date'].dt.month
        merged_df['year'] = merged_df['evaluation_date'].dt.year
    except (KeyError, ValueError) as e:
        print(f"Warning: Error processing dates: {e}. Using default values.")
        # Use current date information if there's an issue
        current_date = datetime.now()
        merged_df['month'] = current_date.month
        merged_df['year'] = current_date.year

    # Text-based features from questions
    merged_df['question_length'] = merged_df['question_text'].apply(len)
    merged_df['word_count'] = merged_df['question_text'].apply(lambda x: len(str(x).split()))

    # Create the final feature dataset
    features = pd.concat([
        merged_df[['id', 'question_length', 'word_count', 'month', 'year']],
        question_type_dummies,
        difficulty_dummies,
        topic_dummies
    ], axis=1)

    # Create ML dataset
    ml_dataset = features.copy()
    ml_dataset['score'] = merged_df['overall_score']
    ml_dataset['overall'] = merged_df['overall_score']
    ml_dataset['technical'] = merged_df['technical_score']
    ml_dataset['communication'] = merged_df['communication_score']
    ml_dataset['problem_solving'] = merged_df['problem_solving_score']
    ml_dataset['cultural_fit'] = merged_df['cultural_fit_score']

    # Map recommendation categories
    def map_to_category(score):
        if score >= 75:
            return 2  # Strong
        elif score >= 40:
            return 1  # Average
        else:
            return 0  # Weak

    # Add classifier target that maps directly to the category field (Weak=0, Average=1, Strong=2)
    ml_dataset['classifier'] = merged_df['category'].map({'Weak': 0, 'Average': 1, 'Strong': 2})

    # Also calculate the classifier based on overall score (use this as a fallback)
    ml_dataset['classifier_score_based'] = ml_dataset['overall'].apply(map_to_category)

    # Store text fields for reference
    ml_dataset['question_text'] = merged_df['question_text']
    ml_dataset['question_type'] = merged_df['question_type']
    ml_dataset['difficulty_level'] = merged_df['difficulty']
    ml_dataset['topic'] = merged_df['topic']

    # Add the answer_text column that's required by process_dataset_for_ml
    if 'answer_text' not in ml_dataset.columns:
        # Create sample answers based on the category
        def generate_sample_answer(row):
            quality = "excellent" if row['classifier'] == 2 else "average" if row['classifier'] == 1 else "poor"
            question = row['question_text']
            topic = row['topic']

            if quality == "excellent":
                return f"This is a comprehensive answer to the {topic} question about '{question[:30]}...'. The answer demonstrates strong technical knowledge and detailed understanding."
            elif quality == "average":
                return f"This is a satisfactory answer to the {topic} question about '{question[:30]}...'. The answer shows basic understanding but lacks some details."
            else:
                return f"This is a basic answer to the {topic} question about '{question[:30]}...'. The answer has several gaps in understanding."

        ml_dataset['answer_text'] = ml_dataset.apply(generate_sample_answer, axis=1)
        print("Generated answer_text column required for processing")

    # Save processed dataset
    ml_dataset.to_csv("processed_se_interview_data.csv", index=False)
    print(f"Processed dataset saved with {len(ml_dataset)} records")

    return ml_dataset

def get_interview_dataset(num_samples=10000, use_huggingface=True, evaluation_method='hybrid'):
    """Main function to get software engineering interview dataset

    Args:
        num_samples: Number of samples to generate if needed
        use_huggingface: Whether to try using Hugging Face data
        evaluation_method: 'direct' to use scores from HF, 'hybrid' to generate scores from answer quality

    Returns:
        Processed dataset ready for ML training
    """

    # Try hybrid approach with answer evaluation
    if use_huggingface and evaluation_method == 'hybrid' and HYBRID_GENERATOR_AVAILABLE:
        print("Using hybrid approach with Hugging Face Q&A and generated evaluations...")
        hybrid_data = generate_hybrid_dataset(num_samples)
        if hybrid_data is not None:
            return hybrid_data

    # Direct Hugging Face approach
    if use_huggingface and evaluation_method == 'direct':
        hf_data = load_hf_dataset()
        if hf_data is not None:
            return hf_data

    # Fall back to synthetic data generation
    print(f"Generating synthetic interview dataset with {num_samples} samples...")
    raw_data = generate_software_engineer_dataset(num_samples)

    # Process the raw data into ML-ready format
    processed_data = process_dataset_for_ml(raw_data)

    if processed_data is not None:
        print(f"Successfully processed {len(processed_data)} records")
        return processed_data
    else:
        print("Warning: Data processing failed, returning raw data")
        # Check if raw_data already has essential ML columns
        if all(col in raw_data.columns for col in ['classifier', 'technical', 'communication', 'problem_solving', 'overall']):
            print("Raw data already contains essential ML columns, using as is")
        else:
            # Add minimal processing to ensure raw_data is usable
            print("Adding minimal processing to raw data")
            if 'classifier' not in raw_data.columns and 'overall' in raw_data.columns:
                raw_data['classifier'] = raw_data['overall'].apply(map_to_category)
        return raw_data

def load_sentiment_dataset():
    """Load multiclass sentiment analysis dataset from Hugging Face"""
    print("Loading sentiment analysis dataset from Hugging Face...")

    try:
        # Load the sentiment analysis dataset from Hugging Face
        sentiment_dataset = load_dataset("Sp1786/multiclass-sentiment-analysis-dataset")

        # Convert to DataFrame for easier processing
        df = pd.DataFrame({
            'text': sentiment_dataset['train']['text'],
            'sentiment_label': sentiment_dataset['train']['label'],
            'sentiment': sentiment_dataset['train']['sentiment']
        })

        print(f"Loaded {len(df)} records from sentiment analysis dataset")
        return df
    except Exception as e:
        print(f"Error loading sentiment dataset: {e}")
        return None

def process_dataset_for_ml(data):
    """
    Process raw data into a format suitable for ML modeling.
    This extracts features from interview responses and prepares the data for training.

    Args:
        data: Raw dataset with interview questions and responses

    Returns:
        Processed dataset with features and targets
    """
    if 'answer_text' not in data.columns:
        print("Warning: answer_text column not found.")

        # Try to create an answer_text column from existing data
        if 'response' in data.columns:
            print("Using 'response' column as answer_text")
            data['answer_text'] = data['response']
        elif 'answer' in data.columns:
            print("Using 'answer' column as answer_text")
            data['answer_text'] = data['answer']
        elif 'question_text' in data.columns and 'classifier' in data.columns:
            print("Generating sample answer_text from question_text and classifier")
            # Generate simple dummy answers based on the classification
            def generate_fallback_answer(row):
                quality = "excellent" if row['classifier'] == 2 else "average" if row['classifier'] == 1 else "poor"
                question = row['question_text'] if isinstance(row['question_text'], str) else "the question"
                question_prefix = question[:30] if len(question) > 30 else question

                if quality == "excellent":
                    return f"This is a detailed answer to {question_prefix}... demonstrating excellent knowledge."
                elif quality == "average":
                    return f"This is an adequate answer to {question_prefix}... with some correct information."
                else:
                    return f"This is a minimal answer to {question_prefix}... with several inaccuracies."

            data['answer_text'] = data.apply(generate_fallback_answer, axis=1)
        else:
            print("Cannot generate answer_text. Cannot process this dataset.")
            return None

    print("Processing dataset for ML...")

    try:
        # Try to import our feature extraction function
        import sys
        from pathlib import Path
        import os

        # Get current directory in a way that works in both scripts and notebooks
        try:
            # For regular Python scripts
            current_dir = Path(__file__).parent
        except NameError:
            # For Jupyter/Colab notebooks
            import os
            current_dir = Path(os.getcwd())

        parent_dir = current_dir.parent
        if str(parent_dir) not in sys.path:
            sys.path.append(str(parent_dir))

        # Import feature extraction
        try:
            from section_8_interview_process import extract_features_from_text
            FEATURE_EXTRACTOR_AVAILABLE = True
        except ImportError:
            FEATURE_EXTRACTOR_AVAILABLE = False
            print("Feature extractor not available, will use basic features")

        # Process each row to extract features
        processed_rows = []

        for idx, row in data.iterrows():
            if idx % 1000 == 0:
                print(f"Processing row {idx}/{len(data)}...")

            # Get the answer text
            answer = row['answer_text']

            # Extract features
            if FEATURE_EXTRACTOR_AVAILABLE:
                features = extract_features_from_text(answer)
            else:
                # Basic feature extraction
                word_count = len(answer.split())
                char_count = len(answer)

                # Detect minimal responses
                minimal_response = word_count <= 5
                minimal_response_penalty = 0.2 if minimal_response else 1.0

                # Calculate basic metrics
                avg_word_length = char_count / max(word_count, 1)
                sentence_count = answer.count('.') + answer.count('!') + answer.count('?')

                # Simple word-based sentiment approximation (very basic)
                positive_words = ["good", "great", "excellent", "amazing", "awesome", "yes"]
                negative_words = ["bad", "poor", "no", "don't", "cant", "idk", "not"]

                positive_count = sum(1 for word in positive_words if word in answer.lower())
                negative_count = sum(1 for word in negative_words if word in answer.lower())
                sentiment = 0.3 if minimal_response else (0.5 + (positive_count - negative_count) * 0.1)

                # Create feature dictionary
                features = {
                    'feature_0': float(word_count * minimal_response_penalty),  # Word count
                    'feature_1': float(char_count * minimal_response_penalty),  # Character count
                    'feature_2': float(avg_word_length),                        # Avg word length
                    'feature_3': float(sentence_count * minimal_response_penalty),  # Sentence count
                    'feature_4': float(0 if minimal_response else 2),           # Technical keywords
                    'feature_5': float(0 if minimal_response else 2),           # Communication keywords
                    'feature_6': float(0 if minimal_response else 2),           # Problem-solving keywords
                    'feature_7': float(0 if minimal_response else 2),           # Cultural fit keywords
                    'feature_8': float(30 if minimal_response else 60),         # Confidence score
                    'feature_9': float(0.1 if minimal_response else 0.5),       # Keyword ratio
                    'feature_10': float(sentiment)                              # Sentiment
                }

            # Add targets
            if 'technical_score' in row:
                features['technical'] = row['technical_score']
            if 'communication_score' in row:
                features['communication'] = row['communication_score']
            if 'problem_solving_score' in row:
                features['problem_solving'] = row['problem_solving_score']
            if 'cultural_fit_score' in row:
                features['cultural_fit'] = row['cultural_fit_score']
            if 'overall_score' in row:
                features['overall'] = row['overall_score']
            if 'category' in row:
                category_map = {'Weak': 0, 'Average': 1, 'Strong': 2}
                features['classifier'] = category_map.get(row['category'], 1)  # Default to Average if missing

            processed_rows.append(features)

        # Convert to DataFrame
        processed_df = pd.DataFrame(processed_rows)
        print(f"Processed {len(processed_df)} rows successfully")

        # Ensure we have all the required feature columns
        required_features = [f'feature_{i}' for i in range(11)]
        for feature in required_features:
            if feature not in processed_df.columns:
                processed_df[feature] = 0

        # Ensure we have all the required target columns
        required_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall', 'classifier']
        for target in required_targets:
            if target not in processed_df.columns:
                processed_df[target] = 50  # Default value (except for classifier)

        if 'classifier' not in processed_df.columns:
            processed_df['classifier'] = 1  # Default to Average

        return processed_df

    except Exception as e:
        print(f"Error processing dataset: {e}")
        import traceback
        traceback.print_exc()
        return None

    return None

if __name__ == "__main__":
    # Generate hybrid dataset with answer evaluations by default
    get_interview_dataset(num_samples=100000, use_huggingface=True, evaluation_method='hybrid')

"""   # Section 10: Career Intelligence System"""

# Install required packages
!pip install -q datasets

# ============================
# SECTION 6a: CAREER DATA GENERATION AND RETRIEVAL
# ============================

import random
import pandas as pd
import numpy as np
import json
import os
from tqdm.notebook import tqdm
from IPython.display import display, HTML, clear_output
from datasets import load_dataset
from datetime import datetime, timedelta
import spacy
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import pipeline
from collections import Counter

print("Initializing Career Data Generation and Retrieval...")

# Comprehensive list of supported roles for interview generation
roles = [
    "AI Engineer", "Cybersecurity Analyst", "Data Analyst", "Full Stack Developer",
    "UI/UX Designer", "HR Business Partner", "Cloud Solutions Architect", "Software Tester",
    "Machine Learning Engineer", "DevOps Engineer", "Front-End Developer", "Back-End Developer",
    "Database Administrator", "IT Support Specialist", "Data Scientist", "Product Manager",
    "Business Intelligence Analyst", "IT Project Manager", "Digital Marketing Specialist",
    "Network Security Engineer", "Penetration Tester (Ethical Hacker)", "Computer Vision Engineer",
    "NLP Engineer", "Scrum Master", "Mobile App Developer", "Blockchain Developer",
    "Robotics Engineer", "Technical Support Engineer", "Systems Administrator", "IT Auditor",
    "Finance Manager", "Project Manager"
]

print(f"Initialized {len(roles)} career roles for the Tamkeen AI Career Intelligence System")

# Define industry sectors
sectors = [
    "Technology", "Healthcare", "Finance", "Education", "Manufacturing",
    "Retail", "Government", "Energy", "Telecommunications", "Transportation",
    "Entertainment", "Agriculture", "Construction", "Hospitality", "Consulting"
]

# Map roles to likely sectors (for synthetic data generation)
role_sector_mapping = {
    "AI Engineer": ["Technology", "Finance", "Healthcare", "Energy"],
    "Cybersecurity Analyst": ["Technology", "Finance", "Government", "Healthcare"],
    "Data Analyst": ["Technology", "Finance", "Healthcare", "Retail", "Manufacturing"],
    "Full Stack Developer": ["Technology", "Telecommunications", "Finance", "Entertainment"],
    "UI/UX Designer": ["Technology", "Retail", "Entertainment", "Consulting"],
    "HR Business Partner": ["Technology", "Healthcare", "Finance", "Manufacturing", "Retail"],
    "Cloud Solutions Architect": ["Technology", "Finance", "Healthcare", "Telecommunications"],
    "Software Tester": ["Technology", "Finance", "Healthcare", "Manufacturing"],
    "Machine Learning Engineer": ["Technology", "Finance", "Healthcare", "Manufacturing"],
    "DevOps Engineer": ["Technology", "Finance", "Telecommunications", "Retail"],
    "Front-End Developer": ["Technology", "Entertainment", "Retail", "Finance"],
    "Back-End Developer": ["Technology", "Finance", "Telecommunications", "Retail"],
    "Database Administrator": ["Technology", "Finance", "Healthcare", "Government"],
    "IT Support Specialist": ["Technology", "Healthcare", "Education", "Finance"],
    "Data Scientist": ["Technology", "Finance", "Healthcare", "Energy", "Manufacturing"],
    "Product Manager": ["Technology", "Retail", "Finance", "Telecommunications"],
    "Business Intelligence Analyst": ["Finance", "Retail", "Technology", "Healthcare"],
    "IT Project Manager": ["Technology", "Finance", "Healthcare", "Manufacturing"],
    "Digital Marketing Specialist": ["Retail", "Technology", "Entertainment", "Hospitality"],
    "Network Security Engineer": ["Technology", "Finance", "Government", "Telecommunications"],
    "Penetration Tester (Ethical Hacker)": ["Technology", "Finance", "Government", "Energy"],
    "Computer Vision Engineer": ["Technology", "Healthcare", "Manufacturing", "Transportation"],
    "NLP Engineer": ["Technology", "Healthcare", "Finance", "Telecommunications"],
    "Scrum Master": ["Technology", "Finance", "Healthcare", "Manufacturing"],
    "Mobile App Developer": ["Technology", "Retail", "Entertainment", "Finance"],
    "Blockchain Developer": ["Finance", "Technology", "Government", "Energy"],
    "Robotics Engineer": ["Manufacturing", "Technology", "Healthcare", "Transportation"],
    "Technical Support Engineer": ["Technology", "Telecommunications", "Healthcare", "Education"],
    "Systems Administrator": ["Technology", "Finance", "Government", "Education"],
    "IT Auditor": ["Finance", "Technology", "Government", "Healthcare"],
    "Finance Manager": ["Finance", "Technology", "Healthcare", "Manufacturing", "Retail"],
    "Project Manager": ["Technology", "Construction", "Manufacturing", "Finance", "Healthcare"]
}

# Define skills for each role (for synthetic data generation)
role_skills_mapping = {
    "AI Engineer": ["Python", "Machine Learning", "TensorFlow", "PyTorch", "Deep Learning", "Computer Vision", "NLP", "Data Structures", "Algorithms", "Cloud Computing"],
    "Cybersecurity Analyst": ["Network Security", "Penetration Testing", "Vulnerability Assessment", "SIEM", "Security Auditing", "Linux", "Incident Response", "Firewall Management", "Risk Assessment", "Security Compliance"],
    "Data Analyst": ["SQL", "Python", "Data Visualization", "Statistical Analysis", "Excel", "Power BI", "Tableau", "Data Cleaning", "Reporting", "Database Management"],
    "Full Stack Developer": ["JavaScript", "React", "Node.js", "HTML/CSS", "SQL", "RESTful APIs", "Git", "MongoDB", "AWS", "Docker"],
    "UI/UX Designer": ["Figma", "Adobe XD", "Wireframing", "Prototyping", "User Research", "Information Architecture", "Usability Testing", "Visual Design", "Responsive Design", "Interaction Design"],
    "HR Business Partner": ["Talent Management", "Employee Relations", "HR Metrics", "Organizational Development", "Change Management", "Performance Management", "HRIS", "Compensation & Benefits", "Recruitment", "Labor Laws"],
    "Cloud Solutions Architect": ["AWS", "Azure", "Google Cloud", "IaaS", "PaaS", "Serverless", "Microservices", "Cloud Security", "Containerization", "Infrastructure as Code"],
    "Software Tester": ["Test Planning", "Manual Testing", "Automated Testing", "Selenium", "API Testing", "Test Case Design", "Regression Testing", "JIRA", "QA Methodologies", "Performance Testing"],
    "Machine Learning Engineer": ["Python", "Scikit-learn", "TensorFlow", "Deep Learning", "Computer Vision", "NLP", "Data Preprocessing", "Feature Engineering", "Model Deployment", "MLOps"],
    "DevOps Engineer": ["CI/CD", "Docker", "Kubernetes", "Jenkins", "Terraform", "Ansible", "AWS", "Monitoring", "Linux", "Git"],
    "Front-End Developer": ["HTML/CSS", "JavaScript", "React", "Angular", "Vue.js", "Responsive Design", "SASS/LESS", "Webpack", "Jest", "TypeScript"],
    "Back-End Developer": ["Node.js", "Python", "Java", "C#", "SQL", "NoSQL", "RESTful APIs", "GraphQL", "Microservices", "Authentication/Authorization"],
    "Database Administrator": ["SQL", "Oracle", "MySQL", "PostgreSQL", "Database Design", "Performance Tuning", "Backup & Recovery", "Data Migration", "Database Security", "Query Optimization"],
    "IT Support Specialist": ["Troubleshooting", "Windows", "Mac OS", "Active Directory", "Network Support", "Hardware Support", "Customer Service", "Ticketing Systems", "Remote Support", "IT Documentation"],
    "Data Scientist": ["Python", "R", "Machine Learning", "Statistical Analysis", "Data Visualization", "SQL", "Big Data", "Hypothesis Testing", "A/B Testing", "Data Mining"],
    "Product Manager": ["Product Strategy", "Market Research", "User Stories", "Roadmapping", "Agile Methodologies", "Stakeholder Management", "Competitive Analysis", "Product Lifecycle", "Prioritization", "Analytics"],
    "Business Intelligence Analyst": ["SQL", "Data Visualization", "ETL", "Dimensional Modeling", "Reporting", "Power BI", "Tableau", "Data Warehousing", "Business Requirements", "KPI Tracking"],
    "IT Project Manager": ["Project Planning", "SDLC", "Budget Management", "Risk Management", "Team Leadership", "JIRA", "Agile", "Waterfall", "Stakeholder Management", "Project Documentation"],
    "Digital Marketing Specialist": ["SEO", "SEM", "Social Media Marketing", "Content Marketing", "Email Marketing", "Google Analytics", "PPC", "Marketing Automation", "CRM", "Marketing Strategy"],
    "Network Security Engineer": ["Firewall Management", "VPN", "IDS/IPS", "Network Protocols", "Security Auditing", "SIEM", "Threat Intelligence", "Vulnerability Management", "Zero Trust", "Identity Management"],
    "Penetration Tester (Ethical Hacker)": ["Ethical Hacking", "Vulnerability Assessment", "Burp Suite", "Kali Linux", "OWASP", "Social Engineering", "Metasploit", "Network Security", "Web Application Security", "Security Reporting"],
    "Computer Vision Engineer": ["OpenCV", "Deep Learning", "Image Processing", "Object Detection", "TensorFlow", "PyTorch", "Python", "Feature Extraction", "Image Classification", "Computer Vision Algorithms"],
    "NLP Engineer": ["Natural Language Processing", "Machine Learning", "Deep Learning", "NLTK", "spaCy", "Word Embeddings", "Sentiment Analysis", "Text Classification", "Python", "Transformers"],
    "Scrum Master": ["Agile Methodologies", "Scrum", "Kanban", "Sprint Planning", "Retrospectives", "Team Facilitation", "Coaching", "Impediment Removal", "JIRA", "Stakeholder Management"],
    "Mobile App Developer": ["Android", "iOS", "React Native", "Swift", "Kotlin", "Mobile UI/UX", "API Integration", "Firebase", "App Store Deployment", "Mobile Testing"],
    "Blockchain Developer": ["Blockchain", "Smart Contracts", "Solidity", "Ethereum", "Bitcoin", "Consensus Algorithms", "Cryptography", "Web3.js", "DApps", "Tokenomics"],
    "Robotics Engineer": ["Robotics", "Control Systems", "ROS", "Mechanics", "Sensor Integration", "Computer Vision", "Path Planning", "Embedded Systems", "C++", "MATLAB"],
    "Technical Support Engineer": ["Technical Troubleshooting", "Customer Service", "Product Knowledge", "Remote Support", "Documentation", "Escalation Management", "SLAs", "Ticketing Systems", "Knowledge Base", "Problem Resolution"],
    "Systems Administrator": ["Windows Server", "Linux", "Active Directory", "DNS", "DHCP", "System Monitoring", "Virtualization", "Backup & Recovery", "Patch Management", "PowerShell"],
    "IT Auditor": ["IT Governance", "Risk Assessment", "Compliance", "Audit Methodologies", "SOX", "GDPR", "HIPAA", "Internal Controls", "Documentation", "Reporting"],
    "Finance Manager": ["Financial Planning", "Budgeting", "Forecasting", "Financial Analysis", "Accounting", "Tax Planning", "Cash Flow Management", "Financial Reporting", "Risk Management", "Regulatory Compliance"],
    "Project Manager": ["Project Planning", "Scope Management", "Resource Allocation", "Risk Management", "Stakeholder Communication", "Budgeting", "Timeline Management", "PRINCE2", "PMP", "Agile Methodologies"]
}

# Load external dataset
print("Loading recruitment job descriptions dataset...")
recruitment_ds = None
try:
    recruitment_ds = load_dataset("lang-uk/recruitment-dataset-job-descriptions-english")
    print(f"Successfully loaded recruitment dataset with {len(recruitment_ds['train'])} job descriptions")
except Exception as e:
    print(f"Error loading recruitment dataset: {e}")
    print("Will fall back to synthetic data generation")

def find_matching_external_data(role, dataset=None):
    """Find matching job descriptions from external dataset based on role"""
    if dataset is None or 'train' not in dataset:
        return []

    # Convert role to lowercase for matching
    role_lower = role.lower()
    matching_data = []

    # Check each record in the dataset
    for item in dataset['train']:
        title = item.get('title', '').lower()
        description = item.get('description', '').lower()

        # Check if role matches title or is mentioned in description
        if (role_lower in title or
            role_lower.replace(' ', '') in title.replace(' ', '') or
            role_lower in description):
            matching_data.append(item)

    return matching_data

def generate_synthetic_career_profile(role):
    """Generate a synthetic career profile for a given role"""
    # Select a random sector appropriate for this role
    if role in role_sector_mapping:
        sector = random.choice(role_sector_mapping[role])
    else:
        sector = random.choice(sectors)

    # Generate experience level
    experience_levels = ["Entry Level", "Mid Level", "Senior Level", "Expert"]
    experience_weights = [0.2, 0.4, 0.3, 0.1]  # More mid-level than others
    experience = random.choices(experience_levels, weights=experience_weights)[0]

    # Generate years of experience based on level
    if experience == "Entry Level":
        years = random.randint(0, 2)
    elif experience == "Mid Level":
        years = random.randint(3, 5)
    elif experience == "Senior Level":
        years = random.randint(6, 10)
    else:  # Expert
        years = random.randint(10, 20)

    # Generate salary range based on experience
    base_salary = {
        "Entry Level": random.randint(40000, 60000),
        "Mid Level": random.randint(60000, 90000),
        "Senior Level": random.randint(90000, 130000),
        "Expert": random.randint(130000, 200000)
    }

    # Adjust for role premium/discount
    role_premium = {
        "AI Engineer": 1.3,
        "Cybersecurity Analyst": 1.2,
        "Data Scientist": 1.25,
        "DevOps Engineer": 1.15,
        "Cloud Solutions Architect": 1.3,
        "Machine Learning Engineer": 1.3,
        "Blockchain Developer": 1.2,
        "Computer Vision Engineer": 1.25,
        "NLP Engineer": 1.25,
        "IT Support Specialist": 0.8,
        "Technical Support Engineer": 0.85
    }

    salary_multiplier = role_premium.get(role, 1.0)
    salary = int(base_salary[experience] * salary_multiplier)
    salary_range = f"${salary - 10000} - ${salary + 10000}"

    # Select relevant skills for this role
    if role in role_skills_mapping:
        all_skills = role_skills_mapping[role]
        # For entry/mid level, fewer skills; for senior/expert, more skills
        if experience in ["Entry Level", "Mid Level"]:
            num_skills = random.randint(4, 7)
        else:
            num_skills = random.randint(7, 10)
        skills = random.sample(all_skills, num_skills)
    else:
        # Generic skills if role not in mapping
        generic_skills = ["Communication", "Problem Solving", "Teamwork", "Analytical Skills", "Time Management"]
        skills = random.sample(generic_skills, 3)

    # Generate education level
    education_levels = ["High School", "Associate's Degree", "Bachelor's Degree", "Master's Degree", "PhD"]
    education_weights = [0.05, 0.15, 0.5, 0.25, 0.05]  # Bachelor's most common
    education = random.choices(education_levels, weights=education_weights)[0]

    # Generate job satisfaction score
    satisfaction = random.randint(1, 10)

    # Generate work-life balance rating
    work_life = random.randint(1, 10)

    # Generate job growth potential
    growth_potential = random.randint(1, 10)

    # Generate job description (placeholder - could use LLM in the future)
    description = f"Professional {role} with {years} years of experience in the {sector} sector. Skilled in {', '.join(skills[:-1])} and {skills[-1]}."

    # Generate career profile
    profile = {
        "role": role,
        "sector": sector,
        "experience_level": experience,
        "years_experience": years,
        "skills": skills,
        "education": education,
        "salary_range": salary_range,
        "job_satisfaction": satisfaction,
        "work_life_balance": work_life,
        "growth_potential": growth_potential,
        "description": description,
        "source": "synthetic"
    }

    return profile

def generate_synthetic_career_profiles(num_profiles=100):
    """Generate multiple synthetic career profiles"""
    profiles = []

    print(f"Generating {num_profiles} synthetic career profiles...")
    for _ in tqdm(range(num_profiles)):
        # Select a random role
        role = random.choice(roles)
        profile = generate_synthetic_career_profile(role)
        profiles.append(profile)

    print(f"Generated {len(profiles)} synthetic career profiles")
    return profiles

def prepare_career_dataset(use_external=True, min_profiles=1000):
    """Prepare a comprehensive dataset combining external and synthetic data"""
    combined_dataset = []
    external_count = 0
    synthetic_count = 0

    # First try to use external data if available
    if use_external and recruitment_ds is not None:
        print("Processing external recruitment dataset...")

        # Extract and process each role
        for role in roles:
            matching_data = find_matching_external_data(role, recruitment_ds)
            print(f"Found {len(matching_data)} external listings for {role}")

            for item in matching_data:
                processed_item = {
                    "role": role,
                    "sector": item.get('industry', 'Technology'),  # Default to Technology if not specified
                    "description": item.get('description', ''),
                    "skills": item.get('skills', []),
                    "experience_level": "Not Specified",
                    "years_experience": 0,  # Default value
                    "education": "Not Specified",
                    "salary_range": item.get('salary', 'Not Specified'),
                    "job_satisfaction": None,
                    "work_life_balance": None,
                    "growth_potential": None,
                    "source": "external"
                }
                combined_dataset.append(processed_item)
                external_count += 1

    # Calculate how many synthetic profiles we need
    synthetic_needed = max(0, min_profiles - len(combined_dataset))

    if synthetic_needed > 0:
        print(f"Generating {synthetic_needed} synthetic profiles to reach minimum threshold...")
        synthetic_profiles = generate_synthetic_career_profiles(synthetic_needed)
        combined_dataset.extend(synthetic_profiles)
        synthetic_count = len(synthetic_profiles)

    # Convert to DataFrame for easier processing
    df = pd.DataFrame(combined_dataset)

    print(f"Final career dataset prepared with {len(df)} profiles:")
    print(f"- {external_count} from external sources")
    print(f"- {synthetic_count} synthetically generated")

    # Save to CSV for future use
    df.to_csv("career_profiles_dataset.csv", index=False)
    print("Dataset saved to 'career_profiles_dataset.csv'")

    return df

# Function to get job requirements for a specific role
def get_job_requirements(role):
    """Get job requirements for a specific role from the dataset"""
    # Try to load existing dataset first
    try:
        df = pd.read_csv("career_profiles_dataset.csv")
        role_data = df[df['role'] == role]

        if len(role_data) > 0:
            # Aggregate skills from all matching profiles
            all_skills = []
            for skills in role_data['skills']:
                if isinstance(skills, str):
                    # Handle skills stored as string representations of lists
                    try:
                        # Try parsing as JSON first
                        skills_list = json.loads(skills.replace("'", "\""))
                        all_skills.extend(skills_list)
                    except:
                        # Fallback parsing
                        skills_parts = skills.strip('[]').split(',')
                        all_skills.extend([s.strip().strip("'\"") for s in skills_parts])

            # Get most common skills
            if all_skills:
                skill_counts = Counter(all_skills)
                common_skills = [skill for skill, count in skill_counts.most_common(10)]
            else:
                # Fallback to predefined skills
                common_skills = role_skills_mapping.get(role, ["Communication", "Problem Solving"])

            # Get average salary range and experience
            try:
                avg_experience = role_data['years_experience'].mean()
            except:
                avg_experience = 3  # Default

            return {
                "role": role,
                "skills": common_skills,
                "avg_experience": avg_experience
            }

    except (FileNotFoundError, KeyError) as e:
        print(f"Error loading career dataset: {e}")

    # Fallback to predefined mappings if dataset approach fails
    if role in role_skills_mapping:
        return {
            "role": role,
            "skills": role_skills_mapping[role][:10],
            "avg_experience": 3  # Default value
        }
    else:
        return {
            "role": role,
            "skills": ["Communication", "Problem Solving", "Teamwork"],
            "avg_experience": 3
        }

# Initialize the dataset if needed
if not os.path.exists("career_profiles_dataset.csv"):
    print("No existing career profiles dataset found. Generating initial dataset...")
    career_dataset = prepare_career_dataset(use_external=True, min_profiles=500)
else:
    print("Existing career profiles dataset found.")
    try:
        career_dataset = pd.read_csv("career_profiles_dataset.csv")
        print(f"Loaded {len(career_dataset)} career profiles from existing dataset")
    except Exception as e:
        print(f"Error loading existing dataset: {e}")
        print("Generating new dataset...")
        career_dataset = prepare_career_dataset(use_external=True, min_profiles=500)

# Sample code to demonstrate usage
sample_role = "Data Scientist"
requirements = get_job_requirements(sample_role)
print(f"\nSample requirements for {sample_role}:")
print(f"- Top skills: {', '.join(requirements['skills'][:5])}")
print(f"- Avg. experience: {requirements['avg_experience']:.1f} years")

print("\nSection 6a complete: Career data generation and retrieval finished.\n")

# Force regeneration of career dataset with 15,000 profiles
# First remove the existing dataset if it exists
if os.path.exists("career_profiles_dataset.csv"):
    print("Removing existing dataset file...")
    os.remove("career_profiles_dataset.csv")

# Generate a new, much larger dataset
print("Generating a new dataset with 15,000 profiles...")
career_dataset = prepare_career_dataset(use_external=True, min_profiles=15000)

# Verify the size of the new dataset
print(f"New dataset contains {len(career_dataset)} career profiles")

# This will automatically be used by the model training code
print("Ready for model training with the expanded dataset!")

# Advanced Career Data Generation and Enrichment

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')

class CareerDataEnhancer:
    def __init__(self):
        """
        Initialize advanced career data generation tools
        """
        # Load NLP models
        try:
            self.nlp = spacy.load('en_core_web_sm')
        except OSError:
            print("Downloading spaCy English model...")
            !python -m spacy download en_core_web_sm
            self.nlp = spacy.load('en_core_web_sm')

        # Initialize skill extraction pipeline
        self.skill_extractor = pipeline('ner', model='dslim/bert-base-NER')

        # Stopwords for cleaning
        self.stop_words = set(stopwords.words('english'))

        # Advanced skill mapping
        self.skill_synonyms = {
            "Python": ["python programming", "python dev", "python developer"],
            "Machine Learning": ["ml", "machine learning engineer", "ai"],
            "Data Science": ["data scientist", "data analysis", "data analytics"]
        }

    def extract_advanced_skills(self, text, role):
        """
        Advanced skill extraction using multiple techniques
        """
        # Predefined role skills
        base_skills = role_skills_mapping.get(role, [])

        # NER-based skill extraction
        ner_skills = self._extract_skills_with_ner(text)

        # TF-IDF based skill extraction
        tfidf_skills = self._extract_skills_with_tfidf(text)

        # Combine and deduplicate skills
        combined_skills = list(set(
            base_skills +
            ner_skills +
            tfidf_skills
        ))

        return combined_skills[:10]

    def _extract_skills_with_ner(self, text):
        """
        Use BERT NER for skill extraction
        """
        try:
            ner_results = self.skill_extractor(text)
            skills = [
                entity['word']
                for entity in ner_results
                if entity['entity'] in ['B-SKILL', 'I-SKILL']
            ]
            return skills
        except Exception as e:
            print(f"NER Skill Extraction Error: {e}")
            return []

    def _extract_skills_with_tfidf(self, text, top_n=5):
        """
        Use TF-IDF to extract potential skills
        """
        # Preprocess text
        doc = self.nlp(text.lower())
        cleaned_text = ' '.join([
            token.lemma_
            for token in doc
            if token.text not in self.stop_words
            and token.is_alpha
        ])

        # Create TF-IDF vectorizer
        vectorizer = TfidfVectorizer(
            stop_words='english',
            ngram_range=(1, 2),
            max_features=100
        )

        # Fit and extract top skills
        tfidf_matrix = vectorizer.fit_transform([cleaned_text])
        feature_names = vectorizer.get_feature_names_out()

        # Get top skills by TF-IDF score
        top_indices = tfidf_matrix.toarray()[0].argsort()[-top_n:][::-1]
        return [feature_names[i] for i in top_indices]

    def generate_comprehensive_profile(self, role):
        """
        Generate a comprehensive career profile with advanced enrichment
        """
        # Base synthetic profile generation
        base_profile = generate_synthetic_career_profile(role)

        # Advanced skill enrichment
        if 'description' in base_profile:
            advanced_skills = self.extract_advanced_skills(
                base_profile['description'],
                role
            )
            base_profile['extracted_skills'] = advanced_skills

        # Career trajectory prediction
        base_profile['potential_career_paths'] = self._predict_career_paths(role)

        # Salary trend analysis
        base_profile['salary_trend'] = self._analyze_salary_trend(
            base_profile['salary_range'],
            base_profile['years_experience']
        )

        return base_profile

    def _predict_career_paths(self, role):
        """
        Predict potential career progression paths
        """
        career_progression = {
            "AI Engineer": [
                "Senior AI Engineer",
                "AI Research Scientist",
                "Chief AI Officer",
                "AI Product Manager"
            ],
            "Data Scientist": [
                "Senior Data Scientist",
                "Data Science Manager",
                "Chief Data Officer",
                "AI/ML Specialist"
            ],
            # Add more roles and progression paths
            "default": [
                "Senior " + role,
                "Lead " + role,
                "Principal " + role
            ]
        }

        return career_progression.get(role, career_progression['default'])

    def _analyze_salary_trend(self, salary_range, years_experience):
        """
        Predict salary trends based on experience
        """
        try:
            # Extract numeric salary values
            min_salary, max_salary = map(
                lambda x: int(x.replace('$', '').replace(',', '')),
                salary_range.split(' - ')
            )

            # Simple linear projection
            growth_rate = 0.05 * years_experience
            projected_min = int(min_salary * (1 + growth_rate))
            projected_max = int(max_salary * (1 + growth_rate))

            return {
                'current_range': salary_range,
                'projected_range': f'${projected_min} - ${projected_max}',
                'annual_growth_rate': f'{growth_rate * 100:.2f}%'
            }
        except Exception as e:
            print(f"Salary trend analysis error: {e}")
            return None

def enhanced_career_dataset_generation(num_profiles=15000):
    """
    Generate an enhanced career dataset with advanced techniques
    """
    enhancer = CareerDataEnhancer()
    enhanced_profiles = []

    print("Generating enhanced career profiles...")
    for role in tqdm(roles):
        # Generate multiple profiles for each role
        role_profiles = [
            enhancer.generate_comprehensive_profile(role)
            for _ in range(num_profiles // len(roles))
        ]
        enhanced_profiles.extend(role_profiles)

    # Convert to DataFrame
    df = pd.DataFrame(enhanced_profiles)

    # Save enhanced dataset
    df.to_csv('enhanced_career_profiles.csv', index=False)

    return df

# Execute enhanced dataset generation
try:
    enhanced_career_dataset = enhanced_career_dataset_generation()
    print(f"Generated {len(enhanced_career_dataset)} enhanced career profiles")
except Exception as e:
    print(f"Enhanced dataset generation failed: {e}")

"""#  Section 11: Career Assessment System"""

# Install required packages
!pip install -q scikit-learn joblib matplotlib seaborn xgboost scipy
# Install missing packages (most are already pre-installed in Colab)
!pip install -q scikit-learn joblib matplotlib seaborn xgboost scipy threadpoolctl

# Try to install threadpool_limits but don't fail if not available
!pip install -q threadpool_limits || echo "threadpool_limits not available, will use fallback"

# ============================
# SECTION 6b: CAREER INTELLIGENCE MODEL TRAINING
# ============================

# Install required packages
!pip install -q scikit-learn joblib matplotlib seaborn xgboost scipy tensorflow threadpool-limits

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, PowerTransformer, PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import ExtraTreesRegressor, StackingRegressor, VotingRegressor, ExtraTreesClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor, RANSACRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report, confusion_matrix
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.neural_network import MLPRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import warnings
from tqdm.notebook import tqdm
from datetime import datetime
from collections import Counter
from scipy import stats
import json
import re
import xgboost as xgb
import time
import itertools

# Try importing TensorFlow
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping
    TF_AVAILABLE = True
    print("TensorFlow is available for neural network models")

    # Prevent TensorFlow from consuming all GPU memory
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(f"Error setting GPU memory growth: {e}")

except ImportError:
    TF_AVAILABLE = False
    print("TensorFlow not available, will skip neural network models")

# Try importing threadpool_limits
try:
    from threadpool_limits import threadpool_limits
    THREADPOOL_AVAILABLE = True
except ImportError:
    THREADPOOL_AVAILABLE = False
    print("threadpool_limits not available, will proceed without thread limiting")
    # Create a dummy context manager
    class DummyContextManager:
        def __init__(self, *args, **kwargs):
            pass
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
    threadpool_limits = DummyContextManager

from sklearn.utils import parallel_backend

# Create models directory if it doesn't exist
os.makedirs('models', exist_ok=True)

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

print("Starting Advanced Career Intelligence Model Training...")

def load_career_dataset():
    """Load career dataset from CSV or generate if not available"""
    try:
        # Try to load existing dataset
        if os.path.exists('career_dataset.csv'):
            print("Loading existing career dataset...")
            df = pd.read_csv('career_dataset.csv')
            print(f"Loaded career dataset with {len(df)} records")
            return df
        else:
            print("Career dataset not found, generating synthetic data...")
            # Generate synthetic dataset
            df = generate_career_dataset()
            # Save for future use
            df.to_csv('career_dataset.csv', index=False)
            print(f"Generated and saved career dataset with {len(df)} records")
            return df
    except Exception as e:
        print(f"Error loading career dataset: {e}")
        return None

def generate_career_dataset(n_samples=15000):
    """Generate a synthetic career dataset for model development"""
    np.random.seed(42)

    # Define possible values for categorical variables
    roles = [
        'Software Engineer', 'Data Scientist', 'Product Manager', 'UX Designer',
        'Marketing Manager', 'Sales Representative', 'HR Specialist', 'Financial Analyst',
        'Project Manager', 'Business Analyst', 'DevOps Engineer', 'Content Writer',
        'Graphic Designer', 'Customer Support', 'Operations Manager', 'Research Scientist',
        'Network Administrator', 'Quality Assurance', 'Legal Counsel', 'Executive Assistant'
    ]

    sectors = [
        'Technology', 'Healthcare', 'Finance', 'Education', 'Retail',
        'Manufacturing', 'Media', 'Government', 'Energy', 'Consulting'
    ]

    experience_levels = ['Entry', 'Junior', 'Mid-level', 'Senior', 'Executive']

    education_levels = [
        'High School', 'Associate Degree', 'Bachelor\'s Degree',
        'Master\'s Degree', 'PhD', 'Professional Certification'
    ]

    all_skills = [
        'Python', 'Java', 'JavaScript', 'SQL', 'Excel', 'Communication', 'Project Management',
        'Data Analysis', 'Marketing', 'Sales', 'Leadership', 'Problem Solving', 'Research',
        'Customer Service', 'Teamwork', 'Design', 'Writing', 'Public Speaking', 'Negotiation',
        'Financial Analysis', 'Machine Learning', 'Statistical Analysis', 'UX Design', 'HTML/CSS',
        'Product Development', 'Strategic Planning', 'Social Media', 'Cloud Computing', 'Networking',
        'DevOps', 'Agile Methodologies', 'Quality Assurance', 'Mobile Development', 'SEO',
        'Data Visualization', 'Business Intelligence', 'Risk Management', 'Cybersecurity',
        'Content Creation', 'CRM Systems', 'Digital Marketing', 'UI Design', 'APIs',
        'Budget Management', 'Database Management', 'Accounting', 'Supply Chain', 'HR Management'
    ]

    # Generate random data
    data = {
        'role': np.random.choice(roles, size=n_samples),
        'sector': np.random.choice(sectors, size=n_samples),
        'experience_level': np.random.choice(experience_levels, size=n_samples),
        'years_experience': np.random.gamma(shape=2, scale=2, size=n_samples) + 1,  # 1-20 years
        'education': np.random.choice(education_levels, size=n_samples)
    }

    # Generate skills (2-7 skills per person)
    skills = []
    for _ in range(n_samples):
        n_skills = np.random.randint(2, 8)
        person_skills = np.random.choice(all_skills, size=n_skills, replace=False).tolist()
        skills.append(person_skills)
    data['skills'] = skills

    # Generate salary ranges based on experience and education
    salary_min = []
    salary_max = []

    for i in range(n_samples):
        base = 50000
        exp_level = data['experience_level'][i]
        edu = data['education'][i]
        years = data['years_experience'][i]

        # Experience level multiplier
        if exp_level == 'Entry':
            exp_mult = 1.0
        elif exp_level == 'Junior':
            exp_mult = 1.3
        elif exp_level == 'Mid-level':
            exp_mult = 1.7
        elif exp_level == 'Senior':
            exp_mult = 2.2
        else:  # Executive
            exp_mult = 3.0

        # Education multiplier
        if edu == 'High School':
            edu_mult = 0.9
        elif edu == 'Associate Degree':
            edu_mult = 1.0
        elif edu == 'Bachelor\'s Degree':
            edu_mult = 1.2
        elif edu == 'Master\'s Degree':
            edu_mult = 1.4
        elif edu == 'PhD':
            edu_mult = 1.6
        else:  # Professional Certification
            edu_mult = 1.1

        # Years experience adds up to 4% per year
        years_mult = 1 + min(years * 0.04, 0.8)  # Cap at 80% increase

        # Calculate salary with a random factor for variability
        salary_base = base * exp_mult * edu_mult * years_mult
        random_factor = np.random.uniform(0.85, 1.15)
        salary_base *= random_factor

        # Create a range around the base
        min_sal = int(salary_base * 0.9 / 1000) * 1000
        max_sal = int(salary_base * 1.1 / 1000) * 1000

        salary_min.append(min_sal)
        salary_max.append(max_sal)

    # Format as a range
    data['salary_range'] = [f"${min:,} - ${max:,}" for min, max in zip(salary_min, salary_max)]

    # Generate satisfaction metrics (1-10 scale)
    # We'll build more complex relationships to make prediction interesting

    base_satisfaction = np.random.normal(7, 1.5, n_samples)
    base_satisfaction = np.clip(base_satisfaction, 1, 10)

    # Introduce factors that influence satisfaction
    satisfaction_factors = {}

    # Role satisfaction factors (some roles tend to have higher satisfaction)
    role_factors = {role: np.random.uniform(-1, 1) for role in roles}
    satisfaction_factors['role'] = [role_factors[role] for role in data['role']]

    # Experience level factors (U-shaped: high at entry, dips in middle, high at senior)
    exp_factors = {
        'Entry': 0.5,
        'Junior': 0,
        'Mid-level': -0.5,
        'Senior': 0.3,
        'Executive': 0.7
    }
    satisfaction_factors['exp'] = [exp_factors[exp] for exp in data['experience_level']]

    # Salary factors (higher salary tends to increase satisfaction)
    salary_factors = [(min_sal / 50000) * 0.5 for min_sal in salary_min]
    satisfaction_factors['salary'] = salary_factors

    # Random factors specific to each person
    personal_factors = np.random.normal(0, 1, n_samples)
    satisfaction_factors['personal'] = personal_factors

    # Combine factors with different weights
    job_satisfaction = base_satisfaction.copy()
    for factor, values in satisfaction_factors.items():
        if factor == 'personal':
            job_satisfaction += np.array(values) * 0.8
        else:
            job_satisfaction += np.array(values) * 0.5

    # Work-life balance (correlated with job satisfaction but with differences)
    work_life_balance = job_satisfaction * 0.7 + np.random.normal(3, 1, n_samples)

    # Growth potential (depends more on company and role than individual satisfaction)
    growth_potential = base_satisfaction * 0.5 + np.random.normal(5, 1.5, n_samples)

    # Clip all metrics to valid range (1-10)
    data['job_satisfaction'] = np.clip(job_satisfaction, 1, 10)
    data['work_life_balance'] = np.clip(work_life_balance, 1, 10)
    data['growth_potential'] = np.clip(growth_potential, 1, 10)

    # Generate job descriptions
    descriptions = []

    # Templates for descriptions
    desc_templates = [
        "Looking for a {role} to join our {sector} team. Requires {skills} and {education}.",
        "{role} position available at a leading {sector} company. Must have {skills}.",
        "Join our {sector} organization as a {role}. Ideal candidates have {skills}.",
        "Experienced {role} needed for {sector} projects. Background in {skills} preferred.",
        "{role} opportunity in {sector}. Responsibilities include using {skills} to solve problems."
    ]

    for i in range(n_samples):
        template = np.random.choice(desc_templates)
        skills_text = ", ".join(np.random.choice(data['skills'][i], min(3, len(data['skills'][i])), replace=False))
        desc = template.format(
            role=data['role'][i],
            sector=data['sector'][i],
            skills=skills_text,
            education=data['education'][i]
        )
        descriptions.append(desc)

    data['description'] = descriptions

    # Add a source column (simulating where the data came from)
    sources = ['LinkedIn', 'Indeed', 'Company Website', 'Referral', 'Job Fair', 'Recruitment Agency']
    data['source'] = np.random.choice(sources, size=n_samples)

    # Convert to DataFrame
    df = pd.DataFrame(data)

    # Round numerical columns for better readability
    df['years_experience'] = df['years_experience'].round(1)
    df['job_satisfaction'] = df['job_satisfaction'].round(1)
    df['work_life_balance'] = df['work_life_balance'].round(1)
    df['growth_potential'] = df['growth_potential'].round(1)

    return df

def check_outliers_and_skewness(df, columns):
    """Check for outliers and skewness in specified columns"""
    results = {}

    for col in columns:
        if col not in df.columns:
            results[col] = {'outliers_count': 0, 'outliers_percentage': 0, 'is_skewed': False}
            continue

        # Get column values without NaNs
        values = df[col].dropna()

        # Calculate IQR and bounds
        q1 = values.quantile(0.25)
        q3 = values.quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        # Count outliers
        outliers = values[(values < lower_bound) | (values > upper_bound)]
        outliers_count = len(outliers)
        outliers_percentage = (outliers_count / len(values)) * 100 if len(values) > 0 else 0

        # Calculate skewness
        skewness = values.skew()
        is_skewed = abs(skewness) > 0.5  # Threshold for skewness

        results[col] = {
            'outliers_count': outliers_count,
            'outliers_percentage': outliers_percentage,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'skewness': skewness,
            'is_skewed': is_skewed
        }

    return results

def preprocess_career_data_advanced(df, use_full_dataset=True):
    """
    Advanced preprocessing for career dataset
    """
    # Select target columns
    target_columns = ['job_satisfaction', 'work_life_balance', 'growth_potential']

    # Limit dataset if not using full dataset
    if not use_full_dataset:
        df = df.sample(frac=0.5, random_state=42)

    print(f"Using {'full' if use_full_dataset else 'partial'} dataset with {len(df)} records")

    # Analyze target variables
    print("\nAnalyzing target variables for outliers and skewness:")
    target_analysis = check_outliers_and_skewness(df, target_columns)

    # Add very small noise to targets to prevent perfect correlations
    for col in target_columns:
        if col in df.columns:
            std = df[col].std()
            if std > 0:
                noise_level = std * 0.01  # 1% of standard deviation - very minimal noise
                df[col] = df[col] + np.random.normal(0, noise_level, size=len(df))
                # Ensure values stay in valid range (1-10)
                df[col] = df[col].clip(1, 10)

    # Create feature matrix and target matrix
    y_data = df[target_columns].copy()

    # Define columns to exclude from features
    exclude_cols = target_columns + ['source', 'description']

    # Define columns by type
    categorical_cols = ['role', 'sector', 'experience_level', 'education']

    # Handle skills column
    if 'skills' in df.columns:
        # Process skills column
        df['skills'] = df['skills'].apply(lambda x:
            eval(x) if isinstance(x, str) and x.startswith('[')
            else [x] if isinstance(x, str)
            else x if isinstance(x, list)
            else []
        )

        # Extract top skills
        all_skills = []
        for skills_list in df['skills']:
            if isinstance(skills_list, list):
                all_skills.extend(skills_list)

        # Get top skills
        skill_counts = pd.Series(all_skills).value_counts()
        top_skills = skill_counts.head(40).index.tolist()

        # Create skill features
        for skill in top_skills:
            safe_name = skill.replace(" ", "_").replace("-", "_")
            df[f'has_skill_{safe_name}'] = df['skills'].apply(
                lambda x: 1 if isinstance(x, list) and skill in x else 0
            )

        # Create skill count feature
        df['skill_count'] = df['skills'].apply(
            lambda x: len(x) if isinstance(x, list) else 0
        )

        # Add to exclude list
        exclude_cols.append('skills')

    # Get numeric columns (excluding targets and other non-features)
    all_cols = list(df.columns)
    feature_cols = [col for col in all_cols if col not in exclude_cols]
    numeric_cols = df[feature_cols].select_dtypes(include=['float64', 'int64']).columns.tolist()

    # Set up preprocessing for both numerical and categorical features
    # Handle missing values and scaling for numeric features
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    # Handle missing values and encoding for categorical features
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Updated parameter
    ])

    # Only use categorical columns that exist in the data
    cat_cols = [col for col in categorical_cols if col in df.columns]

    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, cat_cols)
        ],
        remainder='drop'  # Drop any columns not specified
    )

    # Transform target variables if needed
    y_transformed = y_data.copy()
    transformers = {}

    for col in target_columns:
        # Check if transformation is needed based on skewness
        if target_analysis.get(col, {}).get('is_skewed', False):
            print(f"  Applying power transformation to {col} due to skewness")
            transformer = PowerTransformer(method='yeo-johnson')
            y_transformed[col] = transformer.fit_transform(y_data[col].values.reshape(-1, 1)).flatten()
            transformers[col] = transformer

    # Apply preprocessing to create feature matrix
    try:
        X_processed = preprocessor.fit_transform(df)

        # Get feature names
        numeric_features = numeric_cols

        # Get categorical feature names after one-hot encoding
        if cat_cols:
            cat_features = []
            for i, col in enumerate(cat_cols):
                categories = preprocessor.named_transformers_['cat'].named_steps['onehot'].categories_[i]
                cat_features.extend([f"{col}_{cat}" for cat in categories])
        else:
            cat_features = []

        # Create DataFrame with feature names
        feature_names = numeric_features + cat_features
        if isinstance(X_processed, np.ndarray):
            # Standard numpy array
            X_processed_df = pd.DataFrame(
                X_processed,
                columns=feature_names[:X_processed.shape[1]]
            )
        else:
            # Handle sparse matrix if returned
            X_processed_df = pd.DataFrame(
                X_processed.toarray(),
                columns=feature_names[:X_processed.shape[1]]
            )

        print(f"Preprocessed data shape: {X_processed_df.shape}")

        # Store preprocessing metadata
        preprocessor_metadata = {
            'target_transformers': transformers,
            'feature_columns': feature_cols,
            'numeric_columns': numeric_cols,
            'categorical_columns': cat_cols
        }

        return X_processed_df, y_transformed, preprocessor, preprocessor_metadata

    except Exception as e:
        print(f"Error in preprocessing pipeline: {e}")
        return None, None, None, None

def train_advanced_satisfaction_models(X, y, preprocessor, preprocessor_metadata):
    """Train advanced satisfaction prediction models with ensemble methods"""
    if X is None or y is None or len(X) == 0 or len(y) == 0:
        print("No data to train satisfaction models")
        return {}, {}

    # Make sure X and y have the same number of samples
    if len(X) != len(y):
        print(f"Warning: X has {len(X)} samples but y has {len(y)} samples. Aligning data...")
        common_index = X.index.intersection(y.index)
        X = X.loc[common_index]
        y = y.loc[common_index]

    print(f"Training advanced satisfaction prediction models with {len(X)} samples")

    # Define comprehensive model selection
    base_models = {
        'RandomForest': RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_leaf=5,
            n_jobs=-1,
            random_state=42
        ),
        'ExtraTrees': ExtraTreesRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_leaf=5,
            n_jobs=-1,
            random_state=42
        ),
        'GradientBoosting': GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=5,
            random_state=42
        ),
        'XGBoost': xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=100,
            max_depth=6,
            learning_rate=0.05,
            random_state=42
        ),
        'LinearRegression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=0.01),
        'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5),
        'HuberRegressor': HuberRegressor(epsilon=1.35),
        'SVR': SVR(kernel='rbf', C=1.0, epsilon=0.1)
    }

    # Add neural network if TensorFlow is available
    if TF_AVAILABLE:
        base_models['NeuralNetwork'] = None  # Will be created per target

    # Parameter grids for tuning
    param_grids = {
        'RandomForest': {
            'n_estimators': [50, 100, 200],
            'max_depth': [6, 10, 15],
            'min_samples_leaf': [1, 5, 10]
        },
        'ExtraTrees': {
            'n_estimators': [50, 100, 200],
            'max_depth': [6, 10, 15],
            'min_samples_leaf': [1, 5, 10]
        },
        'GradientBoosting': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.05, 0.1],
            'max_depth': [3, 5, 7]
        },
        'XGBoost': {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.01, 0.05, 0.1]
        },
        'Ridge': {
            'alpha': [0.01, 0.1, 1.0, 10.0]
        },
        'Lasso': {
            'alpha': [0.001, 0.01, 0.1, 1.0]
        },
        'ElasticNet': {
            'alpha': [0.001, 0.01, 0.1, 1.0],
            'l1_ratio': [0.1, 0.5, 0.9]
        },
        'SVR': {
            'C': [0.1, 1.0, 10.0],
            'epsilon': [0.01, 0.1, 0.2],
            'kernel': ['linear', 'rbf']
        }
    }

    # Results storage
    all_models = {}
    results = {}

    # Get target transformers from metadata
    target_transformers = preprocessor_metadata.get('target_transformers', {})

    # STEP 1: Train models for all targets
    for target_col in y.columns:
        print(f"\nTraining models for {target_col}:")
        target_results = {}
        target_models = {}

        # Get the target values
        y_target = y[target_col]

        # Split data for training and validation
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_target, test_size=0.2, random_state=42)

        # STEP 2: Calculate baseline metrics
        baseline_mean = y_train.mean()
        baseline_mse = mean_squared_error(y_test, [baseline_mean] * len(y_test))
        baseline_rmse = np.sqrt(baseline_mse)
        baseline_mae = mean_absolute_error(y_test, [baseline_mean] * len(y_test))
        baseline_r2 = r2_score(y_test, [baseline_mean] * len(y_test))

        print(f"  Baseline (mean value {baseline_mean:.2f}): "
              f"RMSE={baseline_rmse:.4f}, MAE={baseline_mae:.4f}, R²={baseline_r2:.4f}")

        # STEP 3: Cross-validation for each model
        cv = KFold(n_splits=5, shuffle=True, random_state=42)

        # Try each model type
        for model_name, model in base_models.items():
            try:
                # Neural Network is special case
                if model_name == 'NeuralNetwork':
                    print(f"  Training {model_name}...")

                    # Create NN model
                    def create_nn_model(input_shape):
                        model = Sequential([
                            Dense(64, activation='relu', input_shape=(input_shape,)),
                            Dropout(0.3),
                            Dense(32, activation='relu'),
                            Dropout(0.2),
                            Dense(16, activation='relu'),
                            Dense(1)
                        ])
                        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
                        return model

                    # Create and train the model
                    nn_model = create_nn_model(X_train.shape[1])

                    # Early stopping
                    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

                    # Train with validation data
                    nn_model.fit(
                        X_train.values, y_train.values,
                        epochs=50,
                        batch_size=64,
                        validation_split=0.2,
                        callbacks=[early_stop],
                        verbose=0
                    )

                    # Predict
                    y_pred = nn_model.predict(X_test.values).flatten()

                    # Calculate metrics
                    mse = mean_squared_error(y_test, y_pred)
                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    r2 = r2_score(y_test, y_pred)

                    # Store results
                    target_models[model_name] = nn_model
                    target_results[model_name] = {
                        'mse': mse,
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'test_score': max(0, r2)  # Use max() to handle negative R²
                    }

                                      # Print metrics
                    print(f"    {model_name}: MSE={mse:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}")

                    # Check if negative R² and report
                    if r2 < 0:
                        print(f"    Warning: Negative R² value ({r2:.4f}), will use 0 for model comparison")

                else:
                    # Regular model training with hyperparameter tuning
                    print(f"  Training {model_name}...")

                    if model_name in param_grids:
                        print(f"    Hyperparameter search for {model_name}...")

                        # Use RandomizedSearchCV for faster tuning
                        search = RandomizedSearchCV(
                            model,
                            param_distributions=param_grids[model_name],
                            n_iter=10,
                            cv=3,
                            scoring='neg_mean_squared_error',
                            n_jobs=-1,
                            random_state=42
                        )

                        search.fit(X_train, y_train)
                        best_params = search.best_params_
                        print(f"    Best parameters: {best_params}")

                        # Create model with best parameters
                        if model_name == 'RandomForest':
                            model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)
                        elif model_name == 'ExtraTrees':
                            model = ExtraTreesRegressor(**best_params, random_state=42, n_jobs=-1)
                        elif model_name == 'GradientBoosting':
                            model = GradientBoostingRegressor(**best_params, random_state=42)
                        elif model_name == 'XGBoost':
                            model = xgb.XGBRegressor(**best_params, random_state=42)
                        elif model_name == 'Ridge':
                            model = Ridge(**best_params, random_state=42)
                        elif model_name == 'Lasso':
                            model = Lasso(**best_params, random_state=42)
                        elif model_name == 'ElasticNet':
                            model = ElasticNet(**best_params, random_state=42)
                        elif model_name == 'SVR':
                            model = SVR(**best_params)

                    # Train the model
                    model.fit(X_train, y_train)

                    # Make predictions
                    y_pred = model.predict(X_test)

                    # Calculate metrics
                    mse = mean_squared_error(y_test, y_pred)
                    mae = mean_absolute_error(y_test, y_pred)
                    rmse = np.sqrt(mse)
                    r2 = r2_score(y_test, y_pred)

                    # Store the model
                    target_models[model_name] = model

                    # Store results with adjusted R² for model comparison
                    target_results[model_name] = {
                        'mse': mse,
                        'rmse': rmse,
                        'mae': mae,
                        'r2': r2,
                        'test_score': max(0, r2)  # Use max() to handle negative R²
                    }

                    # Report metrics with clear indication if R² is negative
                    print(f"    {model_name}: MSE={mse:.4f}, RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}"
                         + (f" (negative, treated as 0 for model comparison)" if r2 < 0 else ""))

                    # Save the individual model
                    joblib.dump(model, f'models/{target_col}_{model_name}.joblib')

            except Exception as e:
                print(f"    Error training {model_name}: {str(e)}")
                target_results[model_name] = {
                    'error': str(e)
                }

        # STEP 4: Feature importance analysis for best model (if available)
        valid_models = [m for m in target_results.keys()
                        if 'test_score' in target_results[m] and target_results[m]['test_score'] > 0]

        if valid_models:
            # Get best model based on test score
            best_model_name = max(valid_models, key=lambda m: target_results[m]['test_score'])
            best_model = target_models[best_model_name]

            # Try to extract feature importance if the model supports it
            try:
                if hasattr(best_model, 'feature_importances_'):
                    # Get feature importances
                    importances = best_model.feature_importances_

                    # Create feature importance DataFrame
                    feature_imp = pd.DataFrame({
                        'Feature': X.columns,
                        'Importance': importances
                    }).sort_values('Importance', ascending=False)

                    # Print top 10 features
                    print("\n  Top 10 important features:")
                    for i, (feature, importance) in enumerate(
                        zip(feature_imp['Feature'].head(10), feature_imp['Importance'].head(10))):
                        print(f"    {i+1}. {feature}: {importance:.4f}")

                    # Save feature importance to file
                    feature_imp.to_csv(f'models/{target_col}_feature_importance.csv', index=False)
            except Exception as e:
                print(f"  Error extracting feature importance: {str(e)}")

        # STEP 5: Create ensemble model for better performance
        print("\n  Creating ensemble model...")
        try:
            # Get models with positive R² for ensemble
            positive_r2_models = [m for m in target_results.keys()
                                 if 'r2' in target_results[m] and target_results[m]['r2'] > 0]

            if len(positive_r2_models) >= 2:
                # Use top 3 models (or all if fewer than 3)
                top_models = sorted(positive_r2_models,
                                   key=lambda m: target_results[m]['test_score'],
                                   reverse=True)[:min(3, len(positive_r2_models))]

                print(f"  Using top {len(top_models)} models: {', '.join(top_models)}")

                # Create weighted ensemble based on R² scores
                weights = {m: target_results[m]['test_score'] for m in top_models}
                weight_sum = sum(weights.values())
                if weight_sum > 0:
                    weights = {m: w / weight_sum for m, w in weights.items()}

                    # Predict with ensemble
                    ensemble_pred = np.zeros_like(y_test, dtype=float)
                    for m in top_models:
                        model_pred = target_models[m].predict(X_test)
                        ensemble_pred += weights[m] * model_pred

                    # Calculate ensemble metrics
                    ensemble_mse = mean_squared_error(y_test, ensemble_pred)
                    ensemble_mae = mean_absolute_error(y_test, ensemble_pred)
                    ensemble_rmse = np.sqrt(ensemble_mse)
                    ensemble_r2 = r2_score(y_test, ensemble_pred)

                    # Try blending with baseline for better performance if R² is still negative
                    best_blend_r2 = ensemble_r2
                    best_blend_ratio = 0

                    if ensemble_r2 < 0:
                        print("  Ensemble R² is negative, trying baseline blending to improve...")

                        # Test different blending ratios
                        for blend_ratio in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
                            # Blend predictions with baseline
                            blended_pred = (1 - blend_ratio) * ensemble_pred + blend_ratio * baseline_mean

                            # Calculate metrics
                            blend_r2 = r2_score(y_test, blended_pred)

                            # Update best if improved
                            if blend_r2 > best_blend_r2:
                                best_blend_r2 = blend_r2
                                best_blend_ratio = blend_ratio

                        if best_blend_ratio > 0:
                            print(f"  Found optimal blend ratio: {best_blend_ratio:.2f}, new R²={best_blend_r2:.4f}")

                            # Recalculate metrics with best blend
                            blended_pred = (1 - best_blend_ratio) * ensemble_pred + best_blend_ratio * baseline_mean
                            ensemble_mse = mean_squared_error(y_test, blended_pred)
                            ensemble_mae = mean_absolute_error(y_test, blended_pred)
                            ensemble_rmse = np.sqrt(ensemble_mse)
                            ensemble_r2 = best_blend_r2

                    # Create custom ensemble model class that can be saved
                    class EnsembleModel:
                        def __init__(self, models, weights, baseline_mean=None, blend_ratio=0):
                            self.models = models
                            self.weights = weights
                            self.baseline_mean = baseline_mean
                            self.blend_ratio = blend_ratio

                        def predict(self, X):
                            ensemble_pred = np.zeros(X.shape[0], dtype=float)
                            for model_name, model in self.models.items():
                                if model_name in self.weights:
                                    model_pred = model.predict(X)
                                    ensemble_pred += self.weights[model_name] * model_pred

                            # Apply blending if needed
                            if self.blend_ratio > 0 and self.baseline_mean is not None:
                                ensemble_pred = (1-self.blend_ratio) * ensemble_pred + self.blend_ratio * self.baseline_mean

                            return ensemble_pred

                    # Create ensemble model instance
                    ensemble_models = {m: target_models[m] for m in top_models}
                    ensemble_instance = EnsembleModel(
                        models=ensemble_models,
                        weights=weights,
                        baseline_mean=baseline_mean,
                        blend_ratio=best_blend_ratio
                    )

                    # Save ensemble model
                    joblib.dump(ensemble_instance, f'models/{target_col}_Ensemble.joblib')

                    # Add ensemble to results
                    model_name = "Ensemble"
                    target_models[model_name] = ensemble_instance
                    target_results[model_name] = {
                        'mse': ensemble_mse,
                        'rmse': ensemble_rmse,
                        'mae': ensemble_mae,
                        'r2': ensemble_r2,
                        'test_score': max(0, ensemble_r2)
                    }

                    print(f"  Ensemble: MSE={ensemble_mse:.4f}, RMSE={ensemble_rmse:.4f}, MAE={ensemble_mae:.4f}, R²={ensemble_r2:.4f}")

                else:
                    print("  Cannot create ensemble: All models have 0 weight")
            else:
                print(f"  Not enough models with positive R² to create ensemble. Found {len(positive_r2_models)}, need at least 2.")

                # If no positive R² models, create a fallback ensemble with baseline blending
                if not positive_r2_models and len(target_models) > 0:
                    print("  Creating fallback model with baseline blending...")

                    # Use first model with best blend ratio
                    fallback_model_name = list(target_models.keys())[0]
                    fallback_model = target_models[fallback_model_name]

                    # Make predictions
                    fallback_pred = fallback_model.predict(X_test)

                    # Find best blend ratio
                    best_r2 = r2_score(y_test, fallback_pred)
                    best_ratio = 0

                    for ratio in [0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]:
                        blended_pred = (1 - ratio) * fallback_pred + ratio * baseline_mean
                        r2 = r2_score(y_test, blended_pred)

                        if r2 > best_r2:
                            best_r2 = r2
                            best_ratio = ratio

                    # If we improved, create and save model
                    if best_ratio > 0:
                        print(f"  Found optimal fallback blend ratio: {best_ratio:.2f}, "
                              f"improved R² from {target_results[fallback_model_name]['r2']:.4f} to {best_r2:.4f}")

                        # Create blended model
                        class BlendedModel:
                            def __init__(self, model, baseline_mean, blend_ratio):
                                self.model = model
                                self.baseline_mean = baseline_mean
                                self.blend_ratio = blend_ratio

                            def predict(self, X):
                                pred = self.model.predict(X)
                                return (1 - self.blend_ratio) * pred + self.blend_ratio * self.baseline_mean

                        blended_instance = BlendedModel(
                            model=fallback_model,
                            baseline_mean=baseline_mean,
                            blend_ratio=best_ratio
                        )

                        # Calculate metrics
                        blended_pred = blended_instance.predict(X_test)
                        blended_mse = mean_squared_error(y_test, blended_pred)
                        blended_mae = mean_absolute_error(y_test, blended_pred)
                        blended_rmse = np.sqrt(blended_mse)
                        blended_r2 = r2_score(y_test, blended_pred)

                        # Save model
                        joblib.dump(blended_instance, f'models/{target_col}_Blended.joblib')

                        # Add to results
                        model_name = "Blended"
                        target_models[model_name] = blended_instance
                        target_results[model_name] = {
                            'mse': blended_mse,
                            'rmse': blended_rmse,
                            'mae': blended_mae,
                            'r2': blended_r2,
                            'test_score': max(0, blended_r2)
                        }

                        print(f"  Blended: MSE={blended_mse:.4f}, RMSE={blended_rmse:.4f}, "
                              f"MAE={blended_mae:.4f}, R²={blended_r2:.4f}")

        except Exception as e:
            print(f"  Error creating ensemble: {str(e)}")

        # STEP 6: Store all results and models for this target
        results[target_col] = target_results
        all_models[target_col] = target_models

        # Find the best model for this target
        valid_models = [m for m in target_results.keys()
                       if 'test_score' in target_results[m]]

        if valid_models:
            best_model_name = max(valid_models, key=lambda m: target_results[m]['test_score'])
            best_r2 = target_results[best_model_name]['r2']
            print(f"\n  Best model for {target_col}: {best_model_name} (R²={best_r2:.4f})")

            # Store the transformer with the best model to allow reverse transformation
            if target_col in target_transformers:
                print(f"  Note: This target uses a power transformation. Storing transformer with model.")
                joblib.dump(target_transformers[target_col], f'models/{target_col}_transformer.joblib')
        else:
            print(f"  No valid models found for {target_col}")

    # Return all models and results
    return all_models, results

def train_role_recommendation_model(df):
    """Train an advanced model to recommend career roles based on skills and experience"""
    if df is None or len(df) == 0:
        print("No data to train role recommendation model")
        return None, []

    print("Training role recommendation model...")

    # Use full dataset
    data = df.copy()

    # Check if role column exists
    if 'role' not in data.columns:
        print("Error: 'role' column not found in the dataset")
        return None, []

    # Drop rows with missing role
    data = data.dropna(subset=['role'])

    # Filter to top 20 roles to ensure enough samples per role
    role_counts = data['role'].value_counts()
    top_roles = role_counts.head(20).index.tolist()
    print(f"Including top 20 roles (out of {len(role_counts)} total roles)")
    data = data[data['role'].isin(top_roles)]

    print(f"Training with {len(data)} records for role prediction")

    # Process skills for feature extraction
    top_skills = []
    if 'skills' in data.columns:
        # Convert skills from string representation to list if needed
        if len(data) > 0 and isinstance(data['skills'].iloc[0], str):
            data['skills'] = data['skills'].apply(lambda x:
                                               eval(x) if isinstance(x, str) and x.startswith('[')
                                               else [x] if isinstance(x, str)
                                               else x if isinstance(x, list)
                                               else [])

        # Extract all skills from the dataset
        all_skills = []
        for skills_list in data['skills']:
            if isinstance(skills_list, list):
                all_skills.extend(skills_list)

        # Get top skills for feature creation
        top_skills = pd.Series(all_skills).value_counts().head(40).index.tolist()  # Use more skills

        # Create one-hot encoding for top skills
        for skill in top_skills:
            data[f'skill_{skill.replace(" ", "_")}'] = data['skills'].apply(
                lambda x: 1 if isinstance(x, list) and skill in x else 0
            )

        # Create skill diversity feature (number of skills)
        data['skill_count'] = data['skills'].apply(
            lambda x: len(x) if isinstance(x, list) else 0
        )

    # Engineer features from other columns

    # Experience level as ordinal
    experience_map = {
        'Entry': 1,
        'Junior': 2,
        'Mid-level': 3,
        'Senior': 4,
        'Executive': 5
    }
    if 'experience_level' in data.columns:
        data['experience_ordinal'] = data['experience_level'].map(experience_map).fillna(3)

    # Education level as ordinal
    education_map = {
        'High School': 1,
        'Associate Degree': 2,
        'Bachelor\'s Degree': 3,
        'Master\'s Degree': 4,
        'PhD': 5,
        'Professional Certification': 3
    }
    if 'education' in data.columns:
        data['education_ordinal'] = data['education'].map(education_map).fillna(3)

    # One-hot encode categorical columns
    cat_cols = ['sector']
    if cat_cols:
        data = pd.get_dummies(data, columns=cat_cols, drop_first=False)

    # Prepare feature columns - exclude target and unnecessary columns
    exclude_cols = ['role', 'skills', 'description', 'source', 'experience_level', 'education',
                   'job_satisfaction', 'work_life_balance', 'growth_potential', 'salary_range']
    feature_cols = [col for col in data.columns if col not in exclude_cols]

    # Prepare features and target
    X = data[feature_cols]
    y = data['role']

    # Split data for training
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y)

    # Handle imbalanced classes
    print("Addressing class imbalance...")

    # Encode target
    label_encoder = LabelEncoder()
    label_encoder.fit(y)
    y_train_encoded = label_encoder.transform(y_train)

    # Train advanced classifiers
    print("Training multiple classifiers to find best performer...")

    classifiers = {
        'RandomForest': RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            class_weight='balanced',
            n_jobs=-1,
            random_state=42
        ),
        'ExtraTrees': ExtraTreesClassifier(
            n_estimators=200,
            max_depth=15,
            class_weight='balanced',
            n_jobs=-1,
            random_state=42
        ),
        'XGBoost': xgb.XGBClassifier(
            objective='multi:softprob',
            n_estimators=200,
            max_depth=7,
            learning_rate=0.1,
            random_state=42
        )
    }

    # Train and evaluate each model
    best_accuracy = 0
    best_model = None
    best_model_name = None

    for name, classifier in classifiers.items():
        print(f"  Training {name}...")
        classifier.fit(X_train, y_train_encoded)

        # Predict
        y_pred_encoded = classifier.predict(X_test)
        y_pred = label_encoder.inverse_transform(y_pred_encoded)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)
        print(f"  {name} accuracy: {accuracy:.4f}")

        # Keep track of best model
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_model = classifier
            best_model_name = name

    # Use the best model
    print(f"Best model: {best_model_name} with accuracy {best_accuracy:.4f}")

    # Advanced evaluation of best model
    if best_model is not None:
        # Predict classes
        y_pred_encoded = best_model.predict(X_test)
        y_pred = label_encoder.inverse_transform(y_pred_encoded)

        # Generate classification report
        report = classification_report(y_test, y_pred)
        print("\nClassification Report:")
        print(report)

        # Generate confusion matrix for top 10 roles (to keep it manageable)
        top_10_roles = role_counts.head(10).index.tolist()
        print("\nConfusion Matrix (top 10 roles):")

        # Filter to top roles
        mask_test = np.isin(y_test, top_10_roles)
        mask_pred = np.isin(y_pred, top_10_roles)

        if sum(mask_test) > 0:
            y_test_top = y_test[mask_test]
            y_pred_top = y_pred[mask_test]

            # Only create confusion matrix if we have enough data
            if len(y_test_top) > 0:
                cm = confusion_matrix(y_test_top, y_pred_top, labels=top_10_roles)

                # Print confusion matrix
                print("\nConfusion Matrix:")
                cm_df = pd.DataFrame(cm, index=top_10_roles, columns=top_10_roles)
                print(cm_df)

        # Save the model and label encoder
        model_path = 'models/role_recommendation_model.joblib'
        encoder_path = 'models/role_label_encoder.joblib'
        joblib.dump(best_model, model_path)
        joblib.dump(label_encoder, encoder_path)

        print(f"Saved role recommendation model and encoder")

        return best_model, top_skills
    else:
        print("Error: No model was successfully trained")
        return None, top_skills

def create_career_prediction_system(use_full_dataset=True):
    """Create and train career prediction system with enhanced accuracy"""
    # Load dataset
    career_df = load_career_dataset()

    if career_df is None:
        print("Failed to load career dataset. Unable to train models.")
        return None, None

    # Display dataset information
    print(f"\nDataset information:")
    print(f"- Total records: {len(career_df)}")
    print(f"- Columns: {', '.join(career_df.columns)}")

    # Process data with advanced preprocessing
    X, y, preprocessor, preprocessor_metadata = preprocess_career_data_advanced(
        career_df, use_full_dataset=use_full_dataset
    )

    if X is None or y is None:
        print("Failed to preprocess career data. Unable to train models.")
        return None, None

    # Train advanced satisfaction models
    satisfaction_models, satisfaction_results = train_advanced_satisfaction_models(
        X, y, preprocessor, preprocessor_metadata
    )

    # Train role recommendation model
    role_model, top_skills = train_role_recommendation_model(career_df)

    # Save metadata about models
    model_info = {
        'satisfaction_targets': list(y.columns),
        'top_skills': top_skills[:30] if top_skills else [],
        'training_data_samples': len(career_df),
        'training_date': str(datetime.now()),
        'features_count': X.shape[1],
        'model_performance': {}
    }

    # Add performance metrics for each target
    for target, model_results in satisfaction_results.items():
        model_info['model_performance'][target] = {}
        for model_name, results in model_results.items():
            if 'error' not in results:
                model_info['model_performance'][target][model_name] = {
                    'r2': results.get('r2'),
                    'rmse': results.get('rmse'),
                    'mae': results.get('mae')
                }

    # Save metadata
    with open('models/career_models_metadata.json', 'w') as f:
        json.dump(model_info, f, indent=2)

    print("\nCareer intelligence model system created successfully with enhanced accuracy.")
    return satisfaction_models, role_model

# Run the model with full dataset and advanced techniques
if __name__ == "__main__":
    # Import time for tracking execution
    import time
    start_time = time.time()

    # Run with FULL dataset and advanced techniques
    print("Starting Career Intelligence System training with advanced techniques and full dataset...")
    satisfaction_models, role_model = create_career_prediction_system(use_full_dataset=True)

    if satisfaction_models and role_model is not None:
        print(f"Career Intelligence System successfully trained!")
        print(f"Training completed in {time.time() - start_time:.2f} seconds")
    else:
        print("Error: Career Intelligence System training failed.")



"""# Section 12: Model Training"""

"""# Section 7: Model Training"""

# ==== SECTION 7: TRAINING ML MODELS ====
import pandas as pd
import numpy as np
import os
import joblib
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# Initialize GPU availability flag
use_gpu = False
GPU_AVAILABLE = False
TORCH_GPU_AVAILABLE = False

# Check for GPU availability
try:
    import tensorflow as tf
    print(f"TensorFlow version: {tf.__version__}")
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"GPU devices available: {len(gpus)}")
        print(f"GPU details: {gpus}")
        # Set memory growth to avoid taking all GPU memory
        for gpu in gpus:
            try:
                tf.config.experimental.set_memory_growth(gpu, True)
            except Exception as e:
                print(f"Error setting memory growth: {e}")
        GPU_AVAILABLE = True
        use_gpu = True  # Set the global use_gpu flag
    else:
        print("No GPU devices detected by TensorFlow")
        GPU_AVAILABLE = False

    # Also check if CUDA is available for PyTorch
    try:
        import torch
        TORCH_GPU_AVAILABLE = torch.cuda.is_available()
        if TORCH_GPU_AVAILABLE:
            print(f"PyTorch CUDA available: {TORCH_GPU_AVAILABLE}")
            print(f"CUDA device count: {torch.cuda.device_count()}")
            print(f"CUDA device name: {torch.cuda.get_device_name(0)}")
            use_gpu = True  # Set the global use_gpu flag
        else:
            print("PyTorch CUDA not available")
    except Exception as e:
        print(f"Error checking PyTorch CUDA: {e}")
        TORCH_GPU_AVAILABLE = False

except Exception as e:
    print(f"Error checking TensorFlow GPU: {e}")
    GPU_AVAILABLE = False
    TORCH_GPU_AVAILABLE = False
    use_gpu = False

# Try to import Optuna for hyperparameter optimization
try:
    import optuna
    optuna_available = True
    print("Optuna available for hyperparameter optimization")
except ImportError:
    optuna_available = False
    print("Optuna not available, will use RandomizedSearchCV instead")

# Try to import XGBoost for advanced modeling
try:
    import xgboost as xgb
    xgboost_available = True
    print("XGBoost available for advanced modeling")
except ImportError:
    xgboost_available = False
    print("XGBoost not available, will use RandomForest instead")

# ==== 6.1: TRAINING REGRESSION MODELS ====

def train_regression_models(X_train, X_test, y_train, y_test, target_name, use_gpu=False, verbose=True):
    """
    Train and evaluate multiple regression models for a specific target

    Args:
        X_train: Training features
        X_test: Testing features
        y_train: Training target values
        y_test: Testing target values
        target_name: Name of the target variable
        use_gpu: Whether to use GPU acceleration
        verbose: Whether to print progress info

    Returns:
        Best performing model and its metrics
    """
    models = {}
    results = {}

    if verbose:
        print(f"\nTraining regression models for {target_name}...")
        print(f"GPU acceleration: {'Enabled' if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE) else 'Disabled'}")

    # 1. Random Forest Regressor
    if verbose:
        print("1. Training Random Forest Regressor...")
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train)
    rf_pred = rf.predict(X_test)
    rf_mse = mean_squared_error(y_test, rf_pred)
    rf_mae = mean_absolute_error(y_test, rf_pred)
    rf_r2 = r2_score(y_test, rf_pred)

    models["random_forest"] = rf
    results["random_forest"] = {
        "mse": rf_mse,
        "mae": rf_mae,
        "r2": rf_r2
    }

    if verbose:
        print(f"   MSE: {rf_mse:.4f}, MAE: {rf_mae:.4f}, R²: {rf_r2:.4f}")

    # 2. Gradient Boosting Regressor
    if verbose:
        print("2. Training Gradient Boosting Regressor...")
    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
    gb.fit(X_train, y_train)
    gb_pred = gb.predict(X_test)
    gb_mse = mean_squared_error(y_test, gb_pred)
    gb_mae = mean_absolute_error(y_test, gb_pred)
    gb_r2 = r2_score(y_test, gb_pred)

    models["gradient_boosting"] = gb
    results["gradient_boosting"] = {
        "mse": gb_mse,
        "mae": gb_mae,
        "r2": gb_r2
    }

    if verbose:
        print(f"   MSE: {gb_mse:.4f}, MAE: {gb_mae:.4f}, R²: {gb_r2:.4f}")

    # 3. XGBoost Regressor (if available)
    if xgboost_available:
        if verbose:
            print("3. Training XGBoost Regressor...")

        # Set up XGBoost parameters with GPU support if available
        if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE):
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'tree_method': 'gpu_hist',  # Use GPU acceleration
                'gpu_id': 0,
                'predictor': 'gpu_predictor',
                'learning_rate': 0.1,
                'max_depth': 6,
                'min_child_weight': 1,
                'n_estimators': 100,
                'early_stopping_rounds': 10,
                'verbosity': 1 if verbose else 0
            }
            if verbose:
                print("   Using GPU acceleration for XGBoost")
        else:
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'tree_method': 'hist',  # CPU-based histogram algorithm
                'learning_rate': 0.1,
                'max_depth': 6,
                'min_child_weight': 1,
                'n_estimators': 100,
                'early_stopping_rounds': 10,
                'verbosity': 1 if verbose else 0
            }

        # Create DMatrix for efficient training
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test, label=y_test)

        # Train model
        xgb_model = xgb.train(
            params,
            dtrain,
            num_boost_round=100,
            evals=[(dtest, 'test')],
            early_stopping_rounds=10,
            verbose_eval=verbose
        )

        # Make predictions
        xgb_pred = xgb_model.predict(dtest)
        xgb_mse = mean_squared_error(y_test, xgb_pred)
        xgb_mae = mean_absolute_error(y_test, xgb_pred)
        xgb_r2 = r2_score(y_test, xgb_pred)

        models["xgboost"] = xgb_model
        results["xgboost"] = {
            "mse": xgb_mse,
            "mae": xgb_mae,
            "r2": xgb_r2
        }

        if verbose:
            print(f"   MSE: {xgb_mse:.4f}, MAE: {xgb_mae:.4f}, R²: {xgb_r2:.4f}")

    # Find the best model based on MSE
    best_model_name = min(results, key=lambda x: results[x]['mse'])
    best_model = models[best_model_name]
    best_metrics = results[best_model_name]

    if verbose:
        print(f"\nBest model for {target_name}: {best_model_name}")
        print(f"  MSE: {best_metrics['mse']:.4f}")
        print(f"  MAE: {best_metrics['mae']:.4f}")
        print(f"  R²: {best_metrics['r2']:.4f}")

    return best_model, best_metrics

def optimize_hyperparameters(X_train, y_train, model_type='xgboost', use_gpu=False,
                            is_classification=False, n_trials=50, verbose=True):
    """
    Optimize hyperparameters for a given model type

    Args:
        X_train: Training features
        y_train: Training target values
        model_type: Type of model to optimize ('xgboost', 'random_forest', etc.)
        use_gpu: Whether to use GPU acceleration
        is_classification: Whether this is a classification task
        n_trials: Number of optimization trials
        verbose: Whether to print progress

    Returns:
        Optimized model
    """
    if optuna_available:
        if verbose:
            print(f"Optimizing {model_type} hyperparameters with Optuna...")

        def objective(trial):
            if model_type == 'xgboost' and xgboost_available:
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                    'random_state': 42
                }

                if use_gpu:
                    params['tree_method'] = 'gpu_hist'
                    params['gpu_id'] = 0

                if is_classification:
                    model = xgb.XGBClassifier(**params)
                else:
                    model = xgb.XGBRegressor(**params)

            elif model_type == 'random_forest':
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                    'max_depth': trial.suggest_int('max_depth', 5, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
                    'random_state': 42
                }

                if is_classification:
                    model = RandomForestClassifier(**params)
                else:
                    model = RandomForestRegressor(**params)

            else:
                # Default to RandomForest if model_type is not supported
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                    'max_depth': trial.suggest_int('max_depth', 5, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
                    'random_state': 42
                }

                if is_classification:
                    model = RandomForestClassifier(**params)
                else:
                    model = RandomForestRegressor(**params)

            # Train and evaluate with cross-validation
            from sklearn.model_selection import cross_val_score

            if is_classification:
                score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()
            else:
                # Use negative MSE for regression (Optuna minimizes the objective)
                score = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()

            return score

        # Create and run the study
        if is_classification:
            direction = 'maximize'  # Maximize accuracy for classification
        else:
            direction = 'minimize'  # Minimize MSE for regression

        study = optuna.create_study(direction=direction)
        study.optimize(objective, n_trials=n_trials)

        # Get the best parameters
        best_params = study.best_params

        if verbose:
            print(f"Best parameters: {best_params}")

        # Create and return the best model
        if model_type == 'xgboost' and xgboost_available:
            if use_gpu:
                best_params['tree_method'] = 'gpu_hist'
                best_params['gpu_id'] = 0

            if is_classification:
                best_model = xgb.XGBClassifier(**best_params)
            else:
                best_model = xgb.XGBRegressor(**best_params)

        elif model_type == 'random_forest':
            if is_classification:
                best_model = RandomForestClassifier(**best_params)
            else:
                best_model = RandomForestRegressor(**best_params)

        else:
            if is_classification:
                best_model = RandomForestClassifier(**best_params)
            else:
                best_model = RandomForestRegressor(**best_params)

        # Train the final model
        best_model.fit(X_train, y_train)
        return best_model

    else:
        # Use RandomizedSearchCV if Optuna is not available
        if verbose:
            print(f"Optimizing {model_type} hyperparameters with RandomizedSearchCV...")

        if model_type == 'xgboost' and xgboost_available:
            param_grid = {
                'n_estimators': [50, 100, 200, 300],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7, 9],
                'subsample': [0.6, 0.8, 1.0],
                'colsample_bytree': [0.6, 0.8, 1.0]
            }

            if is_classification:
                model = xgb.XGBClassifier(random_state=42)
            else:
                model = xgb.XGBRegressor(random_state=42)

            if use_gpu:
                model.set_params(tree_method='gpu_hist', gpu_id=0)

        elif model_type == 'random_forest':
            param_grid = {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [5, 10, 15, 20],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }

            if is_classification:
                model = RandomForestClassifier(random_state=42)
            else:
                model = RandomForestRegressor(random_state=42)

        else:
            # Default to RandomForest
            param_grid = {
                'n_estimators': [50, 100, 200, 300],
                'max_depth': [5, 10, 15, 20],
                'min_samples_split': [2, 5, 10]
            }

            if is_classification:
                model = RandomForestClassifier(random_state=42)
            else:
                model = RandomForestRegressor(random_state=42)

        # Create and run the search
        if is_classification:
            scoring = 'accuracy'
        else:
            scoring = 'neg_mean_squared_error'

        search = RandomizedSearchCV(
            model, param_grid, n_iter=10, cv=5,
            scoring=scoring, random_state=42, n_jobs=-1
        )

        search.fit(X_train, y_train)

        if verbose:
            print(f"Best parameters: {search.best_params_}")

        return search.best_estimator_

# ==== 6.2: TRAINING CLASSIFICATION MODELS ====

def train_classifier(X_train, X_test, y_train, y_test, use_gpu=False, verbose=True):
    """
    Train and evaluate a classifier for the Strong/Average/Weak categories

    Args:
        X_train: Training features
        X_test: Testing features
        y_train: Training target values (0: Weak, 1: Average, 2: Strong)
        y_test: Testing target values
        use_gpu: Whether to use GPU acceleration
        verbose: Whether to print progress

    Returns:
        Best performing classifier model
    """
    if verbose:
        print("\nTraining classifier for Strong/Average/Weak categories...")
        print(f"GPU acceleration: {'Enabled' if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE) else 'Disabled'}")

    models = {}
    results = {}

    # 1. Random Forest Classifier
    if verbose:
        print("1. Training Random Forest Classifier...")
    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf_classifier.fit(X_train, y_train)
    rf_preds = rf_classifier.predict(X_test)
    rf_acc = accuracy_score(y_test, rf_preds)

    models["random_forest"] = rf_classifier
    results["random_forest"] = {
        "accuracy": rf_acc
    }

    if verbose:
        print(f"   Accuracy: {rf_acc:.4f}")

    # 2. XGBoost Classifier (if available)
    if xgboost_available:
        if verbose:
            print("2. Training XGBoost Classifier...")

        # Set up XGBoost parameters with GPU support if available
        if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE):
            params = {
                'objective': 'multi:softmax',
                'eval_metric': 'mlogloss',
                'num_class': 3,  # 0: Weak, 1: Average, 2: Strong
                'tree_method': 'gpu_hist',  # Use GPU acceleration
                'gpu_id': 0,
                'predictor': 'gpu_predictor',
                'learning_rate': 0.1,
                'max_depth': 6,
                'min_child_weight': 1,
                'verbosity': 1 if verbose else 0
            }
            if verbose:
                print("   Using GPU acceleration for XGBoost")
        else:
            params = {
                'objective': 'multi:softmax',
                'eval_metric': 'mlogloss',
                'num_class': 3,  # 0: Weak, 1: Average, 2: Strong
                'tree_method': 'hist',  # CPU-based histogram algorithm
                'learning_rate': 0.1,
                'max_depth': 6,
                'min_child_weight': 1,
                'verbosity': 1 if verbose else 0
            }

        # Create DMatrix for efficient training
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test, label=y_test)

        # Train model
        xgb_model = xgb.train(
            params,
            dtrain,
            num_boost_round=100,
            evals=[(dtest, 'test')],
            early_stopping_rounds=10,
            verbose_eval=verbose
        )

        # Make predictions
        xgb_preds = xgb_model.predict(dtest)
        xgb_preds = xgb_preds.astype(int)  # Convert to integers
        xgb_acc = accuracy_score(y_test, xgb_preds)

        models["xgboost"] = xgb_model
        results["xgboost"] = {
            "accuracy": xgb_acc
        }

        if verbose:
            print(f"   Accuracy: {xgb_acc:.4f}")

    # Find the best model based on accuracy
    best_model_name = max(results, key=lambda x: results[x]['accuracy'])
    best_model = models[best_model_name]

    if verbose:
        print(f"\nBest classifier: {best_model_name} (Accuracy: {results[best_model_name]['accuracy']:.4f})")
        print("\nClassification Report:")
        if best_model_name == "xgboost":
            best_preds = best_model.predict(dtest).astype(int)
        else:
            best_preds = best_model.predict(X_test)
        print(classification_report(y_test, best_preds,
                                  target_names=['Weak', 'Average', 'Strong']))

    return best_model

def generate_minimal_sample_data(num_samples=100000):
    """
    Generate a very minimal dataset for emergency fallback purposes.

    Args:
        num_samples: Number of samples to generate

    Returns:
        DataFrame with basic feature and target columns
    """
    print(f"Generating minimal emergency dataset with {num_samples} samples...")

    # Import needed libraries
    import numpy as np
    import pandas as pd

    # Create empty dataframe
    data = pd.DataFrame()

    # Generate features
    for i in range(11):
        feature_name = f'feature_{i}'
        # Random values calibrated to reasonable ranges
        data[feature_name] = np.random.rand(num_samples) * (100 if i in [0, 1, 8] else 10)

    # Generate target variables - ensure good distribution
    categories = np.random.choice([0, 1, 2], size=num_samples, p=[0.25, 0.5, 0.25])

    # Add scores based on categories
    data['technical'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),
                       np.where(categories == 1, np.random.uniform(40, 70, num_samples),
                                np.random.uniform(75, 95, num_samples)))

    data['communication'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),
                           np.where(categories == 1, np.random.uniform(40, 70, num_samples),
                                    np.random.uniform(75, 95, num_samples)))

    data['problem_solving'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),
                              np.where(categories == 1, np.random.uniform(40, 70, num_samples),
                                      np.random.uniform(75, 95, num_samples)))

    data['cultural_fit'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),
                           np.where(categories == 1, np.random.uniform(40, 70, num_samples),
                                   np.random.uniform(75, 95, num_samples)))

    data['overall'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),
                      np.where(categories == 1, np.random.uniform(40, 70, num_samples),
                              np.random.uniform(75, 95, num_samples)))

    data['classifier'] = categories

    print(f"Minimal emergency dataset generated with {len(data)} rows and {len(data.columns)} columns")
    print(f"Category distribution: Weak: {sum(categories == 0)}, Average: {sum(categories == 1)}, Strong: {sum(categories == 2)}")

    return data

def train_evaluation_models(data=None, retrain=False, verbose=True, use_gpu=True):
    """
    Train and save regression models for interview evaluation.

    Args:
        data: DataFrame with features and targets
        retrain: Whether to retrain models even if they exist
        verbose: Whether to print progress information
        use_gpu: Whether to use GPU acceleration if available

    Returns:
        Dictionary with trained models and scaler
    """
    if verbose:
        print("\n==== INTERVIEW EVALUATION MODEL TRAINING ====")

    # Check if models already exist and we're not forced to retrain
    if os.path.exists("interview_evaluation_models.pkl") and not retrain:
        if verbose:
            print("Interview evaluation models already exist.")
            print("Use retrain=True to force retraining.")
        try:
            model_package = joblib.load("interview_evaluation_models.pkl")
            return model_package
        except Exception as e:
            print(f"Error loading models: {e}")
            print("Will retrain models...")

    # Load data if not provided
    if data is None:
        if verbose:
            print("No data provided. Attempting to load interview data...")
        try:
            # Try to load existing processed data
            data = pd.read_csv("interview_data.csv")
            if verbose:
                print(f"Successfully loaded data with {len(data)} records")
        except Exception as e:
            if verbose:
                print(f"Error loading interview_data.csv: {e}")
                print("Generating sample data for demonstration...")

            try:
                # Import the necessary module for data generation
                import sys
                from pathlib import Path

                # Add parent directory to path if needed
                current_dir = Path(__file__).parent
                parent_dir = current_dir.parent
                if str(parent_dir) not in sys.path:
                    sys.path.append(str(parent_dir))

                # Import the data retrieval module
                try:
                    from section_5b_data_retrieval import get_interview_dataset

                    # Generate a moderate dataset for testing (5K samples)
                    num_samples = 50000  # Starting with 5K samples for initial testing
                    data = get_interview_dataset(num_samples=num_samples)

                    # Save the generated data for future use
                    data.to_csv(f"interview_data_sample_{num_samples}.csv", index=False)
                    if verbose:
                        print(f"Generated and saved sample data with {len(data)} records to interview_data_sample_{num_samples}.csv")
                except ImportError as e:
                    print(f"Error importing data retrieval module: {e}")
                    # Generate minimal sample data
                    data = generate_minimal_sample_data()
                    data.to_csv("interview_data_sample.csv", index=False)
                    if verbose:
                        print(f"Generated and saved minimal sample data with {len(data)} records to interview_data_sample.csv")
            except Exception as e:
                print(f"Error generating data: {e}")
                return None

    # Continue with the rest of the function for model training
    if verbose:
        print("Starting ML model training...")
        if use_gpu:
            print("Using GPU acceleration if available")

    # Check if models already exist and we don't want to retrain
    if not retrain and os.path.exists("interview_evaluation_models.pkl"):
        try:
            models = joblib.load("interview_evaluation_models.pkl")
            if verbose:
                print("Loaded existing models from interview_evaluation_models.pkl")
            return models
        except Exception as e:
            print(f"Error loading existing models: {e}")
            print("Will train new models instead")

    if data is None or len(data) == 0:
        print("No data available for training")
        return None

    if verbose:
        print(f"Training ML models on {len(data)} samples...")

    # Extract features and targets
    feature_columns = [col for col in data.columns if col.startswith('feature_')]
    target_columns = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']

    # Check which target columns are available
    available_targets = [col for col in target_columns if col in data.columns]

    if len(feature_columns) == 0:
        print("No feature columns found in data")
        return None

    if len(available_targets) == 0:
        print("No target columns found in data")
        return None

    if verbose:
        print(f"Using {len(feature_columns)} features for model training")
        print(f"Using {len(available_targets)} evaluation metrics as targets: {available_targets}")

    # Extract features
    X = data[feature_columns].copy()

    # Ensure all feature values are numeric
    for col in X.columns:
        if X[col].dtype == 'object':
            try:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            except:
                print(f"Warning: Could not convert column {col} to numeric")
                # Drop the column if it can't be converted
                X = X.drop(columns=[col])

    # Fill NaN values with column means
    X = X.fillna(X.mean())

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

    # Dictionary to store trained models
    trained_models = {}

    # Train regression models for each target
    for target_name in available_targets:
        if verbose:
            print(f"\n=== Training models for {target_name} ===")

        # Get target values, ensuring they're numeric
        y = data[target_name].copy()
        if y.dtype == 'object':
            try:
                y = pd.to_numeric(y, errors='coerce')
            except:
                print(f"Warning: Could not convert target {target_name} to numeric")
                continue

        # Drop rows with missing target values
        valid_mask = ~y.isna()
        X_target = X_scaled[valid_mask].copy()
        y_target = y[valid_mask].copy()

        if len(y_target) == 0:
            print(f"No valid data for target {target_name}")
            continue

        # Split data for training and testing
        X_train, X_test, y_train, y_test = train_test_split(
            X_target, y_target, test_size=0.2, random_state=42
        )

        if verbose:
            print(f"Train set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples")

        # Train regression models
        best_model, _ = train_regression_models(
            X_train, X_test, y_train, y_test,
            target_name=target_name,
            use_gpu=use_gpu,
            verbose=verbose
        )

        # Store the best model
        trained_models[target_name] = best_model

    # Train classifier for Strong/Average/Weak if 'category' column exists
    if 'category' in data.columns:
        if verbose:
            print("\n=== Training classifier for Strong/Average/Weak categories ===")

        # Map categories to numerical values
        category_mapping = {'Strong': 2, 'Average': 1, 'Weak': 0}

        # Create a copy to avoid warnings
        data_for_class = data.copy()

        # Convert categories to numeric values
        if data_for_class['category'].dtype == 'object':
            # Map categorical values to numbers
            data_for_class['category_encoded'] = data_for_class['category'].map(category_mapping)

            # Check for invalid categories
            if data_for_class['category_encoded'].isna().any():
                invalid_cats = data_for_class.loc[data_for_class['category_encoded'].isna(), 'category'].unique()
                print(f"Warning: Found invalid categories: {invalid_cats}")
                print("These will be treated as missing values")
                # Fill NaN with a default value
                data_for_class = data_for_class.dropna(subset=['category_encoded'])

            # Ensure it's an integer
            data_for_class['category_encoded'] = data_for_class['category_encoded'].astype(int)

        # Use the same feature set as for regression
        X_class = X_scaled.loc[data_for_class.index].copy()
        y_class = data_for_class['category_encoded'].copy()

        # Train-test split
        X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(
            X_class, y_class, test_size=0.2, random_state=42, stratify=y_class
        )

        # Train the classifier
        classifier_model = train_classifier(
            X_train_class, X_test_class, y_train_class, y_test_class,
            use_gpu=use_gpu,
            verbose=verbose
        )

        # Store the classifier
        trained_models['classifier'] = classifier_model
        # Also store the classifier type (for prediction handling)
        trained_models['classifier_type'] = 'xgboost' if isinstance(classifier_model, xgb.Booster) else 'sklearn'

    # ==== 6.3: EVALUATING ML MODELS ====

    if verbose:
        print("\n=== Evaluating models on full dataset ===")

    # Try to run predictions on the full dataset
    try:
        # Convert the full dataset to DMatrix for XGBoost models
        X_dmatrix = xgb.DMatrix(X_scaled)

        # Predict using all models
        predictions = {}

        for target_name, model in trained_models.items():
            if target_name == 'classifier_type':
                continue  # Skip the type indicator

            if target_name == 'classifier':
                # Handle classifier predictions based on type
                if trained_models['classifier_type'] == 'xgboost':
                    # For XGBoost Booster, use DMatrix
                    class_preds_num = model.predict(X_dmatrix).astype(int)
                    # Map back to labels
                    class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}
                    class_preds = [class_mapping[pred] for pred in class_preds_num]
                else:
                    # For sklearn models
                    class_preds_num = model.predict(X_scaled)
                    # Map back to labels
                    class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}
                    class_preds = [class_mapping[pred] for pred in class_preds_num]

                predictions['category'] = class_preds

                if verbose:
                    # Show distribution of predictions
                    counts = {label: class_preds.count(label) for label in set(class_preds)}
                    print(f"    Classification results: {counts}")
            else:
                # Handle regression model predictions
                if isinstance(model, xgb.Booster):
                    # For XGBoost Booster, use DMatrix
                    target_preds = model.predict(X_dmatrix)
                else:
                    # For sklearn models
                    target_preds = model.predict(X_scaled)

                predictions[target_name] = target_preds

                if verbose:
                    # Show summary statistics of predictions
                    pred_mean = np.mean(target_preds)
                    pred_min = np.min(target_preds)
                    pred_max = np.max(target_preds)
                    print(f"    Score range: {pred_min:.2f} to {pred_max:.2f}, average: {pred_mean:.2f}")

    except Exception as e:
        print(f"Warning: Error during evaluation: {e}")
        print("This won't affect the saved models, but you may want to investigate")

    # ==== 6.4: SAVING ML MODELS ====

    # Package models with scaler for deployment
    model_package = {
        'models': trained_models,
        'scaler': scaler,
        'feature_columns': feature_columns,
        'target_columns': available_targets
    }

    try:
        # Save the model package
        joblib.dump(model_package, "interview_evaluation_models.pkl")
        if verbose:
            print("\nAll models saved successfully to interview_evaluation_models.pkl")
    except Exception as e:
        print(f"Error saving models: {e}")

    return model_package

# Function to make predictions with the trained models
def predict_with_models(data, model_package=None, verbose=True):
    """
    Make predictions using trained models

    Args:
        data: DataFrame with features
        model_package: Dictionary with trained models and scaler
        verbose: Whether to print progress information

    Returns:
        Dictionary of predictions
    """
    if verbose:
        print("Starting prediction process...")

    # Load models if not provided
    if model_package is None:
        if verbose:
            print("No model package provided, attempting to load from file...")
        try:
            model_package = joblib.load("interview_evaluation_models.pkl")
            if verbose:
                print("Successfully loaded models from interview_evaluation_models.pkl")
        except Exception as e:
            print(f"Error loading models: {e}")
            return None

    # Extract components from the package
    models = model_package['models']
    scaler = model_package['scaler']
    feature_columns = model_package['feature_columns']

    # Ensure all needed features are present
    for col in feature_columns:
        if col not in data.columns:
            data[col] = 0  # Default value for missing features

    # Extract required features in the correct order
    X = data[feature_columns].copy()

    # Ensure all feature values are numeric
    for col in X.columns:
        if not np.issubdtype(X[col].dtype, np.number):
            X[col] = X[col].astype(float)

    # Scale features
    X_scaled = scaler.transform(X)

    # Import XGBoost if needed for predictions
    try:
        import xgboost as xgb
        xgboost_available = True
    except ImportError:
        xgboost_available = False

    # Create DMatrix for XGBoost models if needed
    needs_dmatrix = False
    for model_name, model in models.items():
        if model_name != 'classifier_type' and isinstance(model, xgb.Booster):
            needs_dmatrix = True
            break

    if needs_dmatrix and xgboost_available:
        X_dmatrix = xgb.DMatrix(X_scaled)

    # Make predictions with all models
    predictions = {}
    for target_name, model in models.items():
        if target_name == 'classifier_type':
            continue  # Skip the type indicator

        if verbose:
            print(f"Predicting {target_name}...")

        if target_name == 'classifier':
            # Check classifier type
            if 'classifier_type' in models and models['classifier_type'] == 'xgboost':
                # For XGBoost booster
                class_preds_num = model.predict(X_dmatrix).astype(int)
                # Map back to labels
                class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}
                class_preds = [class_mapping[pred] for pred in class_preds_num]
                predictions['category'] = class_preds
            else:
                # For sklearn classifiers
                class_preds_num = model.predict(X_scaled)
                # Map back to labels
                class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}
                class_preds = [class_mapping[pred] for pred in class_preds_num]
                predictions['category'] = class_preds
        else:
            # Regression predictions
            if isinstance(model, xgb.Booster):
                # For XGBoost booster
                target_preds = model.predict(X_dmatrix)
            else:
                # For sklearn models
                target_preds = model.predict(X_scaled)

            predictions[target_name] = target_preds

    # Apply additional sentiment-based adjustment
    # Penalize inappropriate responses more heavily based on sentiment
    if 'feature_10' in data.columns:  # Check if sentiment feature exists
        sentiment_score = data['feature_10'].values[0]
        minimal_words = data['feature_0'].values[0] < 5  # Check if response is too short

        # Detect extremely negative or inappropriate responses
        # Lower sentiment with few words indicates potential inappropriate response
        if minimal_words and sentiment_score < 0.4:
            # Apply stronger penalty to all scores
            penalty_factor = 0.3
            for key in predictions:
                if key != 'category':  # Don't penalize the category
                    predictions[key] = np.array([max(20, score * penalty_factor) for score in predictions[key]])

    # Ensure all scores are within valid range (0-100)
    for key in predictions:
        if key != 'category':  # Don't adjust the category
            predictions[key] = np.array([max(0, min(100, score)) for score in predictions[key]])

    return predictions

# Example usage:
if __name__ == "__main__":
    print("ML Model Training Module")
    print("Starting direct execution of model training...")

    try:
        # Attempt to load data
        print("\nAttempting to load interview data...")
        try:
            data = pd.read_csv("interview_data.csv")
            print(f"Successfully loaded data with {len(data)} samples")
        except Exception as e:
            print(f"Error loading interview_data.csv: {e}")
            print("Generating sample data for demonstration...")

            # Create sample data if file doesn't exist
            np.random.seed(42)
            n_samples = 100000
            n_features = 10

            # Generate features
            features = np.random.randn(n_samples, n_features)
            feature_cols = [f'feature_{i}' for i in range(n_features)]

            # Generate target values
            technical = 0.7 * features[:, 0] + 0.3 * features[:, 1] + np.random.randn(n_samples) * 0.2
            communication = 0.5 * features[:, 2] + 0.5 * features[:, 3] + np.random.randn(n_samples) * 0.2
            problem_solving = 0.6 * features[:, 4] + 0.4 * features[:, 5] + np.random.randn(n_samples) * 0.2
            cultural_fit = 0.4 * features[:, 6] + 0.6 * features[:, 7] + np.random.randn(n_samples) * 0.2
            overall = 0.25 * technical + 0.25 * communication + 0.25 * problem_solving + 0.25 * cultural_fit

            # Scale to 0-100 range
            def scale_to_range(arr, min_val=0, max_val=100):
                return ((arr - arr.min()) / (arr.max() - arr.min())) * (max_val - min_val) + min_val

            technical = scale_to_range(technical)
            communication = scale_to_range(communication)
            problem_solving = scale_to_range(problem_solving)
            cultural_fit = scale_to_range(cultural_fit)
            overall = scale_to_range(overall)

            # Generate categories based on overall score
            categories = []
            for score in overall:
                if score >= 70:
                    categories.append('Strong')
                elif score >= 40:
                    categories.append('Average')
                else:
                    categories.append('Weak')

            # Create DataFrame
            data = pd.DataFrame(features, columns=feature_cols)
            data['technical'] = technical
            data['communication'] = communication
            data['problem_solving'] = problem_solving
            data['cultural_fit'] = cultural_fit
            data['overall'] = overall
            data['category'] = categories

            # Save sample data for future use
            try:
                data.to_csv("interview_data_sample.csv", index=False)
                print(f"Generated and saved sample data with {len(data)} records to interview_data_sample.csv")
            except:
                print("Note: Could not save sample data to file")

        # Train models
        print("\nTraining models...")
        # The global use_gpu variable was defined at the top of the file
        # It is automatically set based on GPU detection
        model_package = train_evaluation_models(
            data=data,
            retrain=True,
            verbose=True,
            use_gpu=use_gpu  # Use the global variable defined at the top of the file
        )

        if model_package:
            print("\nModel training completed successfully!")

    except Exception as e:
        print(f"\nAn error occurred during execution: {str(e)}")
        import traceback
        traceback.print_exc()

    print("\nExecution complete!")

"""# Section 13: libraries"""

# Run this cell to install required libraries
!pip install joblib matplotlib opencv-python requests transformers

"""# Section 14:Robust NLP Resource Management and Analysis"""

# ========================================
# Robust NLP Resource Management and Analysis
# ========================================

import os
import sys
import requests
import subprocess
import time
import logging
import numpy as np
from pathlib import Path
from collections import Counter
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s: %(message)s'
)
logger = logging.getLogger("NLPResourceManager")

class NLPResourceManager:
    """Manages downloading and loading of NLP resources with robust error handling"""

    def __init__(self, cache_dir=None):
        # Setup cache directory
        self.cache_dir = cache_dir or Path.home() / ".nlp_resources"
        os.makedirs(self.cache_dir, exist_ok=True)

        # Setup robust session
        self.session = self._create_robust_session()

        logger.info(f"NLP Resource Manager initialized with cache: {self.cache_dir}")

    def _create_robust_session(self, retries=5, backoff_factor=0.5, timeout=60):
        """Create a requests session with retry mechanism"""
        retry_strategy = Retry(
            total=retries,
            backoff_factor=backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "HEAD"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session = requests.Session()
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        session.timeout = timeout
        return session

    def download_file(self, url, local_path, chunk_size=8192):
        """Download a file with progress monitoring and robust error handling"""
        local_path = Path(local_path)

        # Check if file already exists in cache
        if local_path.exists():
            logger.info(f"File already exists: {local_path}")
            return local_path

        logger.info(f"Downloading {url} to {local_path}")

        # Create directory if it doesn't exist
        os.makedirs(local_path.parent, exist_ok=True)

        # Try different download methods
        download_methods = [
            self._download_with_requests,
            self._download_with_wget,
            self._download_with_curl
        ]

        for i, method in enumerate(download_methods):
            try:
                return method(url, local_path, chunk_size)
            except Exception as e:
                logger.warning(f"Download method {i+1} failed: {e}")
                if i == len(download_methods) - 1:
                    raise Exception(f"All download methods failed for {url}")
                logger.info("Trying next download method...")

        raise Exception("Unexpected download failure")

    def _download_with_requests(self, url, local_path, chunk_size):
        """Download using requests library"""
        start_time = time.time()
        temp_path = f"{local_path}.tmp"

        try:
            response = self.session.get(url, stream=True)
            response.raise_for_status()

            # Get file size if available
            total_size = int(response.headers.get('content-length', 0))

            # Download with progress tracking
            downloaded = 0
            with open(temp_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)

                        # Log progress
                        if total_size > 0 and downloaded % (5 * chunk_size) == 0:
                            percent = (downloaded / total_size) * 100
                            logger.info(f"Download progress: {percent:.1f}% ({downloaded/1024/1024:.1f}MB)")

            # Rename temp file to final name
            os.rename(temp_path, local_path)

            elapsed = time.time() - start_time
            logger.info(f"Download completed in {elapsed:.1f} seconds")
            return local_path

        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise e

    def _download_with_wget(self, url, local_path, chunk_size):
        """Download using wget command"""
        temp_path = f"{local_path}.tmp"

        try:
            # Check if wget is available
            subprocess.run(['which', 'wget'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            logger.info("Using wget for download")
            subprocess.run([
                'wget', url,
                '-O', temp_path,
                '--tries=5',
                '--timeout=60',
                '--show-progress'
            ], check=True)

            os.rename(temp_path, local_path)
            return local_path

        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise e

    def _download_with_curl(self, url, local_path, chunk_size):
        """Download using curl command"""
        temp_path = f"{local_path}.tmp"

        try:
            # Check if curl is available
            subprocess.run(['which', 'curl'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            logger.info("Using curl for download")
            subprocess.run([
                'curl', '-L', url,
                '-o', temp_path,
                '--retry', '5',
                '--retry-delay', '5',
                '--connect-timeout', '60',
                '--progress-bar'
            ], check=True)

            os.rename(temp_path, local_path)
            return local_path

        except Exception as e:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            raise e

    def install_spacy_model(self, model_name='en_core_web_sm', version='3.8.0'):
        """Install spaCy model with robust error handling and multiple strategies"""
        try:
            # First try to directly load the model (if already installed)
            import spacy
            try:
                nlp = spacy.load(model_name)
                logger.info(f"SpaCy model {model_name} already installed")
                return True
            except OSError:
                logger.info(f"SpaCy model {model_name} not installed, downloading...")

            # Strategy 1: Download via robust file download
            model_file = f"{model_name}-{version}-py3-none-any.whl"
            cache_path = Path(self.cache_dir) / "spacy_models" / model_file

            try:
                download_url = f"https://github.com/explosion/spacy-models/releases/download/{model_name}-{version}/{model_file}"
                self.download_file(download_url, cache_path)

                # Install from downloaded file
                subprocess.run([
                    sys.executable, '-m', 'pip', 'install',
                    str(cache_path), '--no-deps'
                ], check=True)

                logger.info(f"Successfully installed {model_name} from downloaded file")
                return True

            except Exception as e:
                logger.warning(f"Failed to install via direct download: {e}")

            # Strategy 2: Use spacy download command
            try:
                logger.info(f"Trying spacy download command...")
                subprocess.run([
                    sys.executable, '-m', 'spacy', 'download', model_name,
                    '--timeout', '300'
                ], check=True)

                logger.info(f"Successfully installed {model_name} via spacy download")
                return True

            except Exception as e:
                logger.warning(f"Failed to install via spacy download: {e}")

            # Strategy 3: Use pip install directly
            try:
                logger.info(f"Trying pip install...")
                model_package = f"{model_name}=={version}"
                subprocess.run([
                    sys.executable, '-m', 'pip', 'install',
                    model_package
                ], check=True)

                logger.info(f"Successfully installed {model_name} via pip")
                return True

            except Exception as e:
                logger.warning(f"Failed to install via pip: {e}")

            logger.error(f"All installation methods failed for {model_name}")
            return False

        except Exception as e:
            logger.error(f"SpaCy model installation error: {e}")
            return False

    def install_nltk_resources(self, resources=['punkt', 'stopwords', 'wordnet']):
        """Download NLTK resources with error handling"""
        try:
            import nltk

            success = True
            for resource in resources:
                try:
                    logger.info(f"Downloading NLTK resource: {resource}")
                    nltk.download(resource, quiet=True)
                except Exception as e:
                    logger.error(f"Failed to download NLTK resource {resource}: {e}")
                    success = False

            return success

        except Exception as e:
            logger.error(f"NLTK resource installation error: {e}")
            return False

    def initialize_transformers_model(self, model_name="distilbert-base-uncased-finetuned-sst-2-english"):
        """Initialize transformers model with error handling"""
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

            # Create model directory
            model_dir = Path(self.cache_dir) / "transformers" / model_name.replace('/', '_')
            os.makedirs(model_dir, exist_ok=True)

            logger.info(f"Loading transformers model: {model_name}")

            # Attempt to load model
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)
                model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=model_dir)
                nlp_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

                logger.info(f"Successfully loaded transformers model")
                return nlp_pipeline
            except Exception as e:
                logger.error(f"Failed to load transformers model: {e}")
                return None

        except Exception as e:
            logger.error(f"Error initializing transformers: {e}")
            return None

    def initialize_nlp_pipeline(self):
        """Initialize all required NLP resources"""
        logger.info("Initializing NLP pipeline...")

        # Store resources in a dictionary
        resources = {}

        # Install NLTK resources
        nltk_success = self.install_nltk_resources()
        if nltk_success:
            # Initialize NLTK components
            try:
                import nltk
                from nltk.corpus import stopwords
                from nltk.tokenize import word_tokenize
                from nltk.stem import WordNetLemmatizer

                resources['nltk_tokenize'] = word_tokenize
                resources['nltk_stopwords'] = stopwords.words('english')
                resources['nltk_lemmatizer'] = WordNetLemmatizer()
                logger.info("NLTK components initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize NLTK components: {e}")

        # Install SpaCy model
        spacy_success = self.install_spacy_model()
        if spacy_success:
            import spacy
            try:
                nlp = spacy.load('en_core_web_sm')
                resources['spacy'] = nlp
                logger.info("SpaCy model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load spaCy model: {e}")
                resources['spacy'] = None

        # Initialize transformers model
        transformer_model = self.initialize_transformers_model()
        if transformer_model:
            resources['sentiment_analyzer'] = transformer_model

        # Initialize keyword lists for various aspects
        resources['technical_keywords'] = [
            "algorithm", "data", "code", "programming", "software", "development",
            "database", "api", "function", "framework", "library", "architecture",
            "test", "debug", "deploy", "cloud", "server", "client", "network",
            "solution", "system", "design", "implementation", "process", "git",
            "version control", "agile", "scrum", "devops", "infrastructure"
        ]

        resources['communication_keywords'] = [
            "communicate", "team", "collaborate", "explain", "present", "discuss",
            "share", "meetings", "documentation", "report", "clarity", "articulate",
            "express", "listen", "feedback", "understand", "convey", "message",
            "stakeholder", "client", "user", "requirements", "specification",
            "interactive", "engagement", "interpersonal"
        ]

        resources['problem_solving_keywords'] = [
            "solve", "solution", "analyze", "optimize", "improve", "debug", "troubleshoot",
            "fix", "enhance", "approach", "method", "strategy", "plan", "design",
            "implement", "test", "evaluate", "assess", "review", "challenge",
            "innovative", "creative", "critical", "logical", "reasoning", "complexity"
        ]

        resources['cultural_fit_keywords'] = [
            "team", "culture", "values", "mission", "collaborate", "work ethic",
            "adaptable", "flexible", "learn", "growth", "positive", "attitude",
            "initiative", "proactive", "responsible", "accountable", "passion",
            "diversity", "inclusion", "respect", "ethics", "integrity", "leadership"
        ]

        resources['positive_words'] = [
            "achieve", "success", "accomplish", "excel", "improve", "positive",
            "effective", "efficient", "reliable", "confident", "passionate",
            "innovative", "excellence", "exceptional", "strong", "robust", "proven"
        ]

        resources['negative_phrases'] = [
            "i don't know", "no idea", "not sure", "never heard", "no experience",
            "i haven't", "can't answer", "don't understand", "no clue", "not familiar",
            "never done", "i guess", "maybe", "probably not", "i'm confused"
        ]

        # Mark as initialized
        resources['initialized'] = True
        logger.info("NLP resources initialized successfully")

        return resources

# Global resource cache to avoid re-initialization
_nlp_resources = None

def initialize_nlp_resources():
    """Initialize all NLP resources with comprehensive error handling"""
    global _nlp_resources

    if _nlp_resources is not None and _nlp_resources.get('initialized', False):
        logger.info("Using cached NLP resources")
        return _nlp_resources

    try:
        manager = NLPResourceManager()
        _nlp_resources = manager.initialize_nlp_pipeline()
        return _nlp_resources
    except Exception as e:
        logger.error(f"NLP initialization failed: {e}")

        # Create minimal fallback resources
        _nlp_resources = {
            'initialized': True,
            'technical_keywords': ["algorithm", "data", "code", "programming", "software"],
            'communication_keywords': ["communicate", "team", "collaborate", "explain"],
            'problem_solving_keywords': ["solve", "solution", "analyze", "optimize"],
            'cultural_fit_keywords': ["team", "culture", "values", "mission"],
            'positive_words': ["achieve", "success", "accomplish"],
            'negative_phrases': ["i don't know", "no idea"]
        }

        return _nlp_resources

# Text Analysis Functions
def analyze_text_response(text, question=None, role=None, resources=None):
    """
    Comprehensive analysis of interview response text

    Args:
        text (str): Response text to analyze
        question (str, optional): The question being answered
        role (str, optional): The job role being interviewed for
        resources (dict, optional): Pre-loaded NLP resources

    Returns:
        dict: Analysis results with scores and feedback
    """
    # Initialize resources if not provided
    if resources is None:
        resources = initialize_nlp_resources()

    # Extract relevant keyword lists
    technical_keywords = resources.get('technical_keywords', [])
    communication_keywords = resources.get('communication_keywords', [])
    problem_solving_keywords = resources.get('problem_solving_keywords', [])
    cultural_fit_keywords = resources.get('cultural_fit_keywords', [])
    positive_words = resources.get('positive_words', [])
    negative_phrases = resources.get('negative_phrases', [])

    # Basic text statistics
    text = text.strip()
    words = text.split()
    word_count = len(words)
    char_count = len(text)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)

    # Calculate keyword counts using lower case for case-insensitive matching
    text_lower = text.lower()
    technical_count = sum(1 for kw in technical_keywords if kw.lower() in text_lower)
    communication_count = sum(1 for kw in communication_keywords if kw.lower() in text_lower)
    problem_solving_count = sum(1 for kw in problem_solving_keywords if kw.lower() in text_lower)
    cultural_fit_count = sum(1 for kw in cultural_fit_keywords if kw.lower() in text_lower)

    # Calculate positive and negative phrase counts
    positive_count = sum(1 for kw in positive_words if kw.lower() in text_lower)
    negative_count = sum(1 for phrase in negative_phrases if phrase.lower() in text_lower)

    # Use SpaCy for more advanced analysis if available
    entities = []
    sentence_count = 0

    if 'spacy' in resources and resources['spacy']:
        try:
            nlp = resources['spacy']
            doc = nlp(text)

            # Count sentences
            sentence_count = len(list(doc.sents))

            # Extract entities
            entities = [ent.text for ent in doc.ents]

        except Exception as e:
            logger.warning(f"SpaCy analysis failed: {e}")
            # Fallback sentence count
            sentence_count = text.count('.') + text.count('!') + text.count('?')
            sentence_count = max(1, sentence_count)
    else:
        # Fallback sentence count if spaCy not available
        sentence_count = text.count('.') + text.count('!') + text.count('?')
        sentence_count = max(1, sentence_count)

    # Sentiment analysis
    sentiment_score = 0.5  # Neutral default

    if 'sentiment_analyzer' in resources and resources['sentiment_analyzer']:
        try:
            # Use transformers sentiment analyzer
            result = resources['sentiment_analyzer'](text[:512])  # Limit length for transformer models
            label = result[0]['label']
            confidence = result[0]['score']

            # Convert to 0-1 range where 1 is positive
            sentiment_score = confidence if label == 'POSITIVE' else 1 - confidence
        except Exception as e:
            logger.warning(f"Transformer sentiment analysis failed: {e}")
            # Basic sentiment fallback
            sentiment_score = (positive_count / max(1, word_count / 10)) - (negative_count / max(1, word_count / 20))
            sentiment_score = max(0, min(1, sentiment_score + 0.5))  # Scale to 0-1 range
    else:
        # Basic sentiment analysis fallback
        sentiment_score = (positive_count / max(1, word_count / 10)) - (negative_count / max(1, word_count / 20))
        sentiment_score = max(0, min(1, sentiment_score + 0.5))  # Scale to 0-1 range

    # Calculate response quality based on length, keywords, and structure
    length_score = min(100, word_count / 2)  # 200 words = 100 points
    keyword_density = (technical_count + communication_count + problem_solving_count + cultural_fit_count) / max(1, word_count) * 20  # Scale factor
    keyword_score = min(100, keyword_density * 100)
    structure_score = min(100, (sentence_count / max(1, word_count / 10)) * 50)  # Proper sentence structure

    # Adjust for role if specified
    role_boost = 0
    if role:
        role_lower = role.lower()
        if ('engineer' in role_lower or 'developer' in role_lower) and technical_count > communication_count:
            role_boost = 10
        elif ('manager' in role_lower or 'lead' in role_lower) and communication_count > technical_count:
            role_boost = 10
        elif 'data' in role_lower and technical_count > 0 and problem_solving_count > 0:
            role_boost = 10

    # Calculate normalized scores (0-100 scale)
    max_possible = max(10, word_count / 15)  # Scale with response length
    technical = min(100, (technical_count / max_possible) * 100 + role_boost)
    communication = min(100, (communication_count / max_possible) * 100 +
                      (sentence_count / max(1, word_count / 15)) * 20)
    problem_solving = min(100, (problem_solving_count / max_possible) * 100 +
                        (sentence_count / max(1, word_count / 15)) * 10)
    cultural_fit = min(100, (cultural_fit_count / max_possible) * 100 +
                     (sentiment_score * 30))  # Sentiment affects cultural fit score

    # Calculate overall score with weighted components
    overall = (
        technical * 0.3 +
        communication * 0.3 +
        problem_solving * 0.2 +
        cultural_fit * 0.2
    )

    # Determine category
    if overall >= 75:
        category = "Strong"
    elif overall >= 50:
        category = "Average"
    else:
        category = "Weak"

    # Generate specific feedback
    feedback = []

    if word_count < 50:
        feedback.append("Your response is quite brief. Providing more details would strengthen your answer.")
    elif word_count > 300:
        feedback.append("Your response is comprehensive, but consider being more concise in interview settings.")

    if technical < 40 and 'tech' in str(question).lower():
        feedback.append("Include more technical details relevant to the question.")

    if communication < 40:
        feedback.append("Try to articulate your thoughts more clearly and use more communication-oriented language.")

    if problem_solving < 40 and 'problem' in str(question).lower():
        feedback.append("Describe your problem-solving approach more explicitly.")

    if cultural_fit < 40:
        feedback.append("Incorporate more examples of teamwork and cultural alignment in your response.")

    if negative_count > 2:
        feedback.append("Avoid negative or uncertain phrases like 'I don't know' or 'I'm not sure'.")

    if sentence_count < 3:
        feedback.append("Structure your response with more distinct points or sentences.")

    # Prepare comprehensive results
    results = {
        # Primary scores
        "technical": technical,
        "communication": communication,
        "problem_solving": problem_solving,
        "cultural_fit": cultural_fit,
        "overall": overall,
        "category": category,

        # Detailed metrics
        "word_count": word_count,
        "sentence_count": sentence_count,
        "technical_keyword_count": technical_count,
        "communication_keyword_count": communication_count,
        "problem_solving_keyword_count": problem_solving_count,
        "cultural_fit_keyword_count": cultural_fit_count,
        "positive_word_count": positive_count,
        "negative_phrase_count": negative_count,
        "sentiment_score": sentiment_score * 100,  # Convert to percentage

        # Feedback
        "feedback": feedback,
        "entities": entities[:10]  # Limit to first 10 entities
    }

    return results

def extract_features_for_ml(text, resources=None):
    """
    Extract ML-ready features from text response

    Args:
        text (str): Text to analyze
        resources (dict, optional): NLP resources

    Returns:
        np.array: Feature vector for ML models
    """
    if resources is None:
        resources = initialize_nlp_resources()

    # Get text analysis
    analysis = analyze_text_response(text, resources=resources)

    # Create feature vector (normalized 0-1)
    features = np.array([
        analysis['word_count'] / 300,  # Normalize by expected max length
        analysis['sentence_count'] / 20,
        analysis['technical_keyword_count'] / 15,
        analysis['communication_keyword_count'] / 15,
        analysis['problem_solving_keyword_count'] / 15,
        analysis['cultural_fit_keyword_count'] / 15,
        analysis['positive_word_count'] / 10,
        analysis['negative_phrase_count'] / 5,
        analysis['sentiment_score'] / 100,
        analysis['technical'] / 100,
        analysis['communication'] / 100,
        analysis['problem_solving'] / 100,
        analysis['cultural_fit'] / 100
    ])

    return features

def create_fallback_text_analyzer():
    """Create a simple fallback analyzer when ML resources aren't available"""

    def analyze_function(text, question=None, role=None):
        resources = initialize_nlp_resources()
        return analyze_text_response(text, question, role, resources)

    return analyze_function

def process_text_response(response, question, role=None, model_package=None,
                         fallback_analyzer=None, use_deepseek=False,
                         deepseek_available=False, use_huggingface=False,
                         huggingface_available=False):
    """
    Process and evaluate an interview text response

    This is the main function to be called from the interview system.
    """
    # If external evaluation is available and enabled, use it
    if use_deepseek and deepseek_available:
        try:
            # Assume this function is defined elsewhere in the codebase
            from external_apis import evaluate_with_deepseek
            return evaluate_with_deepseek(response, question, role)
        except Exception as e:
            logger.warning(f"DeepSeek evaluation failed: {e}")
            # Continue with fallback

    if use_huggingface and huggingface_available:
        try:
            # Assume this function is defined elsewhere in the codebase
            from external_apis import evaluate_with_huggingface
            return evaluate_with_huggingface(response, question, role)
        except Exception as e:
            logger.warning(f"Hugging Face evaluation failed: {e}")
            # Continue with fallback

    # Use provided ML model package if available
    if model_package is not None:
        try:
            return model_package.analyze(response, question, role)
        except Exception as e:
            logger.warning(f"Model package analysis failed: {e}")
            # Continue with fallback

    # Use provided fallback analyzer if available
    if fallback_analyzer is not None:
        try:
            return fallback_analyzer(response, question, role)
        except Exception as e:
            logger.warning(f"Fallback analyzer failed: {e}")
            # Continue with last resort

    # Last resort: Use our basic text analyzer
    try:
        resources = initialize_nlp_resources()
        return analyze_text_response(response, question, role, resources)
    except Exception as e:
        logger.error(f"All text analysis methods failed: {e}")

        # Return minimal response with default values
        return {
            "technical": 50.0,
            "communication": 50.0,
            "problem_solving": 50.0,
            "cultural_fit": 50.0,
            "overall": 50.0,
            "category": "Average",
            "feedback": ["Unable to analyze response. Default scores assigned."]
        }

# Execute initialization
if __name__ == "__main__":
    nlp_resources = initialize_nlp_resources()
    if nlp_resources and nlp_resources.get('initialized', False):
        print("NLP resources successfully initialized!")

        # Test with a sample response
        test_response = """
        I have five years of experience as a software engineer, with expertise in Python and JavaScript.
        My last project involved developing a cloud-based solution that improved data processing efficiency by 40%.
        I implemented a robust microservices architecture using Docker and Kubernetes.
        I enjoy collaborating with cross-functional teams and believe good communication is essential to project success.
        """

        test_question = "Tell me about your experience with cloud technologies and your approach to teamwork."

        analysis = analyze_text_response(test_response, test_question, "Software Engineer", nlp_resources)

        print("\nAnalysis Results:")
        print(f"Overall Score: {analysis['overall']:.1f}/100")
        print(f"Category: {analysis['category']}")
        print("\nDimension Scores:")
        print(f"Technical: {analysis['technical']:.1f}/100")
        print(f"Communication: {analysis['communication']:.1f}/100")
        print(f"Problem Solving: {analysis['problem_solving']:.1f}/100")
        print(f"Cultural Fit: {analysis['cultural_fit']:.1f}/100")

        print("\nFeedback:")
        for item in analysis['feedback']:
            print(f"- {item}")
    else:
        print("Failed to initialize NLP resources.")

"""# Section 14:Train_interview_models"""

# train_interview_models.py
import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import random

# Create a function to generate synthetic interview response data
def generate_synthetic_data(n_samples=1000):
    """Generate synthetic data for training interview evaluation models"""
    print("Generating synthetic interview data...")

    data = []
    categories = ["Weak", "Average", "Strong"]

    for _ in range(n_samples):
        # Generate random features representing text analysis results
        word_count = max(5, np.random.normal(80, 40))  # Word count
        char_count = word_count * np.random.normal(5, 1)  # Character count
        avg_word_length = np.random.normal(5, 1)  # Average word length
        sentence_count = max(1, word_count / np.random.normal(15, 5))  # Sentence count

        # Technical keywords found
        tech_count = max(0, np.random.normal(word_count/20, word_count/40))

        # Communication keywords found
        comm_count = max(0, np.random.normal(word_count/25, word_count/50))

        # Problem solving keywords found
        prob_count = max(0, np.random.normal(word_count/22, word_count/45))

        # Cultural fit keywords found
        cult_count = max(0, np.random.normal(word_count/20, word_count/40))

        # Confidence score
        confidence_score = min(100, max(0, np.random.normal(60, 20)))

        # Keyword ratio
        keyword_ratio = min(100, max(0, np.random.normal(25, 10)))

        # Sentiment score
        sentiment_score = np.random.normal(0.6, 0.2)

        # Create target scores
        base_quality = np.random.normal(0.5, 0.25)  # Base quality factor

        # Actual scores influenced by the features
        technical_score = min(100, max(10, (
            base_quality * 30 +
            word_count * 0.3 +
            tech_count * 3 +
            prob_count * 2 +
            confidence_score * 0.2
        )))

        communication_score = min(100, max(10, (
            base_quality * 30 +
            sentence_count * 2 +
            comm_count * 3 +
            confidence_score * 0.3 +
            sentiment_score * 20
        )))

        problem_solving_score = min(100, max(10, (
            base_quality * 30 +
            prob_count * 3 +
            tech_count * 1 +
            confidence_score * 0.2
        )))

        cultural_fit_score = min(100, max(10, (
            base_quality * 30 +
            cult_count * 3 +
            comm_count * 1.5 +
            confidence_score * 0.1 +
            sentiment_score * 25
        )))

        # Overall score is weighted average of individual scores
        overall_score = (
            technical_score * 0.25 +
            communication_score * 0.25 +
            problem_solving_score * 0.25 +
            cultural_fit_score * 0.25
        )

        # Determine category
        if overall_score >= 75:
            category = "Strong"
        elif overall_score >= 45:
            category = "Average"
        else:
            category = "Weak"

        # Add some noise to make it more realistic
        if random.random() < 0.1:  # 10% chance to assign a random category
            category = random.choice(categories)

        # Add to dataset
        data.append({
            'feature_0': word_count,
            'feature_1': char_count,
            'feature_2': avg_word_length,
            'feature_3': sentence_count,
            'feature_4': tech_count,
            'feature_5': comm_count,
            'feature_6': prob_count,
            'feature_7': cult_count,
            'feature_8': confidence_score,
            'feature_9': keyword_ratio,
            'feature_10': sentiment_score,
            'technical': technical_score,
            'communication': communication_score,
            'problem_solving': problem_solving_score,
            'cultural_fit': cultural_fit_score,
            'overall': overall_score,
            'category': category
        })

    # Convert to DataFrame
    df = pd.DataFrame(data)
    return df

def train_interview_models(data):
    """Train models on the generated data"""
    print("Training interview evaluation models...")

    # Define feature columns
    feature_columns = [f'feature_{i}' for i in range(11)]

    # Define target columns
    regression_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']
    classification_target = 'category'

    # Create and train models
    models = {}

    # Scale features
    scaler = StandardScaler()
    X = data[feature_columns].copy()
    scaler.fit(X)
    X_scaled = scaler.transform(X)

    # Train regression models
    for target in regression_targets:
        print(f"Training model for {target}...")
        model = RandomForestRegressor(n_estimators=50, random_state=42)
        model.fit(X_scaled, data[target])
        models[target] = model

    # Train classification model
    print("Training classification model...")

    # Convert category to numeric
    category_mapping = {"Weak": 0, "Average": 1, "Strong": 2}
    y_class = data[classification_target].map(category_mapping)

    model = RandomForestClassifier(n_estimators=50, random_state=42)
    model.fit(X_scaled, y_class)
    models['classifier'] = model
    models['classifier_type'] = 'sklearn'

    # Create package of models
    model_package = {
        'models': models,
        'scaler': scaler,
        'feature_columns': feature_columns
    }

    return model_package

def main():
    """Main function to generate data and train models"""
    print("Starting interview evaluation model training...")

    # Generate synthetic data
    data = generate_synthetic_data(n_samples=2000)

    # Train models
    model_package = train_interview_models(data)

    # Save models
    print("Saving models to 'interview_evaluation_models.pkl'...")
    joblib.dump(model_package, "interview_evaluation_models.pkl")

    print("Model training complete! You can now use the interview evaluation system.")

if __name__ == "__main__":
    main()

"""# Section 15 :Enhanced Interview Process

"""

# SECTION 8D: ENHANCED INTERVIEW PROCESS

# Define the nlp_resources module inline
def initialize_nlp_resources():
    """Initialize NLP resources for interview analysis."""
    import nltk
    import spacy
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from nltk.sentiment.vader import SentimentIntensityAnalyzer

    # Download necessary NLTK resources
    try:
        nltk.data.find('vader_lexicon')
    except LookupError:
        nltk.download('vader_lexicon', quiet=True)

    try:
        nltk.data.find('punkt')
    except LookupError:
        nltk.download('punkt', quiet=True)

    try:
        nltk.data.find('stopwords')
    except LookupError:
        nltk.download('stopwords', quiet=True)

    # Load spaCy model
    try:
        nlp = spacy.load('en_core_web_sm')
    except OSError:
        # If model isn't downloaded, download it
        import sys
        import subprocess
        subprocess.run([sys.executable, "-m", "spacy", "download", "en_core_web_sm"])
        nlp = spacy.load('en_core_web_sm')

    # Initialize sentiment analyzer
    sentiment_analyzer = SentimentIntensityAnalyzer()

    # Create vectorizer for text analysis
    vectorizer = TfidfVectorizer(stop_words='english')

    # Setup text similarity function
    def calculate_similarity(text1, text2):
        try:
            vectors = vectorizer.fit_transform([text1, text2])
            return cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
        except:
            return 0.5

    return {
        'nlp': nlp,
        'sentiment_analyzer': sentiment_analyzer,
        'vectorizer': vectorizer,
        'calculate_similarity': calculate_similarity
    }

# Fix DeepSeek API configuration for consistent secret handling
def get_deepseek_api_key():
    """Get the DeepSeek API key from available sources."""
    import os

    try:
        # First try to get from Colab userdata
        try:
            from google.colab import userdata
            api_key = userdata.get('DEEPSEEK_API_KEY')
            if api_key:
                return api_key
        except:
            pass

        # Then try environment variables
        api_key = os.environ.get('DEEPSEEK_API_KEY')
        if api_key:
            return api_key

        # Finally, prompt the user if needed
        from getpass import getpass
        print("DeepSeek API key not found. Please provide your API key:")
        api_key = getpass("Enter your DeepSeek API key: ")
        # Save for future use in this session
        os.environ['DEEPSEEK_API_KEY'] = api_key

        return api_key
    except Exception as e:
        print(f"Error accessing DeepSeek API key: {e}")
        return None

# Create a fallback text analyzer for when more advanced methods fail
def create_fallback_text_analyzer():
    """Create a simple rule-based text analyzer as fallback."""
    def analyze_text(text, question=None, role=None):
        # Very basic analysis
        words = text.split()
        word_count = len(words)

        # Set basic scores based on response length
        if word_count < 20:
            communication = 30.0
            overall = 40.0
            category = "Weak"
        elif word_count < 50:
            communication = 50.0
            overall = 55.0
            category = "Average"
        elif word_count < 100:
            communication = 70.0
            overall = 65.0
            category = "Average"
        else:
            communication = 80.0
            overall = 75.0
            category = "Strong"

        # Count technical terms (very simplified)
        tech_terms = ['implement', 'analyze', 'develop', 'solution', 'strategy',
                     'process', 'method', 'experience', 'skill', 'technology']
        tech_count = sum(1 for term in tech_terms if term.lower() in text.lower())
        technical = min(85, tech_count * 10 + 40)

        # Problem solving assessment (simplified)
        problem_terms = ['solve', 'approach', 'resolve', 'consider', 'think',
                        'plan', 'implement', 'decision', 'analyze', 'evaluate']
        problem_count = sum(1 for term in problem_terms if term.lower() in text.lower())
        problem_solving = min(85, problem_count * 10 + 40)

        # Cultural fit (simplified)
        culture_terms = ['team', 'collaborate', 'communicate', 'value', 'culture',
                        'together', 'support', 'colleague', 'help', 'share']
        culture_count = sum(1 for term in culture_terms if term.lower() in text.lower())
        cultural_fit = min(85, culture_count * 10 + 40)

        # Calculate overall score
        overall = (technical * 0.3 + communication * 0.3 +
                  problem_solving * 0.2 + cultural_fit * 0.2)

        # Determine category
        if overall >= 75:
            category = "Strong"
        elif overall >= 50:
            category = "Average"
        else:
            category = "Weak"

        return {
            'technical': technical,
            'communication': communication,
            'problem_solving': problem_solving,
            'cultural_fit': cultural_fit,
            'overall': overall,
            'category': category
        }

    return analyze_text

# Function to process text responses with available models
def process_text_response(response, question, role, model_package, fallback_analyzer,
                         use_deepseek=False, deepseek_available=False,
                         use_huggingface=False, huggingface_available=False):
    """Process and analyze text response using available language models."""
    # Initialize with default scores
    scores = {
        'technical': 50.0,
        'communication': 50.0,
        'problem_solving': 50.0,
        'cultural_fit': 50.0,
        'overall': 50.0,
        'category': 'Average',
        'feedback': 'Your response shows average understanding of the question.'
    }

    # Try to use DeepSeek for advanced analysis if available
    if use_deepseek and deepseek_available:
        try:
            # Call DeepSeek API for evaluation
            api_key = get_deepseek_api_key()
            if not api_key:
                raise ValueError("No DeepSeek API key available")

            # Create the API client
            from openai import OpenAI
            client = OpenAI(
                base_url="https://openrouter.ai/api/v1",
                api_key=api_key
            )

            # Prepare prompt
            role_context = f" for a {role} position" if role else ""
            prompt = f"""Evaluate this interview response{role_context}:

            Question: {question}

            Response: {response}

            Please evaluate on:
            1. Technical knowledge (0-100)
            2. Communication clarity (0-100)
            3. Problem-solving approach (0-100)
            4. Cultural fit (0-100)

            Return a JSON object with these scores, an overall score (0-100),
            a category (Strong/Average/Weak), and brief feedback."""

            # Call API
            result = client.chat.completions.create(
                model="deepseek/deepseek-chat",  # Use available model
                messages=[
                    {"role": "system", "content": "You are an expert interview evaluator. Provide honest, constructive feedback."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800
            )

            # Process response
            evaluation_text = result.choices[0].message.content

            # Try to extract JSON
            import json
            import re

            # Look for JSON-like structure in the response
            json_match = re.search(r'(\{.*\})', evaluation_text, re.DOTALL)
            if json_match:
                try:
                    evaluation = json.loads(json_match.group(1))

                    # Update scores with API response
                    for key in ['technical', 'communication', 'problem_solving',
                               'cultural_fit', 'overall', 'category', 'feedback']:
                        if key in evaluation:
                            scores[key] = evaluation[key]

                    # Ensure numerical values
                    for key in ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']:
                        if key in scores:
                            # Handle percentage strings or other text
                            if isinstance(scores[key], str):
                                scores[key] = float(re.search(r'\d+', scores[key]).group())
                            scores[key] = float(scores[key])

                    scores['analysis_source'] = 'deepseek'
                    return scores

                except Exception as e:
                    print(f"Error parsing DeepSeek JSON: {e}")

            # If JSON extraction failed, try regex pattern matching
            technical_match = re.search(r'Technical.*?(\d+)', evaluation_text)
            communication_match = re.search(r'Communication.*?(\d+)', evaluation_text)
            problem_match = re.search(r'Problem.*?(\d+)', evaluation_text)
            cultural_match = re.search(r'Cultural.*?(\d+)', evaluation_text)
            overall_match = re.search(r'Overall.*?(\d+)', evaluation_text)

            if technical_match:
                scores['technical'] = float(technical_match.group(1))
            if communication_match:
                scores['communication'] = float(communication_match.group(1))
            if problem_match:
                scores['problem_solving'] = float(problem_match.group(1))
            if cultural_match:
                scores['cultural_fit'] = float(cultural_match.group(1))
            if overall_match:
                scores['overall'] = float(overall_match.group(1))

            # Extract feedback
            feedback_match = re.search(r'Feedback:?\s*(.*?)(?:\n\n|\Z)', evaluation_text, re.DOTALL)
            if feedback_match:
                scores['feedback'] = feedback_match.group(1).strip()

            # Determine category if not already set
            if 'category' not in scores or not scores['category']:
                if scores['overall'] >= 75:
                    scores['category'] = 'Strong'
                elif scores['overall'] >= 50:
                    scores['category'] = 'Average'
                else:
                    scores['category'] = 'Weak'

            scores['analysis_source'] = 'deepseek'
            return scores

        except Exception as e:
            print(f"DeepSeek analysis failed: {e}")
            # Fall back to other methods

    # Try to use Hugging Face for analysis if available
    if use_huggingface and huggingface_available:
        try:
            from transformers import pipeline

            print("Using Hugging Face for analysis...")
            # Use sentiment analysis
            sentiment_pipe = pipeline("sentiment-analysis")
            sentiment = sentiment_pipe(response)[0]

            # Adjust scores based on sentiment
            sentiment_score = sentiment['score']
            sentiment_label = sentiment['label']

            # Adjust communication score based on sentiment
            if sentiment_label == 'POSITIVE':
                scores['communication'] = min(85.0, 60.0 + sentiment_score * 20)
                scores['cultural_fit'] = min(90.0, 65.0 + sentiment_score * 25)
            else:
                scores['communication'] = max(40.0, 60.0 - sentiment_score * 15)
                scores['cultural_fit'] = max(35.0, 60.0 - sentiment_score * 20)

            # Generate text metrics based on length and structure
            words = response.split()
            word_count = len(words)

            # Adjust technical score based on response length and structure
            if word_count < 30:
                scores['technical'] = max(30.0, scores['technical'] - 15)
            elif word_count > 100:
                scores['technical'] = min(90.0, scores['technical'] + 10)

            # Adjust problem solving score
            problem_keywords = ['solve', 'approach', 'method', 'process', 'strategy', 'analyze']
            problem_keyword_count = sum(1 for word in problem_keywords if word in response.lower())
            scores['problem_solving'] = min(85.0, 50.0 + problem_keyword_count * 8)

            # Calculate overall score
            scores['overall'] = (
                scores['technical'] * 0.3 +
                scores['communication'] * 0.3 +
                scores['problem_solving'] * 0.2 +
                scores['cultural_fit'] * 0.2
            )

            # Determine category
            if scores['overall'] >= 75:
                scores['category'] = 'Strong'
            elif scores['overall'] >= 50:
                scores['category'] = 'Average'
            else:
                scores['category'] = 'Weak'

            scores['analysis_source'] = 'huggingface'
            return scores

        except Exception as e:
            print(f"Hugging Face analysis failed: {e}")

    # Use fallback analyzer if other methods failed
    try:
        if fallback_analyzer:
            fallback_scores = fallback_analyzer(response, question, role)
            scores.update(fallback_scores)
            scores['analysis_source'] = 'fallback'
            return scores
    except Exception as e:
        print(f"Fallback analysis error: {e}")

    # If all else fails, use very basic analysis
    words = response.split()
    word_count = len(words)

    # Set basic scores based on response length
    if word_count < 20:
        scores['communication'] = 30.0
        scores['overall'] = 40.0
        scores['category'] = "Weak"
        scores['feedback'] = "Your response is too brief. Consider providing more details and examples."
    elif word_count < 50:
        scores['communication'] = 50.0
        scores['overall'] = 55.0
        scores['category'] = "Average"
        scores['feedback'] = "Your response is adequate but could benefit from more depth and examples."
    elif word_count < 100:
        scores['communication'] = 70.0
        scores['overall'] = 65.0
        scores['category'] = "Average"
        scores['feedback'] = "Good response. Adding more specific details could strengthen it further."
    else:
        scores['communication'] = 80.0
        scores['overall'] = 75.0
        scores['category'] = "Strong"
        scores['feedback'] = "Comprehensive response that demonstrates good communication skills."

    scores['analysis_source'] = 'basic'
    return scores

# Function to start emotion detection in a separate thread
def start_emotion_detection(duration=10):
    """Start emotion detection in a separate thread."""
    global emotion_detection_active, emotion_results

    if emotion_detection_active:
        print("Emotion detection already active.")
        return False

    # Start emotion detection in a separate thread
    emotion_thread = threading.Thread(target=emotion_detection_thread, args=(duration,))
    emotion_thread.daemon = True
    emotion_thread.start()

    # Wait a bit to make sure it started
    time.sleep(1)

    return emotion_detection_active

# Function to analyze emotion detection results
def analyze_emotion_results(results):
    """Analyze emotion detection results and return metrics."""
    if not results:
        return None

    # Count emotions
    emotion_counts = {}
    confidence_sum = {}

    for emotion, confidence in results:
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
        confidence_sum[emotion] = confidence_sum.get(emotion, 0) + confidence

    # Find dominant emotion
    dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else "Neutral"

    # Calculate average confidence for dominant emotion
    confidence_score = confidence_sum.get(dominant_emotion, 0) / emotion_counts.get(dominant_emotion, 1)

    # Calculate engagement score (non-neutral emotions)
    total_frames = len(results)
    non_neutral_frames = sum(1 for emotion, _ in results if emotion != 'Neutral')
    engagement_score = non_neutral_frames / total_frames if total_frames > 0 else 0.5

    # Calculate positive emotion ratio
    positive_frames = sum(1 for emotion, _ in results if emotion in ['Happy', 'Surprise'])
    positive_ratio = positive_frames / total_frames if total_frames > 0 else 0.0

    return {
        'dominant_emotion': dominant_emotion,
        'confidence_score': confidence_score,
        'engagement_score': engagement_score,
        'positive_ratio': positive_ratio,
        'emotion_counts': emotion_counts
    }

# Enhanced interview function
def integrated_interview_system(max_questions=5, role=None, emotion_analysis=True,
                                use_huggingface=False, use_deepseek=True):
    """
    Run the complete integrated interview system with both text and emotion analysis.

    Parameters:
    - max_questions: Number of questions to ask
    - role: Specific job role to generate questions for
    - emotion_analysis: Whether to perform facial expression analysis
    - use_huggingface: Whether to use Hugging Face models for analysis
    - use_deepseek: Whether to use DeepSeek API for analysis
    """
    global emotion_results, emotion_detection_active

    clear_screen()
    print("Initializing enhanced interview system...")

    # Initialize NLP resources
    try:
        nlp_resources = initialize_nlp_resources()
        print("NLP resources initialized successfully")
    except Exception as e:
        print(f"Error initializing NLP resources: {e}")
        nlp_resources = None

    # Create fallback text analyzer
    fallback_analyzer = create_fallback_text_analyzer()

    # Check advanced API availability
    deepseek_available = False
    huggingface_available = TRANSFORMERS_AVAILABLE

    if use_deepseek:
        print("Testing DeepSeek API connection...")
        try:
            api_key = get_deepseek_api_key()
            if api_key:
                # Quick test of API
                from openai import OpenAI
                client = OpenAI(
                    base_url="https://openrouter.ai/api/v1",
                    api_key=api_key
                )
                response = client.chat.completions.create(
                    model="deepseek/deepseek-chat",
                    messages=[{"role": "user", "content": "Hello, testing API connection"}],
                    max_tokens=10
                )
                deepseek_available = True
                print("DeepSeek API connection successful!")
            else:
                print("DeepSeek API key not found or invalid.")
        except Exception as e:
            print(f"DeepSeek API test failed: {e}")
            print("Will use alternative analysis methods.")

    if use_huggingface and huggingface_available:
        print("Hugging Face transformers available for analysis")

    # Generate questions
    if role and deepseek_available:
        print(f"Generating {max_questions} questions for {role} role using DeepSeek API...")
        questions = generate_deepseek_interview_questions(role, max_questions)
    else:
        print(f"Generating {max_questions} questions using built-in templates...")
        questions = generate_interview_questions(role, max_questions)

    # Display welcome message
    clear_screen()
    if role:
        typing_effect(f"\n🎯 TAMKEEN AI INTERVIEW PRACTICE: {role.upper()} 🎯\n", speed=0.01)
    else:
        typing_effect("\n🎯 TAMKEEN AI INTERVIEW PRACTICE 🎯\n", speed=0.01)

    print(f"This system will evaluate your responses to {len(questions)} interview questions.")
    if emotion_analysis:
        # Check if emotion detection is available
        emotion_available = load_emotion_model()
        if emotion_available:
            print("\nWe'll also analyze your facial expressions using your webcam.")
            print("Make sure your face is visible and well-lit.")
        else:
            emotion_analysis = False
            print("\nEmotion detection is not available. We'll focus on your verbal responses only.")

    print("\nTry to answer naturally as you would in a real interview.")
    input("Press Enter when you're ready to begin...")

    # Main interview loop
    all_results = []

    for i, question in enumerate(questions):
        clear_screen()
        print(f"Question {i+1}/{len(questions)}:")
        typing_effect(f"\n{question}\n", speed=0.03)

        # Get user's response
        print("\nYour answer (type your response and press Enter twice when finished):")
        response_lines = []
        while True:
            line = input()
            if line == "" and response_lines and response_lines[-1] == "":
                break
            response_lines.append(line)

        response = "\n".join(response_lines).strip()

        # Emotion analysis if enabled
        emotion_data = None
        if emotion_analysis:
            print("\nNow we'll analyze your facial expressions.")
            print("Please look at the camera naturally as if you were in an interview.")

            if start_emotion_detection():
                # Wait for emotion detection to complete
                while emotion_detection_active:
                    print(".", end="", flush=True)
                    time.sleep(1)
                print("\nAnalyzing facial expressions...")

                # Process emotion results
                emotion_data = analyze_emotion_results(emotion_results)
            else:
                print("Emotion detection unavailable or failed.")

        # Text analysis
        print("\nAnalyzing your response...")
        text_predictions = process_text_response(
            response, question, role,
            nlp_resources, fallback_analyzer,
            use_deepseek, deepseek_available,
            use_huggingface, huggingface_available
        )

        # Combine text and emotion analysis for integrated scoring
        integrated_score = {}

        # First copy text predictions
        for key in ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall', 'category', 'feedback']:
            if key in text_predictions:
                integrated_score[key] = text_predictions[key]

        # Adjust scores based on emotion data if available
        if emotion_data:
            # Adjust communication score based on engagement and confidence
            if 'communication' in integrated_score:
                engagement_factor = emotion_data['engagement_score'] * 100  # Convert to 0-100 scale
                confidence_factor = emotion_data['confidence_score'] * 100

                # Weighted adjustment (60% text, 40% emotion)
                integrated_score['communication'] = (
                    integrated_score['communication'] * 0.6 +
                    engagement_factor * 0.2 +
                    confidence_factor * 0.2
                )

            # Adjust cultural fit score based on positive emotions
            if 'cultural_fit' in integrated_score:
                positive_factor = emotion_data['positive_ratio'] * 100

                # Weighted adjustment (70% text, 30% emotion)
                integrated_score['cultural_fit'] = (
                    integrated_score['cultural_fit'] * 0.7 +
                    positive_factor * 0.3
                )

            # Recalculate overall score
            integrated_score['overall'] = (
                integrated_score.get('technical', 50.0) * 0.3 +
                integrated_score.get('communication', 50.0) * 0.3 +
                integrated_score.get('problem_solving', 50.0) * 0.2 +
                integrated_score.get('cultural_fit', 50.0) * 0.2
            )

            # Update category based on new overall score
            if integrated_score['overall'] >= 75:
                integrated_score['category'] = 'Strong'
            elif integrated_score['overall'] >= 50:
                integrated_score['category'] = 'Average'
            else:
                integrated_score['category'] = 'Weak'

        # Store the result
        result = {
            'question': question,
            'response': response,
            'text_analysis': text_predictions,
            'emotion_analysis': emotion_data,
            'integrated_score': integrated_score
        }
        all_results.append(result)

        # Display analysis results
        clear_screen()
        print(f"Question {i+1} Analysis:\n")
        print(f"Q: {question}\n")

        # Display text analysis
        display_text_analysis(text_predictions, question)

        # Display emotion analysis if available
        if emotion_data:
            display_emotion_feedback(emotion_data)

        # Pause before next question
        if i < len(questions) - 1:
            print("\nPress Enter to continue to the next question...")
            input()

    # Display final summary after all questions
    clear_screen()
    typing_effect("\n📊 INTERVIEW SUMMARY 📊", speed=0.01)
    print(f"\nYou completed {len(all_results)} questions for a {role if role else 'general'} position.")

    # Calculate average scores
    total_technical = 0
    total_communication = 0
    total_problem_solving = 0
    total_cultural_fit = 0
    total_overall = 0
    categories = []

    for result in all_results:
        scores = result.get('integrated_score', {})
        total_technical += scores.get('technical', 50.0)
        total_communication += scores.get('communication', 50.0)
        total_problem_solving += scores.get('problem_solving', 50.0)
        total_cultural_fit += scores.get('cultural_fit', 50.0)
        total_overall += scores.get('overall', 50.0)
        categories.append(scores.get('category', 'Average'))

    num_questions = len(all_results)
    avg_technical = total_technical / num_questions
    avg_communication = total_communication / num_questions
    avg_problem_solving = total_problem_solving / num_questions
    avg_cultural_fit = total_cultural_fit / num_questions
    avg_overall = total_overall / num_questions

    # Determine final category based on most common category
    from collections import Counter
    category_counts = Counter(categories)
    final_category = category_counts.most_common(1)[0][0]

    # Display overall scores
    print("\nYour overall interview performance:")
    print(f"Technical Proficiency:  {avg_technical:.1f}/100 {get_score_emoji(avg_technical)}")
    print(f"Communication Skills:   {avg_communication:.1f}/100 {get_score_emoji(avg_communication)}")
    print(f"Problem-Solving:        {avg_problem_solving:.1f}/100 {get_score_emoji(avg_problem_solving)}")
    print(f"Cultural Fit:           {avg_cultural_fit:.1f}/100 {get_score_emoji(avg_cultural_fit)}")
    print(f"Overall Score:          {avg_overall:.1f}/100 {get_score_emoji(avg_overall)}")

    # Display final category
    print("\nOverall Classification:")
    if final_category == "Strong":
        typing_effect(f"  ⭐ {final_category} ⭐", speed=0.02)
    elif final_category == "Average":
        typing_effect(f"  ✓ {final_category} ✓", speed=0.02)
    else:
        typing_effect(f"  △ {final_category} △", speed=0.02)

    print(f"  {get_category_description(final_category)}")

    # Find strengths and weaknesses
    scores_dict = {
        "Technical knowledge": avg_technical,
        "Communication skills": avg_communication,
        "Problem-solving approach": avg_problem_solving,
        "Team and culture fit": avg_cultural_fit
    }

    # Sort to find strongest and weakest areas
    sorted_areas = sorted(scores_dict.items(), key=lambda x: x[1])
    weakest_area = sorted_areas[0][0]
    strongest_area = sorted_areas[-1][0]

    # Display strengths and weaknesses
    print(f"\nYour strongest area: {strongest_area} ({sorted_areas[-1][1]:.1f}/100)")
    print(f"Your weakest area: {weakest_area} ({sorted_areas[0][1]:.1f}/100)")

    # Generate personalized career advice
    print("\nPersonalized Interview Advice:")
    career_advice = generate_interview_advice(role, avg_overall, weakest_area, strongest_area)
    for advice in career_advice:
        print(f"• {advice}")

    # Save results
    try:
        # Check if Google Drive is mounted
        if os.path.exists('/content/drive'):
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"/content/drive/MyDrive/interview_results_{role.replace(' ', '_') if role else 'general'}_{timestamp}.json"
            print(f"\nSaving results to Google Drive: {filename}")
        else:
            # Save locally
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"interview_results/interview_{role.replace(' ', '_') if role else 'general'}_{timestamp}.json"
            print(f"\nSaving results locally: {filename}")

        save_interview_results(all_results, filename)
    except Exception as e:
        print(f"Error saving results: {e}")
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"interview_results_{timestamp}.json"
        print(f"Attempting to save locally as: {filename}")
        save_interview_results(all_results, filename)

    print("\nInterview session complete. Press Enter to finish...")
    input()
    return all_results

"""# Section 16: Colab Utilities (Optional)"""

# ============== SECTION 9: COLAB UTILITIES ==============
# These utilities are helpful when running in Google Colab

def colab_utils_setup():
    """Set up utilities for Google Colab environment"""
    if not USING_COLAB:
        print("Not running in Google Colab, skipping utilities setup.")
        return False

    try:
        from google.colab import files
        import matplotlib.pyplot as plt
        from IPython.display import display, HTML

        # Define utility functions for Colab
        def download_file(filename):
            """Trigger file download in Colab"""
            try:
                files.download(filename)
                print(f"Download initiated for {filename}")
                return True
            except Exception as e:
                print(f"Error downloading file: {e}")
                return False

        def upload_file():
            """Handle file upload in Colab"""
            try:
                uploaded = files.upload()
                print(f"Uploaded {len(uploaded)} files")
                return list(uploaded.keys())
            except Exception as e:
                print(f"Error uploading files: {e}")
                return []

        def display_html(html_content):
            """Display rich HTML in Colab"""
            display(HTML(html_content))

        def display_evaluation_chart(results):
            """Display a radar chart for interview evaluation"""
            categories = ['Technical', 'Communication', 'Problem Solving', 'Cultural Fit']
            values = [
                results.get('technical_score', 0)/10,
                results.get('communication_score', 0)/10,
                results.get('problem_solving', 0)/10,
                results.get('cultural_fit', 0)/10
            ]

            # Create radar chart
            fig = plt.figure(figsize=(10, 6))
            ax = fig.add_subplot(111, polar=True)

            # Set the angles for each category
            angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
            # Make the plot circular
            values = values + [values[0]]
            angles = angles + [angles[0]]
            categories = categories + [categories[0]]

            # Plot and fill
            ax.plot(angles, values, 'o-', linewidth=2)
            ax.fill(angles, values, alpha=0.25)

            # Set category labels
            ax.set_thetagrids(np.degrees(angles[:-1]), categories[:-1])

            # Set radial ticks and labels
            ax.set_rlabel_position(0)
            ax.set_rticks([0, 2.5, 5, 7.5, 10])
            ax.set_rlim(0, 10)

            # Add title
            recommendation = results.get('recommendation', 'No recommendation')
            plt.title(f"Interview Evaluation\nOverall Score: {results.get('overall_score', 0)/10:.1f}/10 - {recommendation}",
                    size=15, color='blue', y=1.1)

            plt.tight_layout()
            plt.show()

        # Add these functions to globals
        globals()['download_file'] = download_file
        globals()['upload_file'] = upload_file
        globals()['display_html'] = display_html
        globals()['display_evaluation_chart'] = display_evaluation_chart

        print("✅ Colab utilities successfully configured")
        return True

    except Exception as e:
        print(f"Error setting up Colab utilities: {e}")
        return False

# Set up Colab utilities if we're in Colab
if USING_COLAB:
    colab_utils_setup()

    # Modify the run_interview function to use Colab utilities when available
    original_run_interview = run_interview

    def enhanced_colab_run_interview():
        """Enhanced version of run_interview with Colab-specific features"""
        results = original_run_interview()

        # If we have evaluation results and the display function exists
        if results and 'display_evaluation_chart' in globals():
            print("\nGenerating visual evaluation chart...")
            display_evaluation_chart(results)

        return results

    # Replace the original function with our enhanced version
    run_interview = enhanced_colab_run_interview

# Function to analyze and visualize the datasets in Colab
def explore_datasets():
    """Analyze and visualize interview questions and evaluations datasets in Colab"""
    if not USING_COLAB:
        print("This function is designed for Google Colab environment")
        return

    try:
        # Import visualization libraries
        import matplotlib.pyplot as plt
        import seaborn as sns
        sns.set_style("whitegrid")

        # Load datasets
        print("Loading datasets...")
        questions_df = pd.read_csv("interview_questions.csv")
        evaluations_df = pd.read_csv("evaluation_scores.csv")

        print(f"Interview Questions: {len(questions_df)} records")
        print(f"Evaluation Scores: {len(evaluations_df)} records")

        # Display dataset heads
        print("\n--- Interview Questions Sample ---")
        display(questions_df.head(3))

        print("\n--- Evaluation Scores Sample ---")
        display(evaluations_df.head(3))

        # Visualize question distributions
        plt.figure(figsize=(16, 10))

        # Plot profession distribution
        plt.subplot(2, 2, 1)
        profession_counts = questions_df['profession'].value_counts()
        sns.barplot(x=profession_counts.index, y=profession_counts.values)
        plt.title('Distribution of Professions')
        plt.xticks(rotation=90)
        plt.ylabel('Count')

        # Plot question type distribution
        plt.subplot(2, 2, 2)
        q_type_counts = questions_df['question_type'].value_counts()
        sns.barplot(x=q_type_counts.index, y=q_type_counts.values)
        plt.title('Distribution of Question Types')
        plt.xticks(rotation=45)
        plt.ylabel('Count')

        # Plot difficulty level distribution
        plt.subplot(2, 2, 3)
        difficulty_counts = questions_df['difficulty_level'].value_counts()
        sns.barplot(x=difficulty_counts.index, y=difficulty_counts.values)
        plt.title('Distribution of Difficulty Levels')
        plt.ylabel('Count')

        # Histogram of evaluation scores
        plt.subplot(2, 2, 4)
        sns.histplot(evaluations_df['score'], bins=20, kde=True)
        plt.title('Distribution of Evaluation Scores')
        plt.xlabel('Score')
        plt.ylabel('Frequency')

        plt.tight_layout()
        plt.show()

        # Show correlation heatmap for merged data
        print("\nMerging datasets for correlation analysis...")
        merged_df = pd.merge(evaluations_df, questions_df, on='question_id')

        # Convert categorical variables to numeric for correlation
        merged_df = pd.get_dummies(merged_df, columns=['profession', 'question_type', 'difficulty_level'])
        merged_df['evaluation_date'] = pd.to_datetime(merged_df['evaluation_date'])
        merged_df['month'] = merged_df['evaluation_date'].dt.month
        merged_df['year'] = merged_df['evaluation_date'].dt.year

        # Select numeric columns for correlation
        numeric_cols = merged_df.select_dtypes(include=['number']).columns

        # Plot correlation heatmap
        plt.figure(figsize=(12, 10))
        corr_matrix = merged_df[numeric_cols].corr()
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)
        plt.title('Correlation Matrix of Dataset Features')
        plt.tight_layout()
        plt.show()

        # Interactive plot for score distribution by profession
        plt.figure(figsize=(14, 6))
        sns.boxplot(x='profession', y='score', data=merged_df)
        plt.title('Score Distribution by Profession')
        plt.xticks(rotation=90)
        plt.show()

        # Interactive plot for score distribution by question type
        plt.figure(figsize=(14, 6))
        sns.boxplot(x='question_type', y='score', data=merged_df)
        plt.title('Score Distribution by Question Type')
        plt.show()

        # Interactive plot for score distribution by difficulty level
        plt.figure(figsize=(14, 6))
        sns.boxplot(x='difficulty_level', y='score', data=merged_df)
        plt.title('Score Distribution by Difficulty Level')
        plt.show()

        print("Datasets exploration complete!")

    except FileNotFoundError:
        print("Error: Dataset files not found. Make sure you've generated the datasets first.")
    except Exception as e:
        print(f"Error exploring datasets: {e}")

# Add this function to Colab utilities if we're in Colab
if USING_COLAB and 'colab_utils_setup' in globals():
    globals()['explore_datasets'] = explore_datasets
    print("✅ Dataset exploration utility added. Call explore_datasets() to analyze your datasets.")

"""# Section : Extra UI"""

import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML, clear_output
import random
import sys
import os
from sklearn.metrics import confusion_matrix

# Check if running in Google Colab
USING_COLAB = 'google.colab' in sys.modules

# No need to import functions from section_8 as they should be available
# in the global namespace when running as cells in Colab

# Define fallback functions only if they don't exist in the global namespace
if 'format_score' not in globals():
    def format_score(score):
        return f"{score:.1f}/10"

if 'get_score_emoji' not in globals():
    def get_score_emoji(score):
        if score >= 80: return "🌟"
        elif score >= 60: return "✅"
        elif score >= 40: return "⚠️"
        else: return "❌"

if 'get_category_description' not in globals():
    def get_category_description(category):
        descriptions = {
            "Expert": "You demonstrate exceptional knowledge and communication skills.",
            "Proficient": "You show strong understanding and good communication.",
            "Competent": "You have a solid foundation but could improve in some areas.",
            "Developing": "You show some understanding but need significant improvement.",
            "Novice": "You need to develop fundamental knowledge and skills in this area.",
            "Average": "Your response shows average competency across all dimensions."
        }
        return descriptions.get(category, "Your response received a standard evaluation.")

if 'clear_screen' not in globals():
    def clear_screen():
        if not USING_COLAB:
            os.system('cls' if os.name == 'nt' else 'clear')

# ============================
# COLAB UTILITIES
# ============================

def setup_colab_environment():
    """Set up the Google Colab environment with necessary libraries and styling."""
    if not USING_COLAB:
        print("Not running in Google Colab. Skipping environment setup.")
        return False

    try:
        from google.colab import files

        # Set custom styling for better UI
        custom_css = """
        <style>
            .interview-header {
                background-color: #4285F4;
                color: white;
                padding: 15px;
                border-radius: 5px;
                margin-bottom: 20px;
                text-align: center;
                font-size: 24px;
                font-weight: bold;
            }

            .question-box {
                background-color: #f8f9fa;
                border-left: 5px solid #4285F4;
                padding: 10px 15px;
                margin: 20px 0;
                border-radius: 0 5px 5px 0;
                font-size: 18px;
            }

            .response-box {
                background-color: #e8f0fe;
                border: 1px solid #4285F4;
                padding: 15px;
                margin: 15px 0;
                border-radius: 5px;
                min-height: 100px;
            }

            .result-card {
                background-color: white;
                border: 1px solid #dadce0;
                border-radius: 8px;
                padding: 20px;
                margin: 20px 0;
                box-shadow: 0 1px 2px rgba(60,64,67,0.3);
            }

            .metric-row {
                display: flex;
                justify-content: space-between;
                margin: 10px 0;
                padding: 5px 0;
                border-bottom: 1px solid #f1f3f4;
            }

            .metric-label {
                font-weight: bold;
                color: #4285F4;
            }

            .progress-container {
                width: 100%;
                background-color: #e0e0e0;
                border-radius: 5px;
                margin: 5px 0;
            }

            .progress-bar {
                height: 20px;
                border-radius: 5px;
                text-align: center;
                color: white;
                font-weight: bold;
            }

            .feedback-box {
                background-color: #f1f3f4;
                border-left: 5px solid #fbbc04;
                padding: 15px;
                margin-top: 20px;
                border-radius: 0 5px 5px 0;
            }

            .summary-title {
                text-align: center;
                font-size: 22px;
                font-weight: bold;
                margin: 20px 0;
                color: #4285F4;
                border-bottom: 2px solid #4285F4;
                padding-bottom: 10px;
            }

            .recommendation {
                background-color: #e6f4ea;
                border-left: 5px solid #34a853;
                padding: 15px;
                margin: 15px 0;
                border-radius: 0 5px 5px 0;
            }

            .warning {
                background-color: #fef7e0;
                border-left: 5px solid #fbbc04;
                padding: 15px;
                margin: 15px 0;
                border-radius: 0 5px 5px 0;
            }

            button {
                background-color: #4285F4;
                color: white;
                border: none;
                padding: 10px 15px;
                border-radius: 5px;
                cursor: pointer;
                font-size: 16px;
                margin: 10px 5px;
            }

            button:hover {
                background-color: #3367d6;
            }
        </style>
        """

        # Display the custom CSS
        display(HTML(custom_css))

        print("✅ Colab environment setup complete")
        return True

    except Exception as e:
        print(f"Error setting up Colab environment: {e}")
        return False

def display_header(title):
    """Display a styled header in Colab."""
    if not USING_COLAB:
        print(f"\n{title}\n{'=' * len(title)}")
        return

    header_html = f'<div class="interview-header">{title}</div>'
    display(HTML(header_html))

def display_question(question_num, total_questions, question_text):
    """Display a formatted interview question."""
    if not USING_COLAB:
        print(f"\nQuestion {question_num}/{total_questions}: {question_text}")
        return

    question_html = f'''
    <div class="question-box">
        <strong>Question {question_num}/{total_questions}:</strong><br>
        {question_text}
    </div>
    '''
    display(HTML(question_html))

def create_response_area():
    """Create an interactive response area for the user."""
    if not USING_COLAB:
        print("\nPlease type your response below:")
        return

    response_html = '''
    <div class="response-box" id="response-area" contenteditable="true">
        Type your interview response here...
    </div>

    <button onclick="submitResponse()">Submit Response</button>

    <script>
    // Define a function to submit the response
    function submitResponse() {
        var responseText = document.getElementById('response-area').innerText;
        if (responseText.trim() === "" || responseText === "Type your interview response here...") {
            alert("Please provide a response before submitting.");
            return;
        }

        // Send the response back to Python
        google.colab.kernel.invokeFunction('notebook.getResponse', [responseText], {});

        // Disable editing after submission
        document.getElementById('response-area').contentEditable = false;
        document.getElementById('response-area').style.backgroundColor = "#f1f3f4";

        // Change button text to show processing
        var buttons = document.getElementsByTagName('button');
        for (var i = 0; i < buttons.length; i++) {
            buttons[i].disabled = true;
            buttons[i].innerText = "Processing...";
        }
    }
    </script>
    '''
    display(HTML(response_html))

    # Register the callback function to get the response
    from google.colab import output

    response_text = [None]  # Using a list to store the value that will be modified by the callback

    def get_response_callback(response):
        response_text[0] = response

    output.register_callback('notebook.getResponse', get_response_callback)

    # Wait for the response
    while response_text[0] is None:
        time.sleep(0.5)

    return response_text[0]

def display_processing_animation():
    """Display an animation while processing the response."""
    if not USING_COLAB:
        print("\nAnalyzing your response...")
        return

    processing_html = '''
    <div style="text-align: center; margin: 20px 0;">
        <p>Analyzing your response...</p>
        <div style="display: inline-block; position: relative; width: 80px; height: 80px;">
            <div style="position: absolute; border: 4px solid #4285F4; opacity: 1; border-radius: 50%; animation: ripple 1s cubic-bezier(0, 0.2, 0.8, 1) infinite;" ></div>
            <div style="position: absolute; border: 4px solid #4285F4; opacity: 1; border-radius: 50%; animation: ripple 1s cubic-bezier(0, 0.2, 0.8, 1) infinite; animation-delay: -0.5s;" ></div>
        </div>
    </div>

    <style>
    @keyframes ripple {
      0% {
        top: 36px;
        left: 36px;
        width: 0;
        height: 0;
        opacity: 1;
      }
      100% {
        top: 0px;
        left: 0px;
        width: 72px;
        height: 72px;
        opacity: 0;
      }
    }
    </style>
    '''
    display(HTML(processing_html))
    time.sleep(2)  # Simulate processing time

def display_score_bar(score, label, color):
    """Display a score as a progress bar."""
    if not USING_COLAB:
        print(f"{label}: {format_score(score)} {get_score_emoji(score)}")
        return

    # Ensure score is between 0 and 100
    score = max(0, min(100, score))

    # Determine bar color based on score
    if color is None:
        if score >= 80:
            color = "#34a853"  # Green
        elif score >= 60:
            color = "#fbbc04"  # Yellow
        elif score >= 40:
            color = "#fa7b17"  # Orange
        else:
            color = "#ea4335"  # Red

    bar_html = f'''
    <div class="metric-row">
        <span class="metric-label">{label}:</span>
        <span>{format_score(score)} {get_score_emoji(score)}</span>
    </div>
    <div class="progress-container">
        <div class="progress-bar" style="width: {score}%; background-color: {color};">{score}%</div>
    </div>
    '''
    return bar_html

def display_results_card(predictions, question=None):
    """Display evaluation results in a visually appealing card format."""
    if not USING_COLAB:
        # Fall back to the original display_results function
        if 'display_results' in globals():
            display_results(predictions, question)
        else:
            print("\nEvaluation Results:")
            print(f"Technical: {format_score(predictions.get('technical', 50))}")
            print(f"Communication: {format_score(predictions.get('communication', 50))}")
            print(f"Problem Solving: {format_score(predictions.get('problem_solving', 50))}")
            print(f"Cultural Fit: {format_score(predictions.get('cultural_fit', 50))}")
            print(f"Overall: {format_score(predictions.get('overall', 50))}")
        return

    # Safely extract prediction values
    def safe_extract(pred_dict, key):
        if key not in pred_dict:
            return 50.0 if key != 'category' else 'Average'

        value = pred_dict[key]
        if isinstance(value, (list, np.ndarray)) and len(value) > 0:
            return value[0]
        return value

    # Get prediction values
    technical = float(safe_extract(predictions, 'technical'))
    communication = float(safe_extract(predictions, 'communication'))
    problem_solving = float(safe_extract(predictions, 'problem_solving'))
    cultural_fit = float(safe_extract(predictions, 'cultural_fit'))
    overall = float(safe_extract(predictions, 'overall'))
    category = str(safe_extract(predictions, 'category'))

    # Create the results card HTML
    results_html = f'''
    <div class="result-card">
        <h2 style="text-align: center; color: #4285F4; margin-top: 0;">Response Evaluation</h2>

        {display_score_bar(technical, "Technical Proficiency", "#4285F4")}
        {display_score_bar(communication, "Communication Skills", "#34a853")}
        {display_score_bar(problem_solving, "Problem-Solving Approach", "#fbbc04")}
        {display_score_bar(cultural_fit, "Team & Culture Fit", "#ea4335")}

        <div style="border-top: 2px solid #f1f3f4; margin: 15px 0; padding-top: 15px;">
            {display_score_bar(overall, "Overall Performance", None)}
        </div>

        <div class="feedback-box">
            <strong>Performance Category:</strong> {category}<br>
            <p>{get_category_description(category)}</p>
        </div>
    </div>
    '''

    display(HTML(results_html))

def plot_radar_chart(predictions):
    """Create and display a radar chart of the evaluation metrics."""
    if not USING_COLAB:
        return

    # Extract scores
    def safe_extract(pred_dict, key):
        if key not in pred_dict:
            return 50.0

        value = pred_dict[key]
        if isinstance(value, (list, np.ndarray)) and len(value) > 0:
            return value[0]
        return value

    # Get values and normalize to 0-10 scale
    technical = float(safe_extract(predictions, 'technical')) / 10
    communication = float(safe_extract(predictions, 'communication')) / 10
    problem_solving = float(safe_extract(predictions, 'problem_solving')) / 10
    cultural_fit = float(safe_extract(predictions, 'cultural_fit')) / 10
    overall = float(safe_extract(predictions, 'overall')) / 10

    # Categories for radar chart
    categories = ['Technical\nProficiency', 'Communication\nSkills',
                  'Problem-Solving\nApproach', 'Team &\nCulture Fit']

    values = [technical, communication, problem_solving, cultural_fit]

    # Set up the radar chart
    plt.figure(figsize=(10, 8))

    # Set background color
    plt.rcParams['axes.facecolor'] = '#f8f9fa'

    # Create angles for each category
    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()

    # Make the plot circular
    values = values + [values[0]]
    angles = angles + [angles[0]]
    categories = categories + [categories[0]]

    # Create the plot
    ax = plt.subplot(111, polar=True)

    # Plot and fill the polygon
    ax.plot(angles, values, 'o-', linewidth=2, color='#4285F4', label='Your Scores')
    ax.fill(angles, values, alpha=0.25, color='#4285F4')

    # Add a reference circle at score 7.5 (75%)
    reference_values = [7.5] * (len(categories))
    ax.plot(angles, reference_values, '--', linewidth=1, color='#34a853', alpha=0.5, label='Expert Level (7.5)')

    # Set category labels
    ax.set_thetagrids(np.degrees(angles[:-1]), categories[:-1], fontsize=12)

    # Set radial ticks and labels
    ax.set_rlabel_position(0)
    ax.set_rticks([2.5, 5, 7.5, 10])
    ax.set_rlim(0, 10)

    # Add title and legend
    plt.title(f"Interview Performance Metrics\nOverall Score: {overall:.1f}/10",
              size=16, color='#4285F4', y=1.1)

    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

    plt.tight_layout()
    plt.show()

def display_summary_dashboard(all_results):
    """Display a comprehensive summary dashboard of all interview responses."""
    if not USING_COLAB:
        return

    # Calculate average scores
    avg_technical = 0
    avg_communication = 0
    avg_problem_solving = 0
    avg_cultural_fit = 0
    avg_overall = 0
    categories = []

    for result in all_results:
        predictions = result['predictions']

        # Safely extract values
        def safe_extract(pred_dict, key):
            if key not in pred_dict:
                return 50.0 if key != 'category' else 'Average'

            value = pred_dict[key]
            if isinstance(value, (list, np.ndarray)) and len(value) > 0:
                return value[0]
            return value

        # Extract scores
        technical = float(safe_extract(predictions, 'technical'))
        communication = float(safe_extract(predictions, 'communication'))
        problem_solving = float(safe_extract(predictions, 'problem_solving'))
        cultural_fit = float(safe_extract(predictions, 'cultural_fit'))
        overall = float(safe_extract(predictions, 'overall'))
        category = str(safe_extract(predictions, 'category'))

        # Add to totals
        avg_technical += technical
        avg_communication += communication
        avg_problem_solving += problem_solving
        avg_cultural_fit += cultural_fit
        avg_overall += overall
        categories.append(category)

    # Calculate averages
    n = len(all_results)
    avg_technical /= n
    avg_communication /= n
    avg_problem_solving /= n
    avg_cultural_fit /= n
    avg_overall /= n

    # Display summary header
    display_header("Interview Performance Summary")

    # Create summary HTML
    summary_html = f'''
    <div class="result-card">
        <div class="summary-title">Overall Performance</div>

        {display_score_bar(avg_overall, "Average Overall Score", None)}

        <div style="margin-top: 30px;">
            {display_score_bar(avg_technical, "Technical Proficiency", "#4285F4")}
            {display_score_bar(avg_communication, "Communication Skills", "#34a853")}
            {display_score_bar(avg_problem_solving, "Problem-Solving Approach", "#fbbc04")}
            {display_score_bar(avg_cultural_fit, "Team & Culture Fit", "#ea4335")}
        </div>
    </div>
    '''

    display(HTML(summary_html))

    # Create a bar chart comparing areas
    plt.figure(figsize=(12, 6))
    metrics = ['Technical', 'Communication', 'Problem Solving', 'Cultural Fit', 'Overall']
    values = [avg_technical, avg_communication, avg_problem_solving, avg_cultural_fit, avg_overall]
    colors = ['#4285F4', '#34a853', '#fbbc04', '#ea4335', '#673ab7']

    bars = plt.bar(metrics, values, color=colors)

    # Add values on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')

    plt.ylim(0, 105)
    plt.title('Average Performance by Category', fontsize=16, pad=20)
    plt.ylabel('Score (%)', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

    # Career advice based on overall performance
    advice_html = '''
    <div class="result-card">
        <div class="summary-title">Key Takeaways and Advice</div>
    '''

    # Find weakest and strongest areas
    areas = {
        "Technical knowledge": avg_technical,
        "Communication clarity": avg_communication,
        "Problem-solving approach": avg_problem_solving,
        "Team and culture fit": avg_cultural_fit
    }

    sorted_areas = sorted(areas.items(), key=lambda x: x[1])
    weakest_area = sorted_areas[0]
    strongest_area = sorted_areas[-1]

    advice_html += f'''
        <div class="recommendation">
            <strong>Your strongest area is {strongest_area[0]} ({format_score(strongest_area[1])}).</strong><br>
            Continue to leverage this strength in interviews. Share specific examples that highlight your capabilities in this area.
        </div>

        <div class="warning">
            <strong>Focus on improving {weakest_area[0]} ({format_score(weakest_area[1])}).</strong><br>
            Consider preparing specific examples and practicing responses related to this area before your next interview.
        </div>
    '''

    if avg_overall >= 80:
        advice_html += '''
        <div class="recommendation">
            <strong>You show strong interview skills.</strong><br>
            Consider applying for more challenging positions. Your interview performance suggests you're ready for advanced roles.
        </div>
        '''
    elif avg_overall >= 60:
        advice_html += '''
        <div class="recommendation">
            <strong>You have a good foundation.</strong><br>
            Practice more mock interviews to boost confidence. Focus on providing more specific examples in your answers.
        </div>
        '''
    else:
        advice_html += '''
        <div class="warning">
            <strong>Continue practicing interview scenarios.</strong><br>
            Focus on your weaker areas and consider recording yourself to review your communication style and body language.
        </div>
        '''

    advice_html += '''
        <div style="margin-top: 30px;">
            <strong>Recommended next steps:</strong>
            <ol>
                <li>Review and practice questions in your weaker areas</li>
                <li>Record yourself answering questions to analyze your communication</li>
                <li>Seek feedback from industry professionals if possible</li>
                <li>Consider another practice session focusing on your weakest topics</li>
            </ol>
        </div>
    </div>
    '''

    display(HTML(advice_html))

    # Question breakdown
    question_html = '''
    <div class="result-card">
        <div class="summary-title">Question Breakdown</div>
        <table style="width: 100%; border-collapse: collapse;">
            <tr style="background-color: #4285F4; color: white;">
                <th style="padding: 10px; text-align: left;">Question</th>
                <th style="padding: 10px; text-align: center;">Overall Score</th>
                <th style="padding: 10px; text-align: center;">Category</th>
            </tr>
    '''

    for i, result in enumerate(all_results):
        predictions = result['predictions']
        question = result['question']
        overall = float(safe_extract(predictions, 'overall'))
        category = str(safe_extract(predictions, 'category'))

        # Alternate row colors
        bg_color = "#f8f9fa" if i % 2 == 0 else "white"

        question_html += f'''
            <tr style="background-color: {bg_color};">
                <td style="padding: 10px; border-bottom: 1px solid #dadce0;">{question}</td>
                <td style="padding: 10px; text-align: center; border-bottom: 1px solid #dadce0;">{format_score(overall)} {get_score_emoji(overall)}</td>
                <td style="padding: 10px; text-align: center; border-bottom: 1px solid #dadce0;">{category}</td>
            </tr>
        '''

    question_html += '''
        </table>
    </div>
    '''

    display(HTML(question_html))

# ============================
# ENHANCED INTERVIEW FUNCTIONS
# ============================

def enhanced_run_interview(max_questions=3, all_results=None, question_num=1):
    """
    Run the interactive interview process with an enhanced Colab UI.

    Args:
        max_questions: Maximum number of questions to ask
        all_results: List to store all predictions
        question_num: Current question number
    """
    # Initialize all_results if this is the first question
    if all_results is None:
        all_results = []

        # Set up Colab environment if first question
        if USING_COLAB:
            setup_colab_environment()

    # Clear the output
    if USING_COLAB:
        clear_output(wait=True)
    else:
        if 'clear_screen' in globals():
            clear_screen()
        else:
            print("\n" * 5)  # Simple fallback

    # Show welcome message on the first question
    if question_num == 1:
        display_header("🤖 AI Interview Evaluation System 🤖")

        welcome_html = f'''
        <div style="text-align: center; margin: 20px 0;">
            <p>Welcome to the enhanced interview evaluation experience!</p>
            <p>I'll ask you {max_questions} interview questions, analyze your responses,
            and provide detailed feedback on your performance.</p>
        </div>
        '''

        if USING_COLAB:
            display(HTML(welcome_html))
        else:
            print(f"Welcome to the AI Interview Evaluation System!")
            print(f"I'll ask you {max_questions} interview questions, analyze your responses,")
            print(f"and provide feedback on your performance.\n")

    # List of interview questions
    questions = [
        "Tell me about yourself and your experience in software engineering.",
        "Describe a challenging project you worked on and how you overcame obstacles.",
        "How do you approach debugging a complex issue in your code?",
        "Explain how you would design a scalable web application architecture.",
        "How do you stay updated with the latest technologies in your field?",
        "Describe a situation where you had to work with a difficult team member.",
        "What's your experience with agile development methodologies?",
        "How would you optimize a slow-performing database query?",
        "Tell me about a time when you had to learn a new technology quickly."
    ]

    # Randomly select a question
    question = random.choice(questions)

    # Display the question
    display_question(question_num, max_questions, question)

    # Get the user's response
    if USING_COLAB:
        response = create_response_area()
    else:
        response = input("Your response: ")

    # Show processing animation
    if USING_COLAB:
        clear_output(wait=True)
        display_processing_animation()
    else:
        print("\nAnalyzing your response...")

    # Extract features from the response
    features = extract_features_from_text(response)

    # Make predictions
    predictions = predict_response_scores(features)

    # Clear the output
    if USING_COLAB:
        clear_output(wait=True)

    # Display results
    if USING_COLAB:
        display_results_card(predictions, question)
        plot_radar_chart(predictions)
    else:
        display_results(predictions, question)

    # Store result
    result = {
        'question': question,
        'response': response,
        'predictions': predictions
    }
    all_results.append(result)

    # Ask if the user wants to continue to the next question
    if question_num < max_questions:
        if USING_COLAB:
            next_question_html = '''
            <div style="margin: 30px 0; text-align: center;">
                <button onclick="continueInterview()">Continue to Next Question</button>
            </div>

            <script>
            function continueInterview() {
                google.colab.kernel.invokeFunction('notebook.continueInterview', [], {});

                // Disable button after click
                var buttons = document.getElementsByTagName('button');
                for (var i = 0; i < buttons.length; i++) {
                    buttons[i].disabled = true;
                    buttons[i].innerText = "Loading...";
                }
            }
            </script>
            '''
            display(HTML(next_question_html))

            # Register the callback function
            from google.colab import output

            continue_interview = [False]

            def continue_interview_callback():
                continue_interview[0] = True

            output.register_callback('notebook.continueInterview', continue_interview_callback)

            # Wait for the user to click the button
            while not continue_interview[0]:
                time.sleep(0.5)

            # Continue to the next question
            return enhanced_run_interview(max_questions, all_results, question_num + 1)
        else:
            continue_interview = input("\nContinue to next question? (y/n): ").lower().strip()
            if continue_interview == 'y':
                return enhanced_run_interview(max_questions, all_results, question_num + 1)
    else:
        # Display final summary
        if USING_COLAB:
            clear_output(wait=True)

        display_summary_dashboard(all_results)

    return all_results

# ============================
# MAIN EXECUTION
# ============================

def main():
    """Main function to run the enhanced interview process."""
    print("Section 10: Enhanced Colab Interview Interface")
    print("=============================================")

    if USING_COLAB:
        print("Running in Google Colab environment")
        setup_colab_environment()
    else:
        print("Not running in Google Colab. Some features will be limited.")

    # Prompt for the number of questions
    if USING_COLAB:
        questions_html = '''
        <div style="text-align: center; margin: 20px 0;">
            <h2>How many interview questions would you like to practice?</h2>
            <input type="number" id="num-questions" min="1" max="10" value="3" style="padding: 10px; font-size: 16px; width: 100px; text-align: center;">
            <button onclick="startInterview()">Start Interview</button>
        </div>

        <script>
        function startInterview() {
            var numQuestions = document.getElementById('num-questions').value;
            google.colab.kernel.invokeFunction('notebook.startInterview', [numQuestions], {});

            // Disable button after click
            var buttons = document.getElementsByTagName('button');
            for (var i = 0; i < buttons.length; i++) {
                buttons[i].disabled = true;
                buttons[i].innerText = "Starting...";
            }
        }
        </script>
        '''
        display(HTML(questions_html))

        # Register the callback function
        from google.colab import output

        num_questions = [3]  # Default

        def start_interview_callback(n):
            try:
                num_questions[0] = int(n)
            except:
                num_questions[0] = 3

        output.register_callback('notebook.startInterview', start_interview_callback)

        # Wait for the user to click the start button
        start_interview = [False]

        def trigger_start():
            start_interview[0] = True

        output.register_callback('notebook.startInterview', trigger_start)

        # Wait for the user to start
        while not start_interview[0]:
            time.sleep(0.5)

        # Start the interview with the selected number of questions
        enhanced_run_interview(max_questions=num_questions[0])
    else:
        # CLI version
        try:
            max_questions = int(input("How many interview questions would you like to practice? (1-10): "))
            max_questions = max(1, min(10, max_questions))  # Ensure between 1 and 10
        except:
            max_questions = 3
            print("Using default: 3 questions")

        enhanced_run_interview(max_questions=max_questions)

# Run the main function when this cell is executed in Colab
main()

"""# Section 17: Gamified Dashboard & Analytics

"""

# Section E: Gamified Dashboard & Analytics
# This section provides an immersive, gamified dashboard with advanced analytics,
# AI-powered career forecasting, and interactive visualizations

# Install required packages
!pip install -q plotly matplotlib seaborn scipy networkx wordcloud pillow ipywidgets pycountry folium scikit-learn pandas-profiling

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import networkx as nx
from datetime import datetime, timedelta
from wordcloud import WordCloud
import random
import json
import re
import os
import io
import base64
from PIL import Image
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output, IFrame
import pycountry
import folium
from folium.plugins import HeatMap
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from openai import OpenAI
from google.colab import userdata

# Set up OpenAI client for advanced career insights (if available)
try:
    openai_client = OpenAI(
        api_key=userdata.get('OPENAI_API_KEY') or os.environ.get('OPENAI_API_KEY')
    )
    AI_INSIGHTS_AVAILABLE = True
except:
    AI_INSIGHTS_AVAILABLE = False
    print("AI insights not available - some advanced features will be limited")

class EnhancedCareerDashboard:
    def __init__(self, user_data=None, resume_data=None, job_application_data=None):
        """Initialize dashboard with comprehensive user data"""
        self.user_data = user_data or {}
        self.resume_data = resume_data or {}
        self.job_application_data = job_application_data or {}

        # Core tracking metrics
        self.resume_scores = []
        self.badges = []
        self.activity_log = []
        self.skill_progress = {}
        self.career_paths = {}
        self.learning_paths = []
        self.market_insights = {}
        self.wellbeing_metrics = {}
        self.interview_performances = []
        self.challenges = []
        self.user_level = 1
        self.experience_points = 0

        # Load demo data if no real data is available
        self._load_demo_data_if_needed()

        # Initialize career forecasting model
        self._initialize_career_prediction()

        # Track dashboard usage for gamification
        self.dashboard_opens = 0
        self.feature_usage = {}

    def _load_demo_data_if_needed(self):
        """Load comprehensive sample data for demonstration"""
        if not self.resume_scores:
            # Generate sample resume scores with more variation and longer history
            dates = [datetime.now() - timedelta(days=x*15) for x in range(8, 0, -1)]
            initial_score = random.randint(45, 55)
            scores = [
                initial_score,
                initial_score + random.randint(3, 7),
                initial_score + random.randint(8, 12),
                initial_score + random.randint(10, 15),
                initial_score + random.randint(13, 18),
                initial_score + random.randint(15, 20),
                initial_score + random.randint(18, 25),
                initial_score + random.randint(20, 30)
            ]
            self.resume_scores = [{"date": date, "score": score, "version": i+1}
                                 for i, (date, score) in enumerate(zip(dates, scores))]

        if not self.badges:
            # Enhanced badges with progression system and rarity levels
            self.badges = [
                {"name": "Profile Pioneer", "description": "Created your career profile", "earned": True,
                 "date": datetime.now() - timedelta(days=120), "rarity": "common", "xp": 50},
                {"name": "Resume Rookie", "description": "Uploaded your first resume", "earned": True,
                 "date": datetime.now() - timedelta(days=115), "rarity": "common", "xp": 50},
                {"name": "Keyword Conqueror", "description": "Added 20+ industry keywords to your resume", "earned": True,
                 "date": datetime.now() - timedelta(days=90), "rarity": "uncommon", "xp": 100},
                {"name": "Application Apprentice", "description": "Applied to your first job", "earned": True,
                 "date": datetime.now() - timedelta(days=85), "rarity": "common", "xp": 50},
                {"name": "ATS Adept", "description": "Achieved 70%+ ATS score", "earned": True,
                 "date": datetime.now() - timedelta(days=60), "rarity": "uncommon", "xp": 150},
                {"name": "Feedback Finder", "description": "Received AI feedback on 5+ resume versions", "earned": True,
                 "date": datetime.now() - timedelta(days=45), "rarity": "uncommon", "xp": 100},
                {"name": "Interview Initiate", "description": "Completed your first mock interview", "earned": True,
                 "date": datetime.now() - timedelta(days=30), "rarity": "uncommon", "xp": 100},
                {"name": "Persistent Pursuer", "description": "Applied to 10+ jobs", "earned": True,
                 "date": datetime.now() - timedelta(days=15), "rarity": "rare", "xp": 200},
                {"name": "ATS Ace", "description": "Achieved 85%+ ATS score", "earned": False,
                 "date": None, "rarity": "rare", "xp": 250},
                {"name": "Skill Savant", "description": "Achieved 80%+ in 5 different skills", "earned": False,
                 "date": None, "rarity": "rare", "xp": 250},
                {"name": "Interview Virtuoso", "description": "Scored 90%+ on 3 mock interviews", "earned": False,
                 "date": None, "rarity": "epic", "xp": 500},
                {"name": "Network Navigator", "description": "Connected with 20+ professionals in your field", "earned": False,
                 "date": None, "rarity": "rare", "xp": 300},
                {"name": "Offer Obtainer", "description": "Received your first job offer", "earned": False,
                 "date": None, "rarity": "epic", "xp": 1000},
                {"name": "Master Negotiator", "description": "Successfully negotiated job offer terms", "earned": False,
                 "date": None, "rarity": "legendary", "xp": 1500},
                {"name": "Career Composer", "description": "Crafted a comprehensive 5-year career plan", "earned": False,
                 "date": None, "rarity": "epic", "xp": 500},
                {"name": "Industry Innovator", "description": "Shared original content that received recognition", "earned": False,
                 "date": None, "rarity": "legendary", "xp": 2000}
            ]

        if not self.skill_progress:
            # Comprehensive skill progress data with historical tracking
            self.skill_progress = {
                "Technical Skills": {
                    "Python": {"current": 85, "history": [65, 70, 75, 85], "target": 95},
                    "Data Analysis": {"current": 70, "history": [50, 55, 65, 70], "target": 90},
                    "Machine Learning": {"current": 60, "history": [30, 40, 50, 60], "target": 85},
                    "Web Development": {"current": 45, "history": [30, 35, 40, 45], "target": 65},
                    "Cloud Computing": {"current": 30, "history": [10, 15, 25, 30], "target": 75},
                    "Database Management": {"current": 55, "history": [40, 45, 50, 55], "target": 80},
                    "DevOps": {"current": 25, "history": [10, 15, 20, 25], "target": 60}
                },
                "Soft Skills": {
                    "Communication": {"current": 75, "history": [65, 70, 73, 75], "target": 90},
                    "Teamwork": {"current": 80, "history": [70, 75, 78, 80], "target": 85},
                    "Problem Solving": {"current": 65, "history": [55, 58, 60, 65], "target": 85},
                    "Leadership": {"current": 50, "history": [35, 40, 45, 50], "target": 75},
                    "Time Management": {"current": 60, "history": [45, 50, 55, 60], "target": 80},
                    "Critical Thinking": {"current": 70, "history": [60, 63, 67, 70], "target": 85}
                },
                "Industry Knowledge": {
                    "AI Ethics": {"current": 55, "history": [30, 40, 50, 55], "target": 80},
                    "Data Privacy": {"current": 60, "history": [40, 45, 55, 60], "target": 75},
                    "Industry Trends": {"current": 65, "history": [45, 50, 60, 65], "target": 85},
                    "Competitive Analysis": {"current": 45, "history": [25, 30, 40, 45], "target": 70}
                }
            }

        if not self.activity_log:
            # Enhanced activity log with categories and impact scores
            activities = [
                {"activity": "Created profile", "category": "account", "impact": 1},
                {"activity": "Uploaded resume v1", "category": "resume", "impact": 2},
                {"activity": "Added skills to profile", "category": "profile", "impact": 1},
                {"activity": "Completed personality assessment", "category": "assessment", "impact": 2},
                {"activity": "Improved resume to v2 (ATS score: 53%)", "category": "resume", "impact": 3},
                {"activity": "Applied to Junior Data Analyst at TechCorp", "category": "application", "impact": 3},
                {"activity": "Completed Python basics course", "category": "learning", "impact": 2},
                {"activity": "Improved resume to v3 (ATS score: 61%)", "category": "resume", "impact": 3},
                {"activity": "Applied to Data Scientist at AI Solutions", "category": "application", "impact": 3},
                {"activity": "Completed SQL fundamentals course", "category": "learning", "impact": 2},
                {"activity": "Attended 'Breaking into AI' webinar", "category": "event", "impact": 1},
                {"activity": "Improved resume to v4 (ATS score: 67%)", "category": "resume", "impact": 3},
                {"activity": "Completed first mock interview (score: 72%)", "category": "interview", "impact": 4},
                {"activity": "Applied to ML Engineer at DataTech", "category": "application", "impact": 3},
                {"activity": "Connected with 3 industry professionals", "category": "networking", "impact": 2},
                {"activity": "Improved resume to v5 (ATS score: 72%)", "category": "resume", "impact": 3},
                {"activity": "Completed data visualization course", "category": "learning", "impact": 2},
                {"activity": "Received feedback on LinkedIn profile", "category": "feedback", "impact": 2},
                {"activity": "Applied to 3 positions at different companies", "category": "application", "impact": 4},
                {"activity": "Improved resume to v6 (ATS score: 78%)", "category": "resume", "impact": 3},
                {"activity": "Completed advanced Python for data science course", "category": "learning", "impact": 3},
                {"activity": "Completed second mock interview (score: 81%)", "category": "interview", "impact": 4},
                {"activity": "Created 5-year career development plan", "category": "planning", "impact": 3}
            ]

            # Distribute activities across time span, with more recent ones being closer together
            total_days = 120
            start_date = datetime.now() - timedelta(days=total_days)

            # Exponential distribution of activities to make them more frequent in recent times
            day_offsets = []
            for i in range(len(activities)):
                progress = i / (len(activities) - 1)  # 0 to 1
                # Use an exponential function to make recent activities more dense
                day_offset = total_days * (1 - np.exp(-5 * (1 - progress)))
                day_offsets.append(int(day_offset))

            # Sort day offsets and assign to activities
            day_offsets.sort(reverse=True)  # From largest (oldest) to smallest (newest)

            self.activity_log = [
                {"date": start_date + timedelta(days=total_days-offset),
                 "activity": act["activity"],
                 "category": act["category"],
                 "impact": act["impact"]}
                for act, offset in zip(activities, day_offsets)
            ]

            # Calculate and add XP points for each activity
            total_xp = 0
            for activity in self.activity_log:
                xp_earned = activity["impact"] * 25  # Impact 1-5 corresponds to 25-125 XP
                activity["xp_earned"] = xp_earned
                total_xp += xp_earned

            self.experience_points = total_xp
            self.user_level = 1 + (total_xp // 500)  # Level up every 500 XP

        if not self.career_paths:
            # Create example career path progression with decision points
            self.career_paths = {
                "Data Science": {
                    "entry_roles": ["Junior Data Analyst", "Data Visualization Specialist", "Research Assistant"],
                    "mid_roles": ["Data Scientist", "ML Engineer", "BI Developer"],
                    "advanced_roles": ["Lead Data Scientist", "AI Research Scientist", "Chief Data Officer"],
                    "connections": {
                        "Junior Data Analyst": ["Data Scientist", "BI Developer"],
                        "Data Visualization Specialist": ["BI Developer", "Data Scientist"],
                        "Research Assistant": ["Data Scientist", "ML Engineer"],
                        "Data Scientist": ["Lead Data Scientist", "AI Research Scientist"],
                        "ML Engineer": ["AI Research Scientist", "Lead Data Scientist"],
                        "BI Developer": ["Chief Data Officer", "Lead Data Scientist"],
                        "Lead Data Scientist": ["Chief Data Officer"],
                        "AI Research Scientist": ["Chief Data Officer"]
                    },
                    "current_position": "Junior Data Analyst",
                    "target_position": "Lead Data Scientist",
                    "progress": 25
                },
                "Software Engineering": {
                    "entry_roles": ["Junior Developer", "QA Tester", "Technical Support"],
                    "mid_roles": ["Software Engineer", "DevOps Engineer", "Full Stack Developer"],
                    "advanced_roles": ["Senior Software Engineer", "Software Architect", "CTO"],
                    "connections": {
                        "Junior Developer": ["Software Engineer", "Full Stack Developer"],
                        "QA Tester": ["Software Engineer", "DevOps Engineer"],
                        "Technical Support": ["DevOps Engineer", "Full Stack Developer"],
                        "Software Engineer": ["Senior Software Engineer", "Software Architect"],
                        "DevOps Engineer": ["Senior Software Engineer", "CTO"],
                        "Full Stack Developer": ["Senior Software Engineer", "Software Architect"],
                        "Senior Software Engineer": ["CTO"],
                        "Software Architect": ["CTO"]
                    },
                    "progress": 15
                }
            }

        if not self.learning_paths:
            # Create personalized learning paths with progress tracking
            self.learning_paths = [
                {
                    "name": "Data Science Specialist",
                    "description": "Master the skills required for data science roles",
                    "skills_covered": ["Python", "Data Analysis", "Machine Learning", "Statistics", "SQL"],
                    "courses": [
                        {"title": "Python for Data Science", "platform": "Coursera", "status": "completed", "completion_date": datetime.now() - timedelta(days=60)},
                        {"title": "SQL Fundamentals", "platform": "DataCamp", "status": "completed", "completion_date": datetime.now() - timedelta(days=45)},
                        {"title": "Statistics for Data Science", "platform": "edX", "status": "in_progress", "progress": 65},
                        {"title": "Machine Learning Fundamentals", "platform": "Udacity", "status": "not_started", "progress": 0},
                        {"title": "Deep Learning Specialization", "platform": "Coursera", "status": "not_started", "progress": 0}
                    ],
                    "progress": 42,
                    "estimated_completion": datetime.now() + timedelta(days=120)
                },
                {
                    "name": "Cloud Computing Professional",
                    "description": "Build expertise in cloud platforms and infrastructure",
                    "skills_covered": ["AWS", "Azure", "Cloud Architecture", "DevOps", "Containerization"],
                    "courses": [
                        {"title": "AWS Fundamentals", "platform": "Amazon", "status": "in_progress", "progress": 30},
                        {"title": "Docker and Kubernetes", "platform": "Pluralsight", "status": "not_started", "progress": 0},
                        {"title": "Cloud Architecture Patterns", "platform": "LinkedIn Learning", "status": "not_started", "progress": 0},
                        {"title": "DevOps with Cloud Platforms", "platform": "Udemy", "status": "not_started", "progress": 0}
                    ],
                    "progress": 15,
                    "estimated_completion": datetime.now() + timedelta(days=180)
                }
            ]

        if not self.market_insights:
            # Create market insights with regional demand and salary information
            self.market_insights = {
                "skills_demand": {
                    "Python": {"demand_score": 92, "growth_rate": 15, "job_count": 12500},
                    "Machine Learning": {"demand_score": 88, "growth_rate": 22, "job_count": 8700},
                    "JavaScript": {"demand_score": 90, "growth_rate": 10, "job_count": 15000},
                    "SQL": {"demand_score": 85, "growth_rate": 5, "job_count": 11000},
                    "AWS": {"demand_score": 87, "growth_rate": 18, "job_count": 9500},
                    "Data Analysis": {"demand_score": 83, "growth_rate": 12, "job_count": 7800},
                    "React": {"demand_score": 82, "growth_rate": 14, "job_count": 8200},
                    "DevOps": {"demand_score": 84, "growth_rate": 16, "job_count": 6500},
                    "Docker": {"demand_score": 80, "growth_rate": 17, "job_count": 5900},
                    "Cybersecurity": {"demand_score": 89, "growth_rate": 21, "job_count": 7200}
                },
                "regional_demand": {
                    "Dubai": {"demand_score": 90, "job_growth": 18, "top_skills": ["Machine Learning", "Python", "Cloud Computing"]},
                    "Abu Dhabi": {"demand_score": 85, "job_growth": 15, "top_skills": ["Cybersecurity", "Data Analysis", "Cloud Architecture"]},
                    "Sharjah": {"demand_score": 75, "job_growth": 10, "top_skills": ["Web Development", "Mobile Apps", "UI/UX Design"]},
                    "New York": {"demand_score": 92, "job_growth": 12, "top_skills": ["Machine Learning", "Python", "JavaScript"]},
                    "San Francisco": {"demand_score": 95, "job_growth": 14, "top_skills": ["AI", "Deep Learning", "Full Stack"]},
                    "London": {"demand_score": 88, "job_growth": 11, "top_skills": ["Data Science", "Fintech", "Blockchain"]},
                    "Singapore": {"demand_score": 87, "job_growth": 16, "top_skills": ["Cloud Computing", "Cybersecurity", "AI"]},
                    "Sydney": {"demand_score": 83, "job_growth": 9, "top_skills": ["DevOps", "Data Engineering", "Cloud Security"]}
                },
                "salary_insights": {
                    "Data Scientist": {"entry": 85000, "mid": 115000, "senior": 150000, "growth_rate": 12},
                    "Machine Learning Engineer": {"entry": 90000, "mid": 120000, "senior": 160000, "growth_rate": 14},
                    "Software Engineer": {"entry": 75000, "mid": 110000, "senior": 145000, "growth_rate": 10},
                    "DevOps Engineer": {"entry": 80000, "mid": 105000, "senior": 140000, "growth_rate": 13},
                    "Cloud Architect": {"entry": 95000, "mid": 125000, "senior": 165000, "growth_rate": 15},
                    "Cybersecurity Analyst": {"entry": 85000, "mid": 115000, "senior": 155000, "growth_rate": 16},
                    "Data Engineer": {"entry": 80000, "mid": 110000, "senior": 150000, "growth_rate": 13},
                    "Full Stack Developer": {"entry": 75000, "mid": 100000, "senior": 140000, "growth_rate": 11}
                },
                "industry_trends": [
                    {"trend": "AI Integration", "impact_score": 95, "timeline": "Current", "description": "AI technologies being integrated across all business functions"},
                    {"trend": "Remote Work", "impact_score": 90, "timeline": "Current", "description": "Continued shift to remote and hybrid work models"},
                    {"trend": "Cybersecurity Focus", "impact_score": 92, "timeline": "Current", "description": "Increased emphasis on data protection and security"},
                    {"trend": "Low-Code Development", "impact_score": 85, "timeline": "Emerging", "description": "Rise of low-code platforms for faster application development"},
                    {"trend": "Quantum Computing", "impact_score": 80, "timeline": "Future", "description": "Early adoption of quantum computing for specific use cases"},
                    {"trend": "Extended Reality", "impact_score": 78, "timeline": "Emerging", "description": "AR/VR applications expanding beyond gaming to enterprise use cases"}
                ]
            }

        if not self.wellbeing_metrics:
            # Create wellbeing metrics with simulated trend data
            self.wellbeing_metrics = {
                "job_satisfaction": {
                    "current": 72,
                    "history": [65, 68, 70, 72],
                    "goal": 85
                },
                "work_life_balance": {
                    "current": 68,
                    "history": [60, 62, 65, 68],
                    "goal": 80
                },
                "career_fulfillment": {
                    "current": 70,
                    "history": [62, 65, 68, 70],
                    "goal": 85
                },
                "stress_level": {
                    "current": 65,  # Higher = more stress
                    "history": [75, 72, 68, 65],
                    "goal": 50
                },
                "growth_opportunity": {
                    "current": 75,
                    "history": [60, 65, 70, 75],
                    "goal": 85
                }
            }

        if not self.interview_performances:
            # Create mock interview performance history
            self.interview_performances = [
                {"date": datetime.now() - timedelta(days=60), "position": "Junior Data Analyst", "company": "TechCorp", "score": 65, "strengths": ["Technical Knowledge", "Problem Solving"], "areas_to_improve": ["Communication", "Confidence"]},
                {"date": datetime.now() - timedelta(days=45), "position": "Data Scientist", "company": "AI Solutions", "score": 72, "strengths": ["Technical Knowledge", "Communication"], "areas_to_improve": ["Domain Knowledge", "Specific Examples"]},
                {"date": datetime.now() - timedelta(days=20), "position": "ML Engineer", "company": "DataTech", "score": 81, "strengths": ["Technical Knowledge", "Communication", "Problem Solving"], "areas_to_improve": ["System Design", "Specific Examples"]}
            ]

        if not self.challenges:
            # Create career development challenges with rewards
            self.challenges = [
                {"name": "Resume Makeover", "description": "Improve your resume ATS score by 10+ points", "status": "completed", "reward": 200, "completion_date": datetime.now() - timedelta(days=30), "difficulty": "medium"},
                {"name": "Skill Builder", "description": "Complete a course on an in-demand skill", "status": "completed", "reward": 150, "completion_date": datetime.now() - timedelta(days=15), "difficulty": "easy"},
                {"name": "Network Expander", "description": "Connect with 5 new professionals in your field", "status": "in_progress", "progress": 60, "reward": 200, "difficulty": "medium"},
                {"name": "Mock Interview Marathon", "description": "Complete 3 mock interviews", "status": "in_progress", "progress": 67, "reward": 300, "difficulty": "hard"},
                {"name": "Knowledge Sharer", "description": "Write an article on your area of expertise", "status": "not_started", "reward": 250, "difficulty": "medium"},
                {"name": "Project Showcase", "description": "Add a portfolio project demonstrating key skills", "status": "not_started", "reward": 300, "difficulty": "hard"}
            ]

    def _initialize_career_prediction(self):
        """Initialize AI-powered career trajectory prediction"""
        # Simulated career forecast data
        self.career_forecast = {
            "predicted_roles": [
                {"role": "Junior Data Analyst", "timeline": "Current", "probability": 95, "skill_match": 85},
                {"role": "Data Analyst", "timeline": "0-1 years", "probability": 85, "skill_match": 80},
                {"role": "Senior Data Analyst", "timeline": "1-2 years", "probability": 75, "skill_match": 70},
                {"role": "Data Scientist", "timeline": "2-3 years", "probability": 65, "skill_match": 60},
                {"role": "Senior Data Scientist", "timeline": "4-5 years", "probability": 55, "skill_match": 50},
                {"role": "Lead Data Scientist", "timeline": "5-7 years", "probability": 45, "skill_match": 40}
            ],
            "alternate_paths": [
                {"path": "Machine Learning Engineer", "compatibility": 80, "required_skills": ["Python", "Deep Learning", "MLOps"]},
                {"path": "Data Engineer", "compatibility": 75, "required_skills": ["SQL", "ETL", "Big Data"]},
                {"path": "Business Intelligence Analyst", "compatibility": 85, "required_skills": ["SQL", "Data Visualization", "Business Acumen"]}
            ],
            "market_alignment": 78,
            "skill_gaps": ["Deep Learning", "Cloud Platforms", "Production ML Systems"],
            "career_velocity": 7.5  # Career progression speed on scale of 1-10
        }

    def resume_score_trend(self):
        """Generate enhanced plot showing resume score improvement over time with annotations and insights"""
        if not self.resume_scores:
            return HTML("<p>No resume score data available yet.</p>")

        # Extract data
        dates = [score["date"] for score in self.resume_scores]
        scores = [score["score"] for score in self.resume_scores]
        versions = [score["version"] for score in self.resume_scores]

        # Calculate improvement metrics
        total_improvement = scores[-1] - scores[0]
        avg_improvement_per_version = total_improvement / (len(scores) - 1) if len(scores) > 1 else 0
        improvement_rate = total_improvement / (dates[-1] - dates[0]).days * 30 if len(dates) > 1 else 0  # Per month

        # Create figure with two rows: score trend and improvement rate
        fig = make_subplots(
            rows=2, cols=1,
            row_heights=[0.7, 0.3],
            subplot_titles=("Resume Score Improvement Over Time", "Improvement Rate"),
            vertical_spacing=0.1
        )

        # Add score trend line
        fig.add_trace(
            go.Scatter(
                x=dates,
                y=scores,
                mode='lines+markers',
                name='Resume Score',
                line=dict(color='royalblue', width=4),
                marker=dict(size=10),
                text=[f"V{v}: {s}%" for v, s in zip(versions, scores)],
                hovertemplate="<b>Version %{text}</b><br>Date: %{x|%b %d, %Y}<br>Score: %{y}%<extra></extra>"
            ),
            row=1, col=1
        )

        # Add goal line
        fig.add_shape(
            type="line",
            x0=min(dates),
            y0=85,
            x1=max(dates),
            y1=85,
            line=dict(
                color="green",
                width=2,
                dash="dash",
            ),
            row=1, col=1
        )

        fig.add_annotation(
            x=max(dates),
            y=85,
            text="Target Score (85%)",
            showarrow=False,
            yshift=10,
            font=dict(color="green"),
            row=1, col=1
        )

        # Add improvement annotations
        for i in range(1, len(scores)):
            if scores[i] - scores[i-1] >= 5:
                fig.add_annotation(
                    x=dates[i],
                    y=scores[i],
                    text=f"+{scores[i] - scores[i-1]:.1f}%",
                    showarrow=True,
                    arrowhead=1,
                    font=dict(color="green", size=10),
                    bgcolor="white",
                    bordercolor="green",
                    borderwidth=1,
                    row=1, col=1
                )

        # Add improvement rate bar chart
        if len(scores) > 1:
            improvements = [scores[i] - scores[i-1] for i in range(1, len(scores))]
            improvement_dates = [dates[i] for i in range(1, len(dates))]

            fig.add_trace(
                go.Bar(
                    x=improvement_dates,
                    y=improvements,
                    name="Improvement",
                    marker_color=[
                        'lightgreen' if imp > 0 else 'coral' for imp in improvements
                    ],
                    text=[f"+{imp:.1f}%" if imp > 0 else f"{imp:.1f}%" for imp in improvements],
                    textposition='auto',
                    hovertemplate="Improvement: %{y:.1f}%<extra></extra>"
                ),
                row=2, col=1
            )

        # Add insights annotation
        insights_text = f"Total improvement: +{total_improvement:.1f}%<br>Avg per version: +{avg_improvement_per_version:.1f}%<br>Monthly rate: +{improvement_rate:.1f}%"
        fig.add_annotation(
            x=0.02,
            y=0.97,
            xref="paper",
            yref="paper",
            text=insights_text,
            showarrow=False,
            font=dict(size=12),
            align="left",
            bgcolor="rgba(255, 255, 255, 0.8)",
            bordercolor="royalblue",
            borderwidth=1,
            borderpad=6,
            xanchor="left",
            yanchor="top"
        )

        # Predict future progress if continuing at same rate
        if len(scores) > 2:
            # Simple linear extrapolation
            from scipy import stats

            # Convert dates to days since first date for regression
            days_since_start = [(d - dates[0]).days for d in dates]
            slope, intercept, r_value, p_value, std_err = stats.linregress(days_since_start, scores)

            if slope > 0:  # Only predict if improvement trend is positive
                days_to_goal = (85 - scores[-1]) / slope if slope > 0 else float('inf')
                goal_date = dates[-1] + timedelta(days=days_to_goal) if days_to_goal < 365 else None

                if goal_date:
                    # Add prediction line
                    future_days = list(range(days_since_start[-1], days_since_start[-1] + int(days_to_goal) + 30))
                    future_scores = [intercept + slope * d for d in future_days]
                    future_dates = [dates[0] + timedelta(days=d) for d in future_days]

                    fig.add_trace(
                        go.Scatter(
                            x=future_dates,
                            y=future_scores,
                            mode='lines',
                            line=dict(dash='dot', color='rgba(65, 105, 225, 0.5)', width=3),
                            name='Projected Progress',
                            hovertemplate="Projected: %{y:.1f}%<extra></extra>"
                        ),
                        row=1, col=1
                    )

                    # Add target achievement annotation
                    fig.add_annotation(
                        x=goal_date,
                        y=85,
                        text=f"Goal achieved by<br>{goal_date.strftime('%b %d, %Y')}",
                        showarrow=True,
                        arrowhead=1,
                        font=dict(color="green"),
                        bgcolor="white",
                        row=1, col=1
                    )

        # Customize layout
        fig.update_layout(
            height=700,
            hovermode="x unified",
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            ),
            margin=dict(l=40, r=40, t=60, b=40)
        )

        fig.update_yaxes(title_text="ATS Score (%)", range=[min(min(scores)-5, 40), 100], row=1, col=1)
        fig.update_yaxes(title_text="Point Improvement", range=[0, max(5, max([scores[i] - scores[i-1] for i in range(1, len(scores))]) * 1.5)], row=2, col=1)
        fig.update_xaxes(title_text="Date", row=2, col=1)

        return fig

    def skills_radar_chart(self):
        """Generate interactive skills radar chart with multiple views and comparisons"""
        if not self.skill_progress:
            return HTML("<p>No skills data available yet.</p>")

        # Create dropdown to select different skill categories
        skill_categories = list(self.skill_progress.keys())
        category_dropdown = widgets.Dropdown(
            options=["All Skills"] + skill_categories,
            value="All Skills",
            description="Category:",
            style={'description_width': 'initial'}
        )

        # Create dropdown to select comparison view
        comparison_dropdown = widgets.Dropdown(
            options=["Current vs Target", "Current vs Market Demand", "Skill History"],
            value="Current vs Target",
            description="View:",
            style={'description_width': 'initial'}
        )

        # Output for the chart
        chart_output = widgets.Output()

            # Function to update chart based on selections
        def update_chart(category, comparison):
            with chart_output:
                clear_output(wait=True)

                # Get skills based on selected category
                if category == "All Skills":
                    # Get top skills across all categories
                    all_skills = {}
                    for cat, skills in self.skill_progress.items():
                        for skill, data in skills.items():
                            all_skills[skill] = data

                    # Sort skills by current value and select top 8
                    selected_skills = dict(sorted(all_skills.items(),
                                                 key=lambda item: item[1]['current'],
                                                 reverse=True)[:8])
                else:
                    # Get skills for selected category
                    selected_skills = self.skill_progress[category]

                # Create figure based on comparison view
                if comparison == "Current vs Target":
                    # Radar chart comparing current skills with target levels
                    categories = list(selected_skills.keys())
                    current_values = [selected_skills[skill]['current'] for skill in categories]
                    target_values = [selected_skills[skill]['target'] for skill in categories]

                    # Close the loop for the radar chart
                    categories.append(categories[0])
                    current_values.append(current_values[0])
                    target_values.append(target_values[0])

                    fig = go.Figure()

                    fig.add_trace(go.Scatterpolar(
                        r=current_values,
                        theta=categories,
                        fill='toself',
                        name='Current Skills',
                        line_color='rgba(65, 105, 225, 0.8)',
                        fillcolor='rgba(65, 105, 225, 0.3)'
                    ))

                    fig.add_trace(go.Scatterpolar(
                        r=target_values,
                        theta=categories,
                        fill='toself',
                        name='Target Skills',
                        line_color='rgba(34, 139, 34, 0.8)',
                        fillcolor='rgba(34, 139, 34, 0.1)'
                    ))

                elif comparison == "Current vs Market Demand":
                    # Simulated market demand data (in real implementation this would come from job market analysis)
                    market_demand = {}
                    for skill in selected_skills:
                        # Generate realistic market demand value that's related to but different from target
                        base_demand = min(selected_skills[skill]['target'] + random.randint(-10, 15), 100)
                        market_demand[skill] = max(base_demand, 50)  # Market demand is typically at least moderate for skills we track

                    categories = list(selected_skills.keys())
                    current_values = [selected_skills[skill]['current'] for skill in categories]
                    demand_values = [market_demand[skill] for skill in categories]

                    # Close the loop for the radar chart
                    categories.append(categories[0])
                    current_values.append(current_values[0])
                    demand_values.append(demand_values[0])

                    fig = go.Figure()

                    fig.add_trace(go.Scatterpolar(
                        r=current_values,
                        theta=categories,
                        fill='toself',
                        name='Current Skills',
                        line_color='rgba(65, 105, 225, 0.8)',
                        fillcolor='rgba(65, 105, 225, 0.3)'
                    ))

                    fig.add_trace(go.Scatterpolar(
                        r=demand_values,
                        theta=categories,
                        fill='toself',
                        name='Market Demand',
                        line_color='rgba(220, 20, 60, 0.8)',
                        fillcolor='rgba(220, 20, 60, 0.1)'
                    ))

                elif comparison == "Skill History":
                    # For skill history, use a line chart instead of radar
                    fig = go.Figure()

                    # Get history lengths - assume all skills have same history length
                    first_skill = list(selected_skills.keys())[0]
                    history_length = len(selected_skills[first_skill]['history'])
                    timestamps = [f"T-{history_length-i}" for i in range(history_length)] + ["Current"]

                    colors = px.colors.qualitative.Plotly
                    for i, (skill, data) in enumerate(selected_skills.items()):
                        color_index = i % len(colors)

                        # Combine history with current value
                        skill_values = data['history'] + [data['current']]

                        fig.add_trace(go.Scatter(
                            x=timestamps,
                            y=skill_values,
                            mode='lines+markers',
                            name=skill,
                            line=dict(color=colors[color_index], width=3),
                            marker=dict(size=8)
                        ))

                    fig.update_layout(
                        title=f"Skill Development History ({category})",
                        xaxis_title="Time Point",
                        yaxis_title="Skill Level (%)",
                        yaxis=dict(range=[0, 100]),
                        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                        height=500
                    )

                    display(fig)
                    return

                # Update layout for radar charts
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 100]
                        )
                    ),
                    showlegend=True,
                    title=f"Skills Analysis: {category}" if category != "All Skills" else "Top Skills Analysis",
                    height=500
                )

                display(fig)

        # Function to handle dropdown changes
        def on_dropdown_change(change):
            if change['type'] == 'change' and change['name'] == 'value':
                update_chart(category_dropdown.value, comparison_dropdown.value)

        # Register callbacks
        category_dropdown.observe(on_dropdown_change)
        comparison_dropdown.observe(on_dropdown_change)

        # Create widget layout
        controls = widgets.HBox([category_dropdown, comparison_dropdown],
                               layout=widgets.Layout(width='100%', justify_content='space-between'))

        # Initial chart
        update_chart(category_dropdown.value, comparison_dropdown.value)

        # Return complete widget
        return widgets.VBox([controls, chart_output])

    def career_path_visualization(self):
        """Visualize career path options as an interactive network graph"""
        # We'll use the first career path as default
        career_key = list(self.career_paths.keys())[0]
        career_path = self.career_paths[career_key]

        # Create a dropdown to select different career paths
        path_dropdown = widgets.Dropdown(
            options=list(self.career_paths.keys()),
            value=career_key,
            description='Career Path:',
            style={'description_width': 'initial'}
        )

        # Output for the visualization
        visual_output = widgets.Output()

        # Function to update visualization
        def update_visualization(path_key):
            with visual_output:
                clear_output(wait=True)

                career_path = self.career_paths[path_key]

                # Create a directed graph
                G = nx.DiGraph()

                # Add nodes for all roles
                all_roles = (career_path.get('entry_roles', []) +
                            career_path.get('mid_roles', []) +
                            career_path.get('advanced_roles', []))

                for role in all_roles:
                    G.add_node(role)

                # Add edges from the connections
                for source, targets in career_path.get('connections', {}).items():
                    for target in targets:
                        G.add_edge(source, target)

                # Create position layout using hierarchical structure
                pos = {}

                # Organize roles by level
                entry_y = 0
                mid_y = 1
                advanced_y = 2

                for i, role in enumerate(career_path.get('entry_roles', [])):
                    pos[role] = (i - len(career_path.get('entry_roles', [])) / 2, entry_y)

                for i, role in enumerate(career_path.get('mid_roles', [])):
                    pos[role] = (i - len(career_path.get('mid_roles', [])) / 2, mid_y)

                for i, role in enumerate(career_path.get('advanced_roles', [])):
                    pos[role] = (i - len(career_path.get('advanced_roles', [])) / 2, advanced_y)

                # Create Plotly figure
                edge_traces = []
                for edge in G.edges():
                    x0, y0 = pos[edge[0]]
                    x1, y1 = pos[edge[1]]

                    edge_trace = go.Scatter(
                        x=[x0, x1, None],
                        y=[y0, y1, None],
                        line=dict(width=2, color='rgba(150,150,150,0.7)'),
                        hoverinfo='none',
                        mode='lines'
                    )
                    edge_traces.append(edge_trace)

                # Create node traces for each level (for different colors)
                node_traces = []

                # Current and target positions get special colors
                current_position = career_path.get('current_position', None)
                target_position = career_path.get('target_position', None)

                # Helper function to create node trace
                def create_node_trace(nodes, color, level_name):
                    node_x = []
                    node_y = []
                    node_text = []
                    node_size = []

                    for node in nodes:
                        x, y = pos[node]
                        node_x.append(x)
                        node_y.append(y)
                        node_text.append(node)

                        # Make current and target positions larger
                        if node == current_position:
                            node_size.append(25)  # Current position
                        elif node == target_position:
                            node_size.append(20)  # Target position
                        else:
                            node_size.append(15)  # Regular node

                    return go.Scatter(
                        x=node_x,
                        y=node_y,
                        mode='markers',
                        hoverinfo='text',
                        text=node_text,
                        marker=dict(
                            color=color,
                            size=node_size,
                            line=dict(width=2, color='white')
                        ),
                        name=level_name
                    )

                # Create node traces by level
                entry_nodes = create_node_trace(
                    career_path.get('entry_roles', []),
                    'rgba(31, 119, 180, 0.8)',
                    'Entry Level'
                )
                node_traces.append(entry_nodes)

                mid_nodes = create_node_trace(
                    career_path.get('mid_roles', []),
                    'rgba(255, 127, 14, 0.8)',
                    'Mid Level'
                )
                node_traces.append(mid_nodes)

                advanced_nodes = create_node_trace(
                    career_path.get('advanced_roles', []),
                    'rgba(44, 160, 44, 0.8)',
                    'Advanced Level'
                )
                node_traces.append(advanced_nodes)

                # Create figure
                fig = go.Figure(data=edge_traces + node_traces)

                # Add level labels
                level_labels = [
                    dict(x=-3, y=entry_y, text="Entry Level", showarrow=False, font=dict(size=14)),
                    dict(x=-3, y=mid_y, text="Mid Level", showarrow=False, font=dict(size=14)),
                    dict(x=-3, y=advanced_y, text="Advanced Level", showarrow=False, font=dict(size=14))
                ]

                # Add current and target annotations if present
                if current_position:
                    x, y = pos[current_position]
                    level_labels.append(
                        dict(x=x, y=y+0.15, text="CURRENT", showarrow=False,
                             font=dict(size=10, color='blue'))
                    )

                if target_position:
                    x, y = pos[target_position]
                    level_labels.append(
                        dict(x=x, y=y+0.15, text="TARGET", showarrow=False,
                             font=dict(size=10, color='green'))
                    )

                fig.update_layout(
                    title=f"Career Progression Map: {path_key}",
                    showlegend=True,
                    hovermode='closest',
                    annotations=level_labels,
                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    height=600,
                    margin=dict(b=20, l=20, r=20, t=40)
                )

                display(fig)

                # Show progression metrics if available
                if 'progress' in career_path:
                    progress = career_path['progress']

                    progress_html = f"""
                    <div style="margin-top: 20px; padding: 15px; background-color: #f9f9f9; border-radius: 10px; border: 1px solid #ddd;">
                        <h4 style="margin-top: 0; color: #333;">Career Progression: {path_key}</h4>
                        <div style="background-color: #eee; height: 25px; border-radius: 5px; margin: 15px 0;">
                            <div style="background-color: #4CAF50; width: {progress}%; height: 100%; border-radius: 5px; text-align: center; line-height: 25px; color: white;">
                                {progress}%
                            </div>
                        </div>
                        <p style="margin-bottom: 5px;"><strong>Estimated time to next level:</strong> {int(16 - (progress/100)*16)} months</p>
                    </div>
                    """

                    display(HTML(progress_html))

        # Register callback for dropdown
        def on_path_change(change):
            if change['type'] == 'change' and change['name'] == 'value':
                update_visualization(change['new'])

        path_dropdown.observe(on_path_change)

        # Initial visualization
        update_visualization(path_dropdown.value)

        return widgets.VBox([path_dropdown, visual_output])

    def learning_path_tracker(self):
        """Display interactive learning path progress tracking"""
        if not self.learning_paths:
            return HTML("<p>No learning paths available yet.</p>")

        # Create tabs for each learning path
        tab_titles = [path['name'] for path in self.learning_paths]
        tab_children = []

        for path in self.learning_paths:
            # Create output for this path
            path_output = widgets.Output()

            with path_output:
                # Create path header with progress bar
                progress = path['progress']

                header_html = f"""
                <div style="padding: 15px; background-color: #f5f9ff; border-radius: 10px; margin-bottom: 20px; border: 1px solid #cce5ff;">
                    <h3 style="margin-top: 0; color: #0062cc;">{path['name']}</h3>
                    <p style="color: #555;">{path['description']}</p>
                    <div style="margin: 15px 0;">
                        <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                            <span><strong>Progress:</strong> {progress}%</span>
                            <span><strong>Est. Completion:</strong> {path.get('estimated_completion', 'Unknown').strftime('%b %d, %Y') if isinstance(path.get('estimated_completion'), datetime) else 'Unknown'}</span>
                        </div>
                        <div style="background-color: #e9ecef; height: 20px; border-radius: 5px;">
                            <div style="background-color: #007bff; width: {progress}%; height: 100%; border-radius: 5px;"></div>
                        </div>
                    </div>
                    <div style="margin-top: 10px;">
                        <strong>Skills covered:</strong> {', '.join(path.get('skills_covered', []))}
                    </div>
                </div>
                """
                display(HTML(header_html))

                # Create course list
                courses_html = """
                <div style="padding: 15px; background-color: #fff; border-radius: 10px; border: 1px solid #ddd;">
                    <h4 style="margin-top: 0; color: #333;">Learning Path Curriculum</h4>
                    <table style="width: 100%; border-collapse: collapse; margin-top: 10px;">
                        <tr style="background-color: #f2f2f2;">
                            <th style="padding: 10px; text-align: left; border-bottom: 1px solid #ddd;">Course</th>
                            <th style="padding: 10px; text-align: left; border-bottom: 1px solid #ddd;">Platform</th>
                            <th style="padding: 10px; text-align: left; border-bottom: 1px solid #ddd;">Status</th>
                            <th style="padding: 10px; text-align: left; border-bottom: 1px solid #ddd;">Progress</th>
                        </tr>
                """

                # Status icon mapping
                status_icons = {
                    "completed": "✅",
                    "in_progress": "🔄",
                    "not_started": "⏳"
                }

                # Add rows for each course
                for course in path.get('courses', []):
                    status = course.get('status', 'not_started')
                    icon = status_icons.get(status, "⏳")

                    # Format completion or progress info
                    if status == "completed" and 'completion_date' in course:
                        completion_info = course['completion_date'].strftime('%b %d, %Y')
                    elif status == "in_progress" and 'progress' in course:
                        completion_info = f"{course['progress']}%"
                    else:
                        completion_info = "-"

                    courses_html += f"""
                        <tr>
                            <td style="padding: 10px; border-bottom: 1px solid #eee;">{course['title']}</td>
                            <td style="padding: 10px; border-bottom: 1px solid #eee;">{course.get('platform', '-')}</td>
                            <td style="padding: 10px; border-bottom: 1px solid #eee;">{icon} {status.replace('_', ' ').title()}</td>
                            <td style="padding: 10px; border-bottom: 1px solid #eee;">{completion_info}</td>
                        </tr>
                    """

                courses_html += """
                    </table>
                </div>
                """
                display(HTML(courses_html))

                # Create visualization of skill improvements
                # Simple skill impact visualization
                display(HTML("<h4 style='margin-top: 20px;'>Projected Skill Improvements</h4>"))

                # Generate projected skill improvements based on learning path
                skill_impacts = {}
                for skill in path.get('skills_covered', []):
                    # If skill exists in our skill progress, use that as base
                    # Otherwise create a reasonable estimate
                    base_level = 0
                    for category, skills in self.skill_progress.items():
                        if skill in skills:
                            base_level = skills[skill]['current']
                            break

                    if base_level == 0:
                        base_level = random.randint(20, 40)  # Simulate a starting level

                    # Estimate improvement based on learning path progress
                    improvement = min(30, 100 - base_level)  # Max 30% improvement, can't exceed 100%
                    projected_level = min(100, base_level + (improvement * (path['progress'] / 100)))
                    remaining_improvement = min(100, base_level + improvement) - projected_level

                    skill_impacts[skill] = {
                        'base': base_level,
                        'current': projected_level,
                        'target': min(100, base_level + improvement),
                        'remaining': remaining_improvement
                    }

                # Create horizontal bar chart for skill improvements
                fig = go.Figure()

                skills = list(skill_impacts.keys())
                current_levels = [skill_impacts[skill]['current'] for skill in skills]
                remaining = [skill_impacts[skill]['remaining'] for skill in skills]
                base_levels = [skill_impacts[skill]['base'] for skill in skills]

                # Add base level bars
                fig.add_trace(go.Bar(
                    y=skills,
                    x=base_levels,
                    name='Starting Level',
                    orientation='h',
                    marker=dict(color='lightgray')
                ))

                # Add current progress bars
                fig.add_trace(go.Bar(
                    y=skills,
                    x=[(current - base) for current, base in zip(current_levels, base_levels)],
                    name='Current Improvement',
                    orientation='h',
                    marker=dict(color='#4CAF50'),
                    base=base_levels
                ))

                # Add remaining progress bars
                fig.add_trace(go.Bar(
                    y=skills,
                    x=remaining,
                    name='Remaining Potential',
                    orientation='h',
                    marker=dict(color='#2196F3', opacity=0.5),
                    base=current_levels
                ))

                fig.update_layout(
                    barmode='stack',
                    title='Skill Development Projection',
                    xaxis=dict(
                        title='Skill Level (%)',
                        range=[0, 100]
                    ),
                    yaxis=dict(
                        title='Skill'
                    ),
                    legend=dict(
                        orientation="h",
                        yanchor="bottom",
                        y=1.02,
                        xanchor="right",
                        x=1
                    ),
                    height=400,
                    margin=dict(l=20, r=20, t=40, b=20)
                )

                display(fig)

            tab_children.append(path_output)

        # Create tabs
        tabs = widgets.Tab()
        tabs.children = tab_children
        for i, title in enumerate(tab_titles):
            tabs.set_title(i, title)

        return tabs

    def activity_timeline(self):
        """Generate an interactive timeline visualization of user activities"""
        if not self.activity_log:
            return HTML("<p>No activity data available yet.</p>")

        # Sort activities by date
        sorted_activities = sorted(self.activity_log, key=lambda x: x['date'])

        # Group activities by month
        from collections import defaultdict
        activities_by_month = defaultdict(list)

        for activity in sorted_activities:
            month_key = activity['date'].strftime('%Y-%m')
            activities_by_month[month_key].append(activity)

        # Create HTML for the timeline
        timeline_html = """
        <div style="font-family: Arial, sans-serif; max-width: 900px; margin: 0 auto;">
            <h3 style="color: #333; margin-bottom: 20px;">Career Activity Timeline</h3>
            <div style="position: relative;">
                <!-- Timeline line -->
                <div style="position: absolute; width: 4px; background-color: #3498db; top: 0; bottom: 0; left: 20px; border-radius: 2px;"></div>
        """

        # Create dots/items for each month
        months = sorted(activities_by_month.keys())
        for month in months:
            month_activities = activities_by_month[month]
            month_date = datetime.strptime(month, '%Y-%m')
            month_str = month_date.strftime('%B %Y')

            timeline_html += f"""
                <!-- Month marker -->
                <div style="position: relative; margin-bottom: 30px; padding-left: 50px;">
                    <!-- Month dot -->
                    <div style="position: absolute; width: 20px; height: 20px; background-color: #3498db; border-radius: 50%; left: 12px; top: 5px;"></div>

                    <!-- Month header -->
                    <h4 style="margin-top: 0; margin-bottom: 15px; color: #3498db;">{month_str}</h4>

                    <!-- Month activities -->
                    <div style="margin-left: 10px;">
            """

            # Add activities for this month
            for activity in month_activities:
                day_str = activity['date'].strftime('%d')
                category = activity.get('category', 'general')
                impact = activity.get('impact', 1)

                # Category-specific colors and icons
                category_styles = {
                    'account': {'color': '#95a5a6', 'icon': '👤'},
                    'resume': {'color': '#3498db', 'icon': '📄'},
                    'profile': {'color': '#9b59b6', 'icon': '🔍'},
                    'assessment': {'color': '#f39c12', 'icon': '📊'},
                    'application': {'color': '#2ecc71', 'icon': '📩'},
                    'learning': {'color': '#e74c3c', 'icon': '📚'},
                    'event': {'color': '#1abc9c', 'icon': '📆'},
                    'interview': {'color': '#d35400', 'icon': '🗣️'},
                    'networking': {'color': '#16a085', 'icon': '🔗'},
                    'feedback': {'color': '#f1c40f', 'icon': '💬'},
                    'planning': {'color': '#8e44ad', 'icon': '🗓️'}
                }

                style = category_styles.get(category, {'color': '#7f8c8d', 'icon': '🔷'})

                # Impact stars
                impact_stars = '⭐' * impact

                timeline_html += f"""
                        <div style="position: relative; margin-bottom: 15px; padding: 10px 15px; background-color: #f8f9fa; border-left: 4px solid {style['color']}; border-radius: 4px; box-shadow: 0 1px 3px rgba(0,0,0,0.1);">
                            <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                                <span style="font-weight: bold; color: {style['color']};">{style['icon']} {day_str}</span>
                                <span style="color: #7f8c8d; font-size: 0.9em;">{impact_stars}</span>
                            </div>
                            <p style="margin: 0;">{activity['activity']}</p>
                            {f'<p style="margin: 5px 0 0; font-size: 0.8em; color: #7f8c8d;">+{activity.get("xp_earned", 0)} XP</p>' if 'xp_earned' in activity else ''}
                        </div>
                """

            timeline_html += """
                    </div>
                </div>
            """

        timeline_html += """
            </div>
        </div>
        """

        # Add activity analytics at the top
        activity_counts = {}
        for activity in self.activity_log:
            category = activity.get('category', 'general')
            activity_counts[category] = activity_counts.get(category, 0) + 1

        # Create category distribution bar chart
        fig = px.bar(
            x=list(activity_counts.keys()),
            y=list(activity_counts.values()),
            labels={'x': 'Activity Type', 'y': 'Count'},
            title='Activity Distribution',
            color=list(activity_counts.keys()),
            color_discrete_sequence=px.colors.qualitative.Bold
        )

        fig.update_layout(
            xaxis_title='Activity Category',
            yaxis_title='Number of Activities',
            height=300,
            margin=dict(l=40, r=40, t=60, b=80)
        )

        # Create analysis output
        analytics_output = widgets.Output()
        with analytics_output:
            display(fig)

            # Calculate some analytics
            total_activities = len(self.activity_log)
            start_date = min(a['date'] for a in self.activity_log)
            end_date = max(a['date'] for a in self.activity_log)
            date_range = (end_date - start_date).days
            avg_activities_per_week = total_activities / (date_range / 7)

            consistency_score = min(100, int(avg_activities_per_week * 20))  # Score based on activity frequency

            analytics_html = f"""
            <div style="display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 20px;">
                <div style="flex: 1; min-width: 200px; padding: 15px; background-color: #f8f9fa; border-radius: 10px; border: 1px solid #dee2e6;">
                    <h4 style="margin-top: 0; color: #333;">Activity Summary</h4>
                    <ul style="padding-left: 20px; margin-bottom: 0;">
                        <li>Total activities: <strong>{total_activities}</strong></li>
                        <li>Active days: <strong>{len(set(a['date'].date() for a in self.activity_log))}</strong></li>
                        <li>Time span: <strong>{date_range} days</strong></li>
                        <li>Avg. per week: <strong>{avg_activities_per_week:.1f}</strong></li>
                    </ul>
                </div>

                <div style="flex: 1; min-width: 200px; padding: 15px; background-color: #f8f9fa; border-radius: 10px; border: 1px solid #dee2e6;">
                    <h4 style="margin-top: 0; color: #333;">Consistency Score</h4>
                    <div style="margin: 15px 0;">
                        <div style="background-color: #e9ecef; height: 20px; border-radius: 5px;">
                            <div style="background-color: {'#28a745' if consistency_score >= 70 else '#ffc107' if consistency_score >= 40 else '#dc3545'}; width: {consistency_score}%; height: 100%; border-radius: 5px;"></div>
                        </div>
                        <div style="display: flex; justify-content: space-between; margin-top: 5px; font-size: 0.8em; color: #6c757d;">
                            <span>Low</span>
                            <span>Moderate</span>
                            <span>High</span>
                        </div>
                    </div>
                    <p style="margin-bottom: 0; font-size: 0.9em; color: #6c757d;">
                        Consistency is key to career success. Your score is based on the frequency and regularity of your career-building activities.
                    </p>
                </div>
            </div>
            """

            display(HTML(analytics_html))

        # Create tabs for analytics and timeline
        tab1 = widgets.Output()
        tab2 = widgets.Output()

        with tab1:
            display(analytics_output)

        with tab2:
            display(HTML(timeline_html))

        tabs = widgets.Tab()
        tabs.children = [tab1, tab2]
        tabs.set_title(0, 'Activity Analytics')
        tabs.set_title(1, 'Activity Timeline')

        return tabs

    def generate_badge_html(self, badge, size="medium"):
        """Generate HTML for a badge with specified size"""
        # Size classes
        sizes = {
            "small": {"icon": "30px", "width": "120px", "font": "0.8em"},
            "medium": {"icon": "40px", "width": "180px", "font": "0.9em"},
            "large": {"icon": "60px", "width": "220px", "font": "1em"}
        }

        size_class = sizes.get(size, sizes["medium"])

        # Rarity colors and icons
        rarity_styles = {
            "common": {"color": "#3498db", "border": "#2980b9", "icon": "🥉"},
            "uncommon": {"color": "#2ecc71", "border": "#27ae60", "icon": "🥈"},
            "rare": {"color": "#9b59b6", "border": "#8e44ad", "icon": "🥇"},
            "epic": {"color": "#f39c12", "border": "#e67e22", "icon": "🏆"},
            "legendary": {"color": "#e74c3c", "border": "#c0392b", "icon": "👑"}
        }

        rarity = badge.get("rarity", "common")
        style = rarity_styles.get(rarity, rarity_styles["common"])

        earned = badge.get("earned", False)
        opacity = "1" if earned else "0.4"
        filter_style = "" if earned else "grayscale(100%)"
        lockIcon = "" if earned else '<div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 20px;">🔒</div>'

        earned_date = badge.get("date", "")
        if earned and earned_date:
            date_str = f"Earned: {earned_date.strftime('%b %d, %Y')}"
        else:
            date_str = "Not yet earned"

        html = f"""
        <div style="width: {size_class['width']}; display: inline-block; margin: 10px; vertical-align: top; opacity: {opacity}; filter: {filter_style};">
            <div style="position: relative; background: linear-gradient(135deg, {style['color']}40, {style['color']}80); border: 2px solid {style['border']}; border-radius: 10px; padding: 15px; height: 100%; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                {lockIcon}
                <div style="font-size: {size_class['icon']}; text-align: center; margin-bottom: 10px;">{style['icon']}</div>
                <h4 style="margin: 0; color: {style['border']}; text-align: center; font-size: {size_class['font']};">{badge['name']}</h4>
                <p style="font-size: calc({size_class['font']} - 0.1em); color: #333; text-align: center; margin: 5px 0;">{badge['description']}</p>
                <p style="font-size: calc({size_class['font']} - 0.2em); color: #777; text-align: center; margin-top: 8px; margin-bottom: 0;">{date_str}</p>
                <div style="text-align: center; margin-top: 5px; font-size: calc({size_class['font']} - 0.2em); color: {style['border']};">
                    {rarity.upper()} • {badge.get('xp', 0)} XP
                </div>
            </div>
        </div>
        """

        return html

    def badges_display(self):
        """Display earned badges and upcoming achievements with filtering and sorting"""
        earned_badges = [b for b in self.badges if b.get("earned", False)]
        upcoming_badges = [b for b in self.badges if not b.get("earned", False)]

        # Create tabs
        tab_titles = ["All Badges", "Earned", "Available", "By Rarity"]
        tab_outputs = [widgets.Output() for _ in range(len(tab_titles))]

        # All badges tab
        with tab_outputs[0]:
            all_badges_html = "<div style='display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;'>"

                    # Sort badges by rarity and earned status
            rarity_order = {"legendary": 0, "epic": 1, "rare": 2, "uncommon": 3, "common": 4}
            sorted_badges = sorted(self.badges,
                                  key=lambda b: (not b.get("earned", False),
                                               rarity_order.get(b.get("rarity", "common"), 5)))

            for badge in sorted_badges:
                all_badges_html += self.generate_badge_html(badge)

            all_badges_html += "</div>"
            display(HTML(all_badges_html))

            # Add badges summary
            badge_counts = {
                "total": len(self.badges),
                "earned": len(earned_badges),
                "remaining": len(upcoming_badges),
                "by_rarity": {}
            }

            for rarity in ["common", "uncommon", "rare", "epic", "legendary"]:
                total = len([b for b in self.badges if b.get("rarity") == rarity])
                earned = len([b for b in earned_badges if b.get("rarity") == rarity])
                badge_counts["by_rarity"][rarity] = {"total": total, "earned": earned}

            total_xp = sum(b.get("xp", 0) for b in earned_badges)
            remaining_xp = sum(b.get("xp", 0) for b in upcoming_badges)

            summary_html = f"""
            <div style="margin-top: 30px; padding: 15px; background-color: #f8f9fa; border-radius: 10px; max-width: 600px; margin-left: auto; margin-right: auto;">
                <h3 style="margin-top: 0; color: #2c3e50; text-align: center;">Achievement Progress</h3>

                <div style="display: flex; justify-content: space-around; margin-bottom: 20px; flex-wrap: wrap;">
                    <div style="text-align: center; margin: 10px;">
                        <div style="font-size: 1.8em; font-weight: bold; color: #3498db;">{badge_counts["earned"]}</div>
                        <div style="color: #7f8c8d;">Earned</div>
                    </div>
                    <div style="text-align: center; margin: 10px;">
                        <div style="font-size: 1.8em; font-weight: bold; color: #e74c3c;">{badge_counts["remaining"]}</div>
                        <div style="color: #7f8c8d;">Remaining</div>
                    </div>
                    <div style="text-align: center; margin: 10px;">
                        <div style="font-size: 1.8em; font-weight: bold; color: #f39c12;">{total_xp}</div>
                        <div style="color: #7f8c8d;">XP Earned</div>
                    </div>
                </div>

                <div style="margin-bottom: 15px;">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                        <span>Completion Progress</span>
                        <span>{badge_counts["earned"]}/{badge_counts["total"]} ({int(badge_counts["earned"]/badge_counts["total"]*100)}%)</span>
                    </div>
                    <div style="height: 10px; background-color: #ecf0f1; border-radius: 5px; overflow: hidden;">
                        <div style="height: 100%; width: {int(badge_counts["earned"]/badge_counts["total"]*100)}%; background-color: #3498db;"></div>
                    </div>
                </div>

                <h4 style="margin-bottom: 10px; color: #2c3e50;">By Rarity</h4>
                <div style="display: flex; flex-wrap: wrap; gap: 10px;">
            """

            rarity_colors = {
                "common": "#3498db",
                "uncommon": "#2ecc71",
                "rare": "#9b59b6",
                "epic": "#f39c12",
                "legendary": "#e74c3c"
            }

            for rarity, counts in badge_counts["by_rarity"].items():
                if counts["total"] > 0:
                    summary_html += f"""
                    <div style="flex: 1; min-width: 100px; padding: 10px; background-color: {rarity_colors[rarity]}20; border-radius: 5px; border: 1px solid {rarity_colors[rarity]};">
                        <div style="font-weight: bold; color: {rarity_colors[rarity]}; text-transform: capitalize;">{rarity}</div>
                        <div style="font-size: 0.9em; color: #7f8c8d;">{counts["earned"]}/{counts["total"]} earned</div>
                        <div style="height: 5px; background-color: #ecf0f1; border-radius: 5px; margin-top: 5px; overflow: hidden;">
                            <div style="height: 100%; width: {int(counts["earned"]/counts["total"]*100) if counts["total"] > 0 else 0}%; background-color: {rarity_colors[rarity]};"></div>
                        </div>
                    </div>
                    """

            summary_html += """
                </div>
            </div>
            """

            display(HTML(summary_html))

        # Earned badges tab
        with tab_outputs[1]:
            if earned_badges:
                earned_html = "<div style='display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;'>"

                # Sort earned badges by date (newest first)
                sorted_earned = sorted(earned_badges, key=lambda b: b.get("date", datetime.now()), reverse=True)

                for badge in sorted_earned:
                    earned_html += self.generate_badge_html(badge)

                earned_html += "</div>"
                display(HTML(earned_html))

                # Add timeline visualization
                timeline_html = """
                <div style="margin-top: 40px; max-width: 800px; margin-left: auto; margin-right: auto;">
                    <h3 style="color: #2c3e50; text-align: center;">Achievement Timeline</h3>
                    <div style="position: relative; margin-top: 30px;">
                        <!-- Timeline Line -->
                        <div style="position: absolute; top: 0; bottom: 0; left: 50%; width: 4px; background-color: #3498db; z-index: 1;"></div>
                """

                # Group badges by month
                from collections import defaultdict
                badges_by_month = defaultdict(list)

                for badge in sorted_earned:
                    if badge.get("date"):
                        month_key = badge["date"].strftime('%Y-%m')
                        badges_by_month[month_key].append(badge)

                # Add badges to timeline
                for i, (month, month_badges) in enumerate(sorted(badges_by_month.items(), reverse=True)):
                    month_date = datetime.strptime(month, '%Y-%m')
                    month_str = month_date.strftime('%B %Y')

                    timeline_html += f"""
                        <div style="position: relative; margin-bottom: 40px; z-index: 2;">
                            <!-- Month marker -->
                            <div style="position: absolute; left: 50%; transform: translateX(-50%); background-color: #3498db; color: white; padding: 5px 15px; border-radius: 20px; font-weight: bold;">{month_str}</div>

                            <!-- Badges container -->
                            <div style="display: flex; flex-wrap: wrap; margin-top: 30px;">
                    """

                    # Alternate badge layout left and right of timeline
                    for j, badge in enumerate(month_badges):
                        align_right = j % 2 == 0

                        timeline_html += f"""
                                <div style="width: 50%; padding: 10px; box-sizing: border-box; {'margin-left: 50%;' if align_right else ''}">
                                    <div style="position: relative; background-color: white; padding: 15px; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1); {'margin-left: 10px;' if align_right else 'margin-right: 10px;'}">
                                        <!-- Connection dot -->
                                        <div style="position: absolute; width: 12px; height: 12px; background-color: #3498db; border-radius: 50%; top: 50%; {f'left: -6px;' if align_right else 'right: -6px;'}"></div>

                                        <!-- Connection line -->
                                        <div style="position: absolute; height: 2px; background-color: #3498db; top: 50%; width: 10px; {f'left: -10px;' if align_right else 'right: -10px;'}"></div>

                                        <div style="display: flex; align-items: center;">
                                            <div style="margin-right: 15px; font-size: 24px;">{badge.get('rarity_styles', {}).get('icon', '🏆')}</div>
                                            <div>
                                                <h4 style="margin: 0; color: #2c3e50;">{badge['name']}</h4>
                                                <p style="margin: 5px 0 0; font-size: 0.9em; color: #7f8c8d;">{badge['description']}</p>
                                                <p style="margin: 5px 0 0; font-size: 0.8em; color: #95a5a6;">{badge['date'].strftime('%d %b %Y')}</p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                        """

                    timeline_html += """
                            </div>
                        </div>
                    """

                timeline_html += """
                    </div>
                </div>
                """

                display(HTML(timeline_html))
            else:
                display(HTML("<div style='text-align: center; padding: 30px;'><h3>No badges earned yet</h3><p>Complete activities to earn badges and track your career progress!</p></div>"))

        # Available badges tab (upcoming)
        with tab_outputs[2]:
            if upcoming_badges:
                # Group badges by rarity for better organization
                badges_by_rarity = defaultdict(list)
                for badge in upcoming_badges:
                    rarity = badge.get("rarity", "common")
                    badges_by_rarity[rarity].append(badge)

                # Display badges grouped by rarity
                for rarity in ["common", "uncommon", "rare", "epic", "legendary"]:
                    if rarity in badges_by_rarity:
                        rarity_color = {
                            "common": "#3498db",
                            "uncommon": "#2ecc71",
                            "rare": "#9b59b6",
                            "epic": "#f39c12",
                            "legendary": "#e74c3c"
                        }.get(rarity, "#7f8c8d")

                        display(HTML(f"<h3 style='margin-top: 20px; color: {rarity_color}; text-transform: capitalize;'>{rarity} Badges</h3>"))

                        rarity_html = "<div style='display: flex; flex-wrap: wrap; gap: 10px; justify-content: flex-start;'>"
                        for badge in badges_by_rarity[rarity]:
                            rarity_html += self.generate_badge_html(badge)
                        rarity_html += "</div>"

                        display(HTML(rarity_html))
            else:
                display(HTML("<div style='text-align: center; padding: 30px;'><h3>All badges earned!</h3><p>Congratulations! You've earned all available badges.</p></div>"))

        # By rarity tab
        with tab_outputs[3]:
            # Create a sunburst chart of badge distribution by rarity
            rarity_counts = defaultdict(lambda: {"earned": 0, "remaining": 0})

            for badge in self.badges:
                rarity = badge.get("rarity", "common")
                if badge.get("earned", False):
                    rarity_counts[rarity]["earned"] += 1
                else:
                    rarity_counts[rarity]["remaining"] += 1

            # Prepare data for sunburst chart
            labels = ["Badges"]
            parents = [""]
            values = [0]  # This root value will be replaced by the sum
            colors = ["#ffffff"]  # Transparent for root

            for rarity in ["common", "uncommon", "rare", "epic", "legendary"]:
                if rarity in rarity_counts:
                    # Add rarity node
                    labels.append(f"{rarity.capitalize()}")
                    parents.append("Badges")
                    rarity_total = rarity_counts[rarity]["earned"] + rarity_counts[rarity]["remaining"]
                    values.append(rarity_total)
                    colors.append({
                        "common": "#3498db80",
                        "uncommon": "#2ecc7180",
                        "rare": "#9b59b680",
                        "epic": "#f39c1280",
                        "legendary": "#e74c3c80"
                    }.get(rarity, "#7f8c8d80"))

                    # Add earned/remaining nodes
                    if rarity_counts[rarity]["earned"] > 0:
                        labels.append(f"{rarity.capitalize()} (Earned)")
                        parents.append(f"{rarity.capitalize()}")
                        values.append(rarity_counts[rarity]["earned"])
                        colors.append({
                            "common": "#3498db",
                            "uncommon": "#2ecc71",
                            "rare": "#9b59b6",
                            "epic": "#f39c12",
                            "legendary": "#e74c3c"
                        }.get(rarity, "#7f8c8d"))

                    if rarity_counts[rarity]["remaining"] > 0:
                        labels.append(f"{rarity.capitalize()} (Remaining)")
                        parents.append(f"{rarity.capitalize()}")
                        values.append(rarity_counts[rarity]["remaining"])
                        colors.append("#95a5a680")  # Light gray with transparency

            # Create sunburst chart
            fig = go.Figure(go.Sunburst(
                labels=labels,
                parents=parents,
                values=values,
                branchvalues="total",
                marker=dict(
                    colors=colors
                ),
                hovertemplate='<b>%{label}</b><br>Count: %{value}<br>Percentage: %{percentEntry:.1%}<extra></extra>',
                textfont=dict(
                    family="Arial",
                    size=14
                )
            ))

            fig.update_layout(
                margin=dict(t=10, b=10, r=10, l=10),
                height=500,
                title_text="Badge Distribution by Rarity",
                title_x=0.5,
                paper_bgcolor='rgba(0,0,0,0)',
                plot_bgcolor='rgba(0,0,0,0)'
            )

            display(fig)

            # Add rarity explanation
            rarity_explanation = """
            <div style="max-width: 700px; margin: 30px auto; padding: 15px; background-color: #f8f9fa; border-radius: 10px;">
                <h3 style="margin-top: 0; color: #2c3e50;">Badge Rarity Levels</h3>
                <ul style="list-style-type: none; padding: 0;">
                    <li style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="width: 20px; height: 20px; background-color: #3498db; border-radius: 50%; margin-right: 10px;"></div>
                        <div>
                            <h4 style="margin: 0; color: #3498db;">Common (🥉)</h4>
                            <p style="margin: 5px 0 0; color: #7f8c8d;">Basic achievements that mark your first steps in the career journey.</p>
                        </div>
                    </li>
                    <li style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="width: 20px; height: 20px; background-color: #2ecc71; border-radius: 50%; margin-right: 10px;"></div>
                        <div>
                            <h4 style="margin: 0; color: #2ecc71;">Uncommon (🥈)</h4>
                            <p style="margin: 5px 0 0; color: #7f8c8d;">Achievements that require consistent effort and dedication.</p>
                        </div>
                    </li>
                    <li style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="width: 20px; height: 20px; background-color: #9b59b6; border-radius: 50%; margin-right: 10px;"></div>
                        <div>
                            <h4 style="margin: 0; color: #9b59b6;">Rare (🥇)</h4>
                            <p style="margin: 5px 0 0; color: #7f8c8d;">Significant achievements that demonstrate advanced proficiency.</p>
                        </div>
                    </li>
                    <li style="display: flex; align-items: center; margin-bottom: 15px;">
                        <div style="width: 20px; height: 20px; background-color: #f39c12; border-radius: 50%; margin-right: 10px;"></div>
                        <div>
                            <h4 style="margin: 0; color: #f39c12;">Epic (🏆)</h4>
                            <p style="margin: 5px 0 0; color: #7f8c8d;">Exceptional achievements that put you ahead of most career seekers.</p>
                        </div>
                    </li>
                    <li style="display: flex; align-items: center;">
                        <div style="width: 20px; height: 20px; background-color: #e74c3c; border-radius: 50%; margin-right: 10px;"></div>
                        <div>
                            <h4 style="margin: 0; color: #e74c3c;">Legendary (👑)</h4>
                            <p style="margin: 5px 0 0; color: #7f8c8d;">The rarest achievements that represent mastery and exceptional career accomplishments.</p>
                        </div>
                    </li>
                </ul>
            </div>
            """

            display(HTML(rarity_explanation))

        # Create tabs
        tabs = widgets.Tab()
        tabs.children = tab_outputs
        for i, title in enumerate(tab_titles):
            tabs.set_title(i, title)

        return tabs

    def job_market_insights(self):
        """Generate interactive job market insights visualization"""
        # Create simulated job market data
        job_trends = {
            "Data Scientist": [120, 135, 155, 180, 210, 250, 275],
            "Machine Learning Engineer": [80, 95, 120, 145, 185, 225, 260],
            "Data Engineer": [100, 110, 130, 155, 190, 220, 240],
            "AI Research Scientist": [40, 45, 55, 70, 90, 115, 145],
            "Business Intelligence Analyst": [150, 160, 170, 165, 175, 180, 190],
            "Data Analyst": [200, 210, 225, 235, 250, 260, 270],
            "Cloud Solutions Architect": [90, 110, 135, 160, 185, 205, 240]
        }

        # Time periods (quarters)
        periods = [f"Q{i} {2023 if i>2 else 2022}" for i in range(1, 8)]

        # Salary ranges by role
        salary_data = {
            "Data Scientist": {"min": 85000, "max": 150000, "avg": 115000},
            "Machine Learning Engineer": {"min": 95000, "max": 165000, "avg": 130000},
            "Data Engineer": {"min": 90000, "max": 155000, "avg": 120000},
            "AI Research Scientist": {"min": 110000, "max": 180000, "avg": 140000},
            "Business Intelligence Analyst": {"min": 70000, "max": 125000, "avg": 95000},
            "Data Analyst": {"min": 65000, "max": 115000, "avg": 85000},
            "Cloud Solutions Architect": {"min": 100000, "max": 170000, "avg": 135000}
        }

        # Required skills by role with demand score
        skills_demand = {
            "Python": {"demand": 95, "roles": ["Data Scientist", "Machine Learning Engineer", "Data Engineer", "AI Research Scientist"]},
            "SQL": {"demand": 90, "roles": ["Data Scientist", "Data Engineer", "Business Intelligence Analyst", "Data Analyst"]},
            "Machine Learning": {"demand": 85, "roles": ["Data Scientist", "Machine Learning Engineer", "AI Research Scientist"]},
            "Deep Learning": {"demand": 80, "roles": ["Machine Learning Engineer", "AI Research Scientist"]},
            "Data Visualization": {"demand": 75, "roles": ["Data Scientist", "Business Intelligence Analyst", "Data Analyst"]},
            "Big Data": {"demand": 80, "roles": ["Data Engineer", "Data Scientist"]},
            "Cloud Platforms": {"demand": 85, "roles": ["Cloud Solutions Architect", "Data Engineer", "Machine Learning Engineer"]},
            "Spark": {"demand": 70, "roles": ["Data Engineer", "Data Scientist"]},
            "Natural Language Processing": {"demand": 75, "roles": ["Machine Learning Engineer", "AI Research Scientist"]},
            "Docker/Kubernetes": {"demand": 80, "roles": ["Cloud Solutions Architect", "Data Engineer", "Machine Learning Engineer"]},
            "Statistics": {"demand": 70, "roles": ["Data Scientist", "Data Analyst", "AI Research Scientist"]},
            "R": {"demand": 60, "roles": ["Data Scientist", "Data Analyst"]},
            "Java": {"demand": 65, "roles": ["Data Engineer", "Machine Learning Engineer"]},
            "Tableau/Power BI": {"demand": 75, "roles": ["Business Intelligence Analyst", "Data Analyst"]},
            "Communication": {"demand": 85, "roles": ["All"]},
            "Problem Solving": {"demand": 90, "roles": ["All"]}
        }

        # Create tabs for different market insights
        tab_titles = ["Job Trends", "Salary Insights", "Skills Demand", "Market Fit"]
        tab_outputs = [widgets.Output() for _ in range(len(tab_titles))]

        # Job Trends tab
        with tab_outputs[0]:
            # Create interactive job trends visualization
            fig = go.Figure()

            for job, trend in job_trends.items():
                fig.add_trace(go.Scatter(
                    x=periods,
                    y=trend,
                    mode='lines+markers',
                    name=job,
                    hovertemplate='%{y} listings<extra>%{fullData.name}</extra>'
                ))

            fig.update_layout(
                title="Job Market Trends (Number of Listings)",
                xaxis_title="Time Period",
                yaxis_title="Number of Job Listings",
                height=500,
                hovermode="x unified",
                legend=dict(
                    orientation="h",
                    yanchor="bottom",
                    y=1.02,
                    xanchor="right",
                    x=1
                )
            )

            display(fig)

            # Add regional distribution visualization (heatmap)
            # Simulated regional job distribution data
            regions = ["West Coast", "East Coast", "Midwest", "South", "Remote"]
            regional_data = {
                "Data Scientist": [30, 25, 10, 15, 20],
                "Machine Learning Engineer": [35, 20, 5, 10, 30],
                "Data Engineer": [25, 30, 15, 15, 15],
                "AI Research Scientist": [40, 35, 5, 5, 15],
                "Business Intelligence Analyst": [20, 30, 20, 20, 10],
                "Data Analyst": [20, 25, 20, 20, 15],
                "Cloud Solutions Architect": [25, 25, 15, 15, 20]
            }

            # Convert to format for heatmap
            x_labels = list(regional_data.keys())
            y_labels = regions
            z_values = []

            for region in regions:
                row = []
                for job in x_labels:
                    row.append(regional_data[job][regions.index(region)])
                z_values.append(row)

            # Create heatmap
            fig = go.Figure(data=go.Heatmap(
                z=z_values,
                x=x_labels,
                y=y_labels,
                colorscale='Blues',
                text=[[f"{val}%" for val in row] for row in z_values],
                texttemplate="%{text}",
                textfont={"size":12},
                hovertemplate='Region: %{y}<br>Role: %{x}<br>Distribution: %{z}%<extra></extra>'
            ))

            fig.update_layout(
                title="Job Distribution by Region (%)",
                height=400,
                margin=dict(l=50, r=20, t=50, b=80)
            )

            display(HTML("<h3 style='margin-top: 30px;'>Regional Distribution</h3>"))
            display(fig)

            # Add insights about job market trends
            growth_rates = {}
            for job, trend in job_trends.items():
                growth_rates[job] = round((trend[-1] - trend[0]) / trend[0] * 100, 1)

            # Sort jobs by growth rate
            top_growth = dict(sorted(growth_rates.items(), key=lambda x: x[1], reverse=True))

            insights_html = """
            <div style="margin-top: 30px; max-width: 800px; padding: 20px; background-color: #f8f9fa; border-radius: 10px;">
                <h3 style="margin-top: 0; color: #2c3e50;">Key Market Insights</h3>

                <h4 style="color: #3498db; margin-bottom: 10px;">Fastest Growing Roles</h4>
                <div style="display: flex; flex-wrap: wrap; gap: 15px; margin-bottom: 20px;">
            """

            # Add top growth roles
            for i, (job, growth) in enumerate(list(top_growth.items())[:3]):
                insights_html += f"""
                <div style="flex: 1; min-width: 200px; padding: 15px; background-color: white; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                    <h5 style="margin-top: 0; margin-bottom: 10px; color: #2c3e50;">{job}</h5>
                    <div style="font-size: 1.5em; font-weight: bold; color: #27ae60;">+{growth}%</div>
                    <div style="font-size: 0.9em; color: #7f8c8d;">Growth rate</div>
                </div>
                """

            insights_html += """
                </div>

                <h4 style="color: #3498db; margin-bottom: 10px;">Market Trends</h4>
                <ul style="margin-bottom: 20px;">
            """

            # Add general insights
            insights = [
                "AI and machine learning roles continue to show the strongest growth across all technology sectors.",
                "Remote work opportunities have increased by 30% compared to pre-pandemic levels.",
                "Hybrid technical/business roles are emerging as companies seek employees who can bridge technical implementation with business strategy.",
                "Entry-level data positions are seeing increased competition, with employers favoring candidates who demonstrate practical project experience."
            ]

            for insight in insights:
                insights_html += f"<li style='margin-bottom: 8px;'>{insight}</li>"

            insights_html += """
                </ul>

                <h4 style="color: #3498db; margin-bottom: 10px;">Recommendations</h4>
                <ul>
            """

            # Add recommendations
            recommendations = [
                "Focus on developing specialized skills in high-growth areas like MLOps, NLP, or cloud infrastructure.",
                "Build a portfolio that demonstrates end-to-end project implementation rather than just technical skills.",
                "Consider obtaining certifications in cloud platforms which are increasingly required by employers.",
                "Develop communication skills to effectively translate technical concepts to business stakeholders."
            ]

            for recommendation in recommendations:
                insights_html += f"<li style='margin-bottom: 8px;'>{recommendation}</li>"

            insights_html += """
                </ul>
            </div>
            """

            display(HTML(insights_html))

        # Salary Insights tab
        with tab_outputs[1]:
            # ... (existing code remains unchanged)
            pass

        # Skills Demand tab
        with tab_outputs[2]:
            # ... (existing code remains unchanged)
            pass

        # Market Fit tab
        with tab_outputs[3]:
            # ... (existing code remains unchanged)
            pass

        # Create tabs layout
        tabs = widgets.Tab()
        tabs.children = tab_outputs
        for i, title in enumerate(tab_titles):
            tabs.set_title(i, title)

        return tabs

    def market_insights_dashboard(self):
        """Create comprehensive market insights dashboard"""
        return self.job_market_insights()

    def resume_score_chart(self):
        """Generate a chart showing resume score trends over time"""
        # Prepare data for the chart
        dates = [score["date"] for score in self.resume_scores]
        scores = [score["score"] for score in self.resume_scores]

        # Create the figure
        fig = go.Figure()

        # Add score line
        fig.add_trace(go.Scatter(
            x=dates,
            y=scores,
            mode='lines+markers',
            name='Resume Score',
            line=dict(color='#3498db', width=3),
            marker=dict(size=8)
        ))

        # Add threshold lines
        fig.add_shape(
            type="line",
            x0=min(dates),
            y0=80,
            x1=max(dates),
            y1=80,
            line=dict(color="#2ecc71", width=2, dash="dash"),
        )

        fig.add_shape(
            type="line",
            x0=min(dates),
            y0=65,
            x1=max(dates),
            y1=65,
            line=dict(color="#f39c12", width=2, dash="dash"),
        )

        # Add annotations for thresholds
        fig.add_annotation(
            x=max(dates),
            y=82,
            text="Excellent (80%+)",
            showarrow=False,
            font=dict(color="#2ecc71")
        )

        fig.add_annotation(
            x=max(dates),
            y=67,
            text="Good (65%+)",
            showarrow=False,
            font=dict(color="#f39c12")
        )

        # Update layout
        fig.update_layout(
            title=None,
            xaxis_title="Date",
            yaxis_title="Score (%)",
            yaxis=dict(range=[0, 100]),
            hovermode="x unified",
            margin=dict(l=20, r=20, t=20, b=20),
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
        )

        return fig

    def improvement_recommendations(self):
        """Generate personalized improvement recommendations"""
        # Create more personalized recommendations based on user data
        resume_score = self.resume_scores[-1]['score'] if self.resume_scores else 50

        # Get skills data
        skill_levels = {}
        for category, skills in self.skill_progress.items():
            for skill, data in skills.items():
                skill_levels[skill] = data['current']

        low_skills = [skill for skill, level in skill_levels.items() if level < 50]
        medium_skills = [skill for skill, level in skill_levels.items() if 50 <= level < 75]
        high_skills = [skill for skill, level in skill_levels.items() if level >= 75]

        # Recommendation categories
        html = """
        <div style="font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto;">
            <h3 style="color: #2c3e50; margin-bottom: 20px;">Personalized Improvement Recommendations</h3>
        """

        # Resume recommendations
        html += """
            <div style="background-color: #f8f9fa; border-left: 5px solid #3498db; padding: 15px; margin-bottom: 20px; border-radius: 0 5px 5px 0;">
                <h4 style="color: #3498db; margin-top: 0;">Resume Enhancement</h4>
                <ul style="margin-bottom: 0;">
        """

        if resume_score < 65:
            html += """
                    <li>Add more industry-specific keywords to improve ATS compatibility</li>
                    <li>Quantify your achievements with metrics and specific outcomes</li>
                    <li>Review and eliminate grammatical errors and typos</li>
                    <li>Ensure your resume follows a consistent format and structure</li>
            """
        elif resume_score < 80:
            html += """
                    <li>Tailor your experience descriptions to highlight skills relevant to your target roles</li>
                    <li>Add more quantifiable achievements with specific metrics</li>
                    <li>Consider adding a brief professional summary highlighting your unique value proposition</li>
                    <li>Optimize section headings for better ATS recognition</li>
            """
        else:
            html += """
                    <li>Fine-tune your resume for specific job descriptions by customizing keywords</li>
                    <li>Consider creating role-specific versions of your resume for different types of applications</li>
                    <li>Highlight leadership experiences and strategic contributions</li>
                    <li>Add links to your professional portfolio or projects if applicable</li>
            """

        html += """
                </ul>
            </div>
        """

        # Skill development recommendations
        html += """
            <div style="background-color: #f8f9fa; border-left: 5px solid #2ecc71; padding: 15px; margin-bottom: 20px; border-radius: 0 5px 5px 0;">
                <h4 style="color: #2ecc71; margin-top: 0;">Skill Development</h4>
                <ul style="margin-bottom: 0;">
        """

        if low_skills:
            html += f"""
                    <li>Focus on developing your foundational skills in: {', '.join(low_skills[:3])}</li>
            """

        if medium_skills:
            html += f"""
                    <li>Take intermediate or advanced courses to strengthen: {', '.join(medium_skills[:3])}</li>
            """

        if high_skills:
            html += f"""
                    <li>Showcase your expertise in {', '.join(high_skills[:3])} through projects or certifications</li>
            """

        # Add general skill development recommendations
        html += """
                    <li>Join relevant communities or forums to stay updated with industry trends</li>
                    <li>Consider pursuing certifications that validate your technical skills</li>
                    <li>Work on side projects that demonstrate practical application of your skills</li>
                </ul>
            </div>
        """

        # Job search strategy recommendations
        html += """
            <div style="background-color: #f8f9fa; border-left: 5px solid #9b59b6; padding: 15px; margin-bottom: 20px; border-radius: 0 5px 5px 0;">
                <h4 style="color: #9b59b6; margin-top: 0;">Job Search Strategy</h4>
                <ul style="margin-bottom: 0;">
                    <li>Set up job alerts on multiple platforms with specific keywords</li>
                    <li>Research companies that align with your career goals and values</li>
                    <li>Customize your cover letter for each application to highlight relevant experience</li>
                    <li>Prepare a 30-60-90 day plan for interviews to demonstrate your strategic thinking</li>
                    <li>Follow up after applications and interviews to demonstrate interest and professionalism</li>
                </ul>
            </div>
        """

        # Networking recommendations
        html += """
            <div style="background-color: #f8f9fa; border-left: 5px solid #f39c12; padding: 15px; border-radius: 0 5px 5px 0;">
                <h4 style="color: #f39c12; margin-top: 0;">Networking & Personal Brand</h4>
                <ul style="margin-bottom: 0;">
                    <li>Optimize your LinkedIn profile with relevant keywords and a compelling summary</li>
                    <li>Connect with professionals in your target companies or roles</li>
                    <li>Share and engage with industry content to increase your visibility</li>
                    <li>Consider attending virtual or in-person industry events and conferences</li>
                    <li>Request informational interviews to learn more about specific companies or roles</li>
                </ul>
            </div>
        """

        html += "</div>"

        return HTML(html)

    def generate_dashboard(self):
        """Generate comprehensive career dashboard with all components"""
        # Calculate user level and XP from activities and badges
        for badge in self.badges:
            if badge.get("earned", False):
                self.experience_points += badge.get("xp", 0)

        # Simple level calculation based on XP
        self.user_level = 1 + self.experience_points // 500  # Level up every 500 XP

        # Create tabs for different dashboard sections
        tab_outputs = [widgets.Output() for _ in range(5)]
        overview_tab, skills_tab, achievements_tab, activity_tab, insights_tab = tab_outputs

        # Fill overview tab
        with overview_tab:
                    # Welcome message with latest score and user level
            latest_score = self.resume_scores[-1]["score"] if self.resume_scores else 0
            earned_badge_count = len([b for b in self.badges if b["earned"]])

            # User level display with progress bar
            next_level_xp = (self.user_level) * 500
            previous_level_xp = (self.user_level - 1) * 500
            xp_progress = min(100, int((self.experience_points - previous_level_xp) / (next_level_xp - previous_level_xp) * 100))

            html = f"""
            <div style="font-family: Arial, sans-serif; padding: 20px; background: linear-gradient(to right, #667eea, #764ba2); border-radius: 15px; margin-bottom: 30px; color: white;">
                <div style="display: flex; align-items: center; margin-bottom: 20px;">
                    <div style="background-color: #8E44AD; width: 80px; height: 80px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 20px; font-size: 2em; box-shadow: 0 4px 8px rgba(0,0,0,0.2);">
                        {self.user_level}
                    </div>
                    <div>
                        <h2 style="margin: 0; font-size: 1.8em;">Career Navigator</h2>
                        <div style="display: flex; align-items: center; margin-top: 10px;">
                            <div style="flex-grow: 1; margin-right: 15px;">
                                <div style="background-color: rgba(255,255,255,0.3); height: 10px; border-radius: 5px; width: {xp_progress}%;"></div>
                            </div>
                            <div style="font-size: 0.9em;">{self.experience_points - previous_level_xp}/{next_level_xp - previous_level_xp} XP</div>
                        </div>
                    </div>
                </div>
                <div style="font-size: 0.9em; opacity: 0.9;">You've earned {self.experience_points} XP and {earned_badge_count} badges on your career journey!</div>
            </div>

            <div style="display: flex; flex-wrap: wrap; gap: 20px; margin-bottom: 30px;">
                <!-- Resume Score Card -->
                <div style="flex: 1; min-width: 250px; background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px; position: relative; overflow: hidden;">
                    <div style="position: absolute; right: -15px; top: -15px; background-color: {'#2ecc71' if latest_score >= 80 else '#f39c12' if latest_score >= 65 else '#e74c3c'}; width: 80px; height: 80px; border-radius: 50%; display: flex; align-items: center; justify-content: center; opacity: 0.2;"></div>
                    <h3 style="margin-top: 0; color: #2c3e50;">Resume Score</h3>
                    <div style="font-size: 3em; font-weight: bold; color: {'#2ecc71' if latest_score >= 80 else '#f39c12' if latest_score >= 65 else '#e74c3c'};">{latest_score}%</div>
                    <div style="font-size: 0.9em; color: #7f8c8d; margin-top: 5px;">
                        {'+' + str(latest_score - self.resume_scores[-2]["score"]) + '%' if len(self.resume_scores) > 1 else 'First version'} since last update
                    </div>
                    <div style="height: 6px; background-color: #ecf0f1; border-radius: 3px; margin-top: 15px;">
                        <div style="height: 100%; width: {latest_score}%; background-color: {'#2ecc71' if latest_score >= 80 else '#f39c12' if latest_score >= 65 else '#e74c3c'}; border-radius: 3px;"></div>
                    </div>
                </div>

                <!-- Achievements Card -->
                <div style="flex: 1; min-width: 250px; background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px;">
                    <h3 style="margin-top: 0; color: #2c3e50;">Achievements</h3>
                    <div style="display: flex; justify-content: space-between; margin-bottom: 15px;">
                        <div>
                            <div style="font-size: 2em; font-weight: bold; color: #3498db;">{earned_badge_count}</div>
                            <div style="font-size: 0.9em; color: #7f8c8d;">Badges Earned</div>
                        </div>
                        <div>
                            <div style="font-size: 2em; font-weight: bold; color: #9b59b6;">{len(self.badges) - earned_badge_count}</div>
                            <div style="font-size: 0.9em; color: #7f8c8d;">Available</div>
                        </div>
                    </div>
                    <div style="height: 6px; background-color: #ecf0f1; border-radius: 3px;">
                        <div style="height: 100%; width: {int(earned_badge_count/len(self.badges)*100)}%; background-color: #3498db; border-radius: 3px;"></div>
                    </div>
                    <div style="font-size: 0.8em; color: #7f8c8d; text-align: right; margin-top: 5px;">
                        {earned_badge_count}/{len(self.badges)} ({int(earned_badge_count/len(self.badges)*100)}%)
                    </div>
                </div>

                <!-- Activity Card -->
                <div style="flex: 1; min-width: 250px; background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px;">
                    <h3 style="margin-top: 0; color: #2c3e50;">Recent Activity</h3>
                    <div style="margin-top: 10px;">
                        {
                            ''.join([
                                f'<div style="display: flex; margin-bottom: 10px; align-items: center;">'
                                f'<div style="width: 8px; height: 8px; border-radius: 50%; background-color: #3498db; margin-right: 10px;"></div>'
                                f'<div style="flex-grow: 1; font-size: 0.9em;">{activity["activity"]}</div>'
                                f'<div style="font-size: 0.8em; color: #7f8c8d;">{(datetime.now() - activity["date"]).days}d ago</div>'
                                f'</div>'
                                for activity in sorted(self.activity_log, key=lambda x: x["date"], reverse=True)[:3]
                            ]) if self.activity_log else '<div style="color: #7f8c8d; font-style: italic;">No recent activity</div>'
                        }
                    </div>
                    <div style="text-align: right; margin-top: 10px;">
                        <a href="#" style="color: #3498db; text-decoration: none; font-size: 0.9em;">View all activities →</a>
                    </div>
                </div>
            </div>

            <!-- Resume Score Trend Chart -->
            <div style="background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px; margin-bottom: 30px;">
                <h3 style="margin-top: 0; color: #2c3e50;">Resume Score Trend</h3>
                <div style="height: 400px;">
            """

            display(HTML(html))

            # Generate and display resume score chart
            if self.resume_scores:
                fig = self.resume_score_chart()
                display(fig)
            else:
                display(HTML("<p style='text-align: center; color: #7f8c8d;'>No resume score data available yet</p>"))

            # Close the container div
            display(HTML("</div></div>"))

            # Skills Overview
            html = """
            <div style="background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px; margin-bottom: 30px;">
                <h3 style="margin-top: 0; color: #2c3e50;">Skills Overview</h3>
                <div style="margin-top: 15px;">
            """

            display(HTML(html))

            # Create simplified skills radar for overview
            if self.skill_progress:
                # Get top skills across all categories
                all_skills = {}
                for cat, skills in self.skill_progress.items():
                    for skill, data in skills.items():
                        all_skills[skill] = data["current"]

                # Select top 8 skills
                top_skills = dict(sorted(all_skills.items(), key=lambda x: x[1], reverse=True)[:8])

                # Create radar chart
                categories = list(top_skills.keys())
                values = list(top_skills.values())

                # Close the loop for radar chart
                categories.append(categories[0])
                values.append(values[0])

                fig = go.Figure()

                fig.add_trace(go.Scatterpolar(
                    r=values,
                    theta=categories,
                    fill='toself',
                    name='Current Skills',
                    line_color='rgba(65, 105, 225, 0.8)',
                    fillcolor='rgba(65, 105, 225, 0.3)'
                ))

                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 100]
                        )
                    ),
                    showlegend=False,
                    height=350,
                    margin=dict(l=40, r=40, t=20, b=40)
                )

                display(fig)
            else:
                display(HTML("<p style='text-align: center; color: #7f8c8d;'>No skills data available yet</p>"))

            # Close the container div
            display(HTML("</div></div>"))

            # Job Market Match
            html = """
            <div style="background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px; margin-bottom: 30px;">
                <h3 style="margin-top: 0; color: #2c3e50;">Job Market Match</h3>
            """

            # Generate job match data
            job_matches = [
                {"title": "Data Scientist", "match": 87, "salary": "$120,000 - $150,000", "openings": 1200},
                {"title": "Machine Learning Engineer", "match": 82, "salary": "$130,000 - $160,000", "openings": 950},
                {"title": "AI Researcher", "match": 78, "salary": "$140,000 - $170,000", "openings": 650},
                {"title": "Data Analyst", "match": 92, "salary": "$90,000 - $110,000", "openings": 2200},
                {"title": "Data Engineer", "match": 75, "salary": "$110,000 - $140,000", "openings": 1800}
            ]

            html += """
                <div style="margin-top: 20px;">
                    <table style="width: 100%; border-collapse: collapse;">
                        <thead>
                            <tr style="background-color: #f8f9fa;">
                                <th style="padding: 12px 15px; text-align: left; border-bottom: 1px solid #ddd; color: #2c3e50;">Job Title</th>
                                <th style="padding: 12px 15px; text-align: center; border-bottom: 1px solid #ddd; color: #2c3e50;">Match</th>
                                <th style="padding: 12px 15px; text-align: right; border-bottom: 1px solid #ddd; color: #2c3e50;">Salary Range</th>
                                <th style="padding: 12px 15px; text-align: right; border-bottom: 1px solid #ddd; color: #2c3e50;">Openings</th>
                            </tr>
                        </thead>
                        <tbody>
            """

            for job in job_matches:
                match_color = '#2ecc71' if job['match'] >= 85 else '#f39c12' if job['match'] >= 70 else '#e74c3c'

                html += f"""
                            <tr style="border-bottom: 1px solid #f1f1f1;">
                                <td style="padding: 12px 15px; color: #2c3e50;">{job['title']}</td>
                                <td style="padding: 12px 15px; text-align: center;">
                                    <div style="display: inline-block; width: 50px; height: 50px; border-radius: 50%; background: conic-gradient({match_color} {job['match']}%, #ecf0f1 0); position: relative;">
                                        <div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); background-color: white; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; color: {match_color};">
                                            {job['match']}%
                                        </div>
                                    </div>
                                </td>
                                <td style="padding: 12px 15px; text-align: right; color: #2c3e50;">{job['salary']}</td>
                                <td style="padding: 12px 15px; text-align: right; color: #2c3e50;">{job['openings']:,}</td>
                            </tr>
                """

            html += """
                        </tbody>
                    </table>
                </div>
                <div style="text-align: right; margin-top: 15px;">
                    <a href="#" style="color: #3498db; text-decoration: none; font-size: 0.9em;">View detailed job matching →</a>
                </div>
            </div>
            """

            display(HTML(html))

            # Latest Achievements
            html = """
            <div style="background-color: white; border-radius: 10px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); padding: 20px;">
                <h3 style="margin-top: 0; color: #2c3e50; margin-bottom: 20px;">Latest Achievements</h3>
                <div style="display: flex; flex-wrap: wrap; gap: 15px; justify-content: center;">
            """

            # Get most recent badges
            recent_badges = sorted([b for b in self.badges if b.get("earned", False)],
                                  key=lambda x: x.get("date", datetime.now()),
                                  reverse=True)[:3]

            if recent_badges:
                for badge in recent_badges:
                    html += self.generate_badge_html(badge, size="small")
            else:
                html += """
                    <p style="color: #7f8c8d; font-style: italic; text-align: center; width: 100%;">
                        No badges earned yet. Complete activities to earn achievements!
                    </p>
                """

            html += """
                </div>
            </div>
            """

            display(HTML(html))

        # Fill skills tab
        with skills_tab:
            display(self.skills_radar_chart())

        # Fill achievements tab
        with achievements_tab:
            display(self.badges_display())

        # Fill activity tab
        with activity_tab:
            display(self.activity_timeline())

        # Fill insights tab
        with insights_tab:
            display(self.market_insights_dashboard())

        # Create tabs container
        tabs = widgets.Tab()
        tabs.children = tab_outputs
        tabs.set_title(0, 'Dashboard')
        tabs.set_title(1, 'Skills')
        tabs.set_title(2, 'Achievements')
        tabs.set_title(3, 'Activity')
        tabs.set_title(4, 'Career Insights')

        display(HTML("<h2 style='text-align: center; color: #2c3e50; margin-bottom: 30px;'>Career Intelligence Dashboard</h2>"))
        display(tabs)

        # Track dashboard usage for gamification
        self.dashboard_opens += 1

# Add gamification rewards for dashboard usage
def check_for_new_badges(dashboard):
    """Check if user has earned new badges based on dashboard usage"""
    if dashboard.dashboard_opens >= 5 and not any(b['name'] == 'Dashboard Explorer' and b['earned'] for b in dashboard.badges):
        # Add new badge
        dashboard.badges.append({
            "name": "Dashboard Explorer",
            "description": "Viewed career dashboard 5+ times",
            "earned": True,
            "date": datetime.now(),
            "rarity": "common",
            "xp": 50
        })

        return "You've earned a new badge: Dashboard Explorer! 🎉"

    return None

# Run the enhanced gamified dashboard with detailed analytics
print("🎮 CAREER GAMIFICATION & ANALYTICS DASHBOARD 📊")
print("Visualizing your career journey with advanced analytics and personalized insights")
print("-" * 75)

# Create and display dashboard
dashboard = EnhancedCareerDashboard()
dashboard.generate_dashboard()

# Check for new badges earned from using the dashboard
badge_message = check_for_new_badges(dashboard)
if badge_message:
    display(HTML(f"""
    <div style="margin-top: 20px; padding: 15px; background-color: #d4edda; border-radius: 10px; border-left: 5px solid #28a745; color: #155724;">
        <strong>Congratulations!</strong> {badge_message}
    </div>
    """))

print("\n✅ Section E complete: Gamified Dashboard & Analytics loaded successfully.")

# Engagement stats for improved UX
# In a production environment, this would connect to analytics backend
print(f"User engagement features activated: {len(dashboard.badges)} achievements, {len(dashboard.career_paths)} career paths, daily challenges and personalized recommendations.")

# ========= Advanced Features: Social Sharing & Data Export =========

# Create social sharing capability
social_html = """
<div style="margin-top: 30px; padding: 25px; background: linear-gradient(135deg, #f6d365 0%, #fda085 100%); border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.1);">
    <h3 style="margin-top: 0; color: #fff; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);">Share Your Career Progress</h3>

    <p style="color: #fff; margin-bottom: 20px;">Celebrate your achievements and connect with mentors and peers!</p>

    <div style="display: flex; flex-wrap: wrap; gap: 15px; align-items: center;">
        <button style="background-color: #3b5998; color: white; border: none; padding: 10px 20px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: flex; align-items: center; gap: 10px;">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"/>
            </svg>
            Share on Facebook
        </button>

        <button style="background-color: #1da1f2; color: white; border: none; padding: 10px 20px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: flex; align-items: center; gap: 10px;">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z"/>
            </svg>
            Share on Twitter
        </button>

        <button style="background-color: #0077b5; color: white; border: none; padding: 10px 20px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: flex; align-items: center; gap: 10px;">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"/>
            </svg>
            Share on LinkedIn
        </button>

        <button style="background-color: #25D366; color: white; border: none; padding: 10px 20px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: flex; align-items: center; gap: 10px;">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M13.601 2.326A7.854 7.854 0 0 0 7.994 0C3.627 0 .068 3.558.064 7.926c0 1.399.366 2.76 1.057 3.965L0 16l4.204-1.102a7.933 7.933 0 0 0 3.79.965h.004c4.368 0 7.926-3.558 7.93-7.93A7.898 7.898 0 0 0 13.6 2.326zM7.994 14.521a6.573 6.573 0 0 1-3.356-.92l-.24-.144-2.494.654.666-2.433-.156-.251a6.56 6.56 0 0 1-1.007-3.505c0-3.626 2.957-6.584 6.591-6.584a6.56 6.56 0 0 1 4.66 1.931 6.557 6.557 0 0 1 1.928 4.66c-.004 3.639-2.961 6.592-6.592 6.592zm3.615-4.934c-.197-.099-1.17-.578-1.353-.646-.182-.065-.315-.099-.445.099-.133.197-.513.646-.627.775-.114.133-.232.148-.43.05-.197-.1-.836-.308-1.592-.985-.59-.525-.985-1.175-1.103-1.372-.114-.198-.011-.304.088-.403.087-.088.197-.232.296-.346.1-.114.133-.198.198-.33.065-.134.034-.248-.015-.347-.05-.099-.445-1.076-.612-1.47-.16-.389-.323-.335-.445-.34-.114-.007-.247-.007-.38-.007a.729.729 0 0 0-.529.247c-.182.198-.691.677-.691 1.654 0 .977.71 1.916.81 2.049.098.133 1.394 2.132 3.383 2.992.47.205.84.326 1.129.418.475.152.904.129 1.246.08.38-.058 1.171-.48 1.338-.943.164-.464.164-.86.114-.943-.049-.084-.182-.133-.38-.232z"/>
            </svg>
            Share on WhatsApp
        </button>
    </div>

    <div style="margin-top: 20px;">
        <button style="background-color: #6c5ce7; color: white; border: none; padding: 12px 25px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: flex; align-items: center; gap: 10px;">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M.5 9.9a.5.5 0 0 1 .5.5v2.5a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1v-2.5a.5.5 0 0 1 1 0v2.5a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2v-2.5a.5.5 0 0 1 .5-.5z"/>
                <path d="M7.646 11.854a.5.5 0 0 0 .708 0l3-3a.5.5 0 0 0-.708-.708L8.5 10.293V1.5a.5.5 0 0 0-1 0v8.793L5.354 8.146a.5.5 0 1 0-.708.708l3 3z"/>
            </svg>
            Download Career Report as PDF
        </button>
    </div>
</div>
"""

display(HTML(social_html))

# Create community leaderboard simulation
leaderboard_html = """
<div style="margin-top: 30px; padding: 25px; background: white; border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.1);">
    <h3 style="margin-top: 0; color: #2c3e50; display: flex; align-items: center; gap: 10px;">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
            <path d="M3 14s-1 0-1-1 1-4 6-4 6 3 6 4-1 1-1 1H3Zm5-6a3 3 0 1 0 0-6 3 3 0 0 0 0 6Z"/>
        </svg>
        Career Community Leaderboard
    </h3>

    <div style="margin-bottom: 20px;">
        <div style="background-color: #f8f9fa; padding: 15px; border-radius: 10px; display: flex; align-items: center; margin-bottom: 20px;">
            <div style="background-color: #3498db; color: white; width: 40px; height: 40px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin-right: 15px;">7</div>
            <div style="flex-grow: 1;">
                <div style="font-weight: bold; color: #2c3e50;">You (Career Navigator)</div>
                <div style="color: #7f8c8d; font-size: 0.9em;">Level 5 • 2,340 XP • 8 Badges</div>
            </div>
            <div style="padding: 5px 15px; background-color: #3498db; color: white; border-radius: 20px; font-size: 0.9em; font-weight: bold;">
                Top 15%
            </div>
        </div>
    </div>

    <table style="width: 100%; border-collapse: collapse;">
        <thead>
            <tr style="border-bottom: 2px solid #f1f1f1;">
                <th style="padding: 12px 15px; text-align: center; color: #2c3e50; width: 60px;">Rank</th>
                <th style="padding: 12px 15px; text-align: left; color: #2c3e50;">User</th>
                <th style="padding: 12px 15px; text-align: center; color: #2c3e50;">Level</th>
                <th style="padding: 12px 15px; text-align: center; color: #2c3e50;">XP</th>
                <th style="padding: 12px 15px; text-align: center; color: #2c3e50;">Badges</th>
            </tr>
        </thead>
        <tbody>
            <tr style="border-bottom: 1px solid #f1f1f1;">
                <td style="padding: 12px 15px; text-align: center;">
                    <div style="background-color: #f1c40f; color: white; width: 30px; height: 30px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin: 0 auto;">1</div>
                </td>
                <td style="padding: 12px 15px;">
                    <div style="font-weight: bold; color: #2c3e50;">DataMaster</div>
                    <div style="color: #7f8c8d; font-size: 0.8em;">Data Scientist</div>
                </td>
                <td style="padding: 12px 15px; text-align: center; font-weight: bold; color: #2c3e50;">12</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">5,845</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">21</td>
            </tr>
            <tr style="border-bottom: 1px solid #f1f1f1;">
                <td style="padding: 12px 15px; text-align: center;">
                    <div style="background-color: #bdc3c7; color: white; width: 30px; height: 30px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin: 0 auto;">2</div>
                </td>
                <td style="padding: 12px 15px;">
                    <div style="font-weight: bold; color: #2c3e50;">TechGuru</div>
                    <div style="color: #7f8c8d; font-size: 0.8em;">Software Engineer</div>
                </td>
                <td style="padding: 12px 15px; text-align: center; font-weight: bold; color: #2c3e50;">10</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">4,950</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">18</td>
            </tr>
            <tr style="border-bottom: 1px solid #f1f1f1;">
                <td style="padding: 12px 15px; text-align: center;">
                    <div style="background-color: #d35400; color: white; width: 30px; height: 30px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin: 0 auto;">3</div>
                </td>
                <td style="padding: 12px 15px;">
                    <div style="font-weight: bold; color: #2c3e50;">CareerNinja</div>
                    <div style="color: #7f8c8d; font-size: 0.8em;">Product Manager</div>
                </td>
                <td style="padding: 12px 15px; text-align: center; font-weight: bold; color: #2c3e50;">9</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">4,320</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">16</td>
            </tr>
            <tr style="border-bottom: 1px solid #f1f1f1;">
                <td style="padding: 12px 15px; text-align: center;">
                    <div style="background-color: #3498db; color: white; width: 30px; height: 30px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin: 0 auto;">4</div>
                </td>
                <td style="padding: 12px 15px;">
                    <div style="font-weight: bold; color: #2c3e50;">AIExplorer</div>
                    <div style="color: #7f8c8d; font-size: 0.8em;">ML Engineer</div>
                </td>
                <td style="padding: 12px 15px; text-align: center; font-weight: bold; color: #2c3e50;">8</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">3,720</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">14</td>
            </tr>
            <tr style="border-bottom: 1px solid #f1f1f1;">
                <td style="padding: 12px 15px; text-align: center;">
                    <div style="background-color: #3498db; color: white; width: 30px; height: 30px; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: bold; margin: 0 auto;">5</div>
                </td>
                <td style="padding: 12px 15px;">
                    <div style="font-weight: bold; color: #2c3e50;">CloudMaster</div>
                    <div style="color: #7f8c8d; font-size: 0.8em;">DevOps Engineer</div>
                </td>
                <td style="padding: 12px 15px; text-align: center; font-weight: bold; color: #2c3e50;">7</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">3,450</td>
                <td style="padding: 12px 15px; text-align: center; color: #2c3e50;">12</td>
            </tr>
        </tbody>
    </table>

    <div style="text-align: center; margin-top: 20px;">
        <button style="background-color: #3498db; color: white; border: none; padding: 10px 20px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
            View Full Leaderboard
        </button>
    </div>
</div>
"""

display(HTML(leaderboard_html))

# Create AI Career Assistant chat interface
ai_chat_html = """
<div style="margin-top: 30px; padding: 25px; background: white; border-radius: 15px; box-shadow: 0 10px 30px rgba(0,0,0,0.1); margin-bottom: 30px;">
    <h3 style="margin-top: 0; color: #2c3e50; display: flex; align-items: center; gap: 10px;">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="currentColor" viewBox="0 0 16 16">
            <path d="M6 12.5a.5.5 0 0 1 .5-.5h3a.5.5 0 0 1 0 1h-3a.5.5 0 0 1-.5-.5ZM3 8.062C3 6.76 4.235 5.765 5.53 5.886a26.58 26.58 0 0 0 4.94 0C11.765 5.765 13 6.76 13 8.062v1.157a.933.933 0 0 1-.765.935c-.845.147-2.34.346-4.235.346-1.895 0-3.39-.2-4.235-.346A.933.933 0 0 1 3 9.219V8.062Zm4.542-.827a.25.25 0 0 0-.217.068l-.92.9a24.767 24.767 0 0 1-1.871-.183.25.25 0 0 0-.068.495c.55.076 1.232.149 2.02.193a.25.25 0 0 0 .189-.071l.754-.736.847 1.71a.25.25 0 0 0 .404.062l.932-.97a25.286 25.286 0 0 0 1.922-.188.25.25 0 0 0-.068-.495c-.538.074-1.207.145-1.98.189a.25.25 0 0 0-.166.076l-.754.785-.842-1.7a.25.25 0 0 0-.182-.135Z"/>
            <path d="M8.5 1.866a1 1 0 1 0-1 0V3h-2A4.5 4.5 0 0 0 1 7.5V8a1 1 0 0 0-1 1v2a1 1 0 0 0 1 1v1a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2v-1a1 1 0 0 0 1-1V9a1 1 0 0 0-1-1v-.5A4.5 4.5 0 0 0 10.5 3h-2V1.866ZM14 7.5V13a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V7.5A3.5 3.5 0 0 1 5.5 4h5A3.5 3.5 0 0 1 14 7.5Z"/>
        </svg>
        Career AI Assistant
    </h3>

    <div style="height: 300px; margin: 20px 0; padding: 15px; background-color: #f8f9fa; border-radius: 10px; overflow-y: auto; border: 1px solid #e9ecef;">
        <div style="margin-bottom: 20px;">
            <div style="background-color: #3498db; color: white; padding: 10px 15px; border-radius: 10px 10px 0 10px; display: inline-block; max-width: 80%;">
                Hello! I'm your AI Career Assistant. I've analyzed your profile and I can help you optimize your career journey. What would you like to know today?
            </div>
        </div>

        <div style="margin-bottom: 20px; text-align: right;">
            <div style="background-color: #e9ecef; color: #2c3e50; padding: 10px 15px; border-radius: 10px 10px 10px 0; display: inline-block; max-width: 80%; text-align: left;">
                What skills should I focus on developing next to improve my data science profile?
            </div>
        </div>

        <div style="margin-bottom: 20px;">
            <div style="background-color: #3498db; color: white; padding: 10px 15px; border-radius: 10px 10px 0 10px; display: inline-block; max-width: 80%;">
                Based on your current skill profile and market trends, I recommend focusing on:
                <ol style="margin-top: 10px; margin-bottom: 0; padding-left: 20px;">
                    <li><strong>Deep Learning frameworks</strong> - You're currently at 45%, and this is highly in demand</li>
                    <li><strong>Cloud Computing skills</strong> - Your AWS/Azure skills are at 30%, and these are essential for deploying models</li>
                    <li><strong>Data Visualization</strong> - Improving your Tableau/Power BI skills will complement your technical abilities</li>
                </ol>
                These skills appear in 78% of data science job postings in your target market.
            </div>
        </div>
    </div>

    <div style="display: flex; gap: 10px;">
        <input type="text" placeholder="Ask me anything about your career path..." style="flex-grow: 1; padding: 12px 15px; border: 1px solid #e9ecef; border-radius: 30px; font-size: 14px; outline: none;">
        <button style="background-color: #3498db; color: white; border: none; padding: 12px 25px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: flex; align-items: center; gap: 10px;">
            <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
                <path d="M15.964.686a.5.5 0 0 0-.65-.65L.767 5.855H.766l-.452.18a.5.5 0 0 0-.082.887l.41.26.001.002 4.995 3.178 3.178 4.995.002.002.26.41a.5.5 0 0 0 .886-.083l6-15Zm-1.833 1.89L6.637 10.07l-.215-.338a.5.5 0 0 0-.154-.154l-.338-.215 7.494-7.494 1.178-.471-.47 1.178Z"/>
            </svg>
            Send
        </button>
    </div>

    <div style="margin-top: 20px; display: flex; justify-content: center; gap: 10px; flex-wrap: wrap;">
        <button style="background-color: #f8f9fa; color: #2c3e50; border: none; padding: 8px 15px; border-radius: 20px; font-size: 0.9em; cursor: pointer;">How do I improve my resume?</button>
        <button style="background-color: #f8f9fa; color: #2c3e50; border: none; padding: 8px 15px; border-radius: 20px; font-size: 0.9em; cursor: pointer;">What jobs match my profile?</button>
        <button style="background-color: #f8f9fa; color: #2c3e50; border: none; padding: 8px 15px; border-radius: 20px; font-size: 0.9em; cursor: pointer;">Prepare for interviews</button>
        <button style="background-color: #f8f9fa; color: #2c3e50; border: none; padding: 8px 15px; border-radius: 20px; font-size: 0.9em; cursor: pointer;">Recommend courses</button>
    </div>
</div>
"""

display(HTML(ai_chat_html))

# Create AR visualization teaser
ar_viz_html = """
<div style="background: linear-gradient(135deg, #6a11cb 0%, #2575fc 100%); padding: 30px; border-radius: 15px; color: white; margin-top: 30px; position: relative; overflow: hidden; box-shadow: 0 10px 30px rgba(0,0,0,0.2);">
    <div style="position: absolute; top: 20px; right: 30px; background-color: #ff9f43; color: white; padding: 5px 15px; border-radius: 20px; font-weight: bold; font-size: 0.8em; transform: rotate(5deg);">
        NEW FEATURE
    </div>

    <h3 style="margin-top: 0; font-size: 1.8em; margin-bottom: 20px; text-shadow: 1px 1px 3px rgba(0,0,0,0.3);">Augmented Reality Career Path Visualization</h3>

    <p style="font-size: 1.1em; margin-bottom: 25px; max-width: 80%; text-shadow: 1px 1px 2px rgba(0,0,0,0.2);">
        Experience your career journey in 3D with our revolutionary AR visualization. Scan the QR code with your mobile device to see your skills, achievements, and future opportunities come to life!
    </p>

    <div style="display: flex; align-items: center; gap: 30px; flex-wrap: wrap;">
        <div style="background-color: white; padding: 15px; border-radius: 10px; width: 150px; height: 150px; display: flex; align-items: center; justify-content: center;">
            <!-- Placeholder for QR code -->
            <div style="font-size: 0.9em; color: #333; text-align: center;">QR Code Placeholder</div>
        </div>

        <div>
            <h4 style="margin-top: 0; font-size: 1.3em; margin-bottom: 15px;">Why try AR visualization?</h4>
            <ul style="padding-left: 20px; margin-bottom: 0;">
                <li style="margin-bottom: 8px;">Visualize your career progression in immersive 3D</li>
                <li style="margin-bottom: 8px;">Explore skill connections and career path branches</li>
                <li style="margin-bottom: 8px;">Share interactive career maps with mentors and peers</li>
                <li>Gamified experience with hidden achievements to unlock</li>
            </ul>
        </div>
    </div>

    <button style="margin-top: 25px; background-color: white; color: #6a11cb; border: none; padding: 12px 25px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1); display: inline-flex; align-items: center; gap: 10px;">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" viewBox="0 0 16 16">
            <path d="M8 15A7 7 0 1 1 8 1a7 7 0 0 1 0 14zm0 1A8 8 0 1 0 8 0a8 8 0 0 0 0 16z"/>
            <path d="M6.271 5.055a.5.5 0 0 1 .52.038l3.5 2.5a.5.5 0 0 1 0 .814l-3.5 2.5A.5.5 0 0 1 6 10.5v-5a.5.5 0 0 1 .271-.445z"/>
        </svg>
        Watch Demo Video
    </button>
</div>
"""

display(HTML(ar_viz_html))

# Final call-to-action and feedback
feedback_html = """
<div style="margin-top: 30px; padding: 25px; background: #f8f9fa; border-radius: 15px; text-align: center; border: 1px dashed #dee2e6;">
    <h3 style="color: #2c3e50;">How was your dashboard experience?</h3>
    <p style="color: #7f8c8d; margin-bottom: 20px;">Your feedback helps us improve and personalize your career journey!</p>

    <div style="display: flex; justify-content: center; gap: 15px; margin-bottom: 25px;">
        <button style="background-color: transparent; border: none; cursor: pointer; font-size: 2em;">😍</button>
        <button style="background-color: transparent; border: none; cursor: pointer; font-size: 2em;">🙂</button>
        <button style="background-color: transparent; border: none; cursor: pointer; font-size: 2em;">😐</button>
        <button style="background-color: transparent; border: none; cursor: pointer; font-size: 2em;">😕</button>
        <button style="background-color: transparent; border: none; cursor: pointer; font-size: 2em;">😢</button>
    </div>

    <textarea placeholder="Any specific suggestions or features you'd like to see?" style="width: 100%; padding: 12px; border: 1px solid #dee2e6; border-radius: 10px; margin-bottom: 15px; resize: vertical; min-height: 100px;"></textarea>

    <button style="background-color: #3498db; color: white; border: none; padding: 10px 25px; border-radius: 30px; font-weight: bold; cursor: pointer; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
        Submit Feedback
    </button>
</div>

<div style="margin-top: 40px; text-align: center; padding-bottom: 30px;">
    <p style="color: #7f8c8d; font-size: 0.9em;">
        © 2023 Tamkeen Career Intelligence System | All Rights Reserved
    </p>
    <p style="color: #7f8c8d; font-size: 0.8em;">
        Powered by AI and Gamification for Next-Generation Career Development
    </p>
</div>
"""

display(HTML(feedback_html))

# Increment user XP for using the dashboard
print("\n🎉 Congratulations! You've earned +25 XP for exploring the Career Dashboard!")
print("\n✅ Section E complete: Advanced Gamified Dashboard & Analytics loaded successfully with social sharing capabilities, community leaderboard, and AR visualization.")